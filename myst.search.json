{"version":"1","records":[{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment"},"type":"lvl1","url":"/appendix/gpu-maths","position":0},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment"},"content":"Before provisioning a GPU VM or submitting a Vertex AI training job, it is worth spending a few minutes estimating whether your chosen GPU can actually fit your experiment. Running out of GPU memory mid-training is one of the most common and frustrating errors in ML, and it is entirely avoidable with some simple upfront calculation.","type":"content","url":"/appendix/gpu-maths","position":1},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"What Determines GPU Memory Usage?"},"type":"lvl2","url":"/appendix/gpu-maths#what-determines-gpu-memory-usage","position":2},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"What Determines GPU Memory Usage?"},"content":"The GPU memory required for a training run is determined by four main factors:\n\nModel parameters - the weights of the model itself. The memory cost depends on how many parameters the model has and the precision they are stored in\n\nGradients - a copy of the model parameters computed during the backward pass. In full fine-tuning these match the size of the model weights\n\nOptimizer states - the Adam optimizer stores two additional copies of the parameters (momentum and variance), roughly doubling the gradient memory cost\n\nActivations - intermediate values computed during the forward pass. These scale with batch size and sequence length","type":"content","url":"/appendix/gpu-maths#what-determines-gpu-memory-usage","position":3},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"Worked Example: Fine-tuning Gemma 3 1B Model with LoRA"},"type":"lvl2","url":"/appendix/gpu-maths#worked-example-fine-tuning-gemma-3-1b-model-with-lora","position":4},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"Worked Example: Fine-tuning Gemma 3 1B Model with LoRA"},"content":"Let us walk through how to estimate the GPU memory required for fine-tuning Gemma 3 1B with LoRA in bfloat16, which is exactly the experiment we used in this tutorial.\n\nStep 1: Base model weights\n\nGemma 3 1B has 1 billion parameters. We load it in bfloat16, where each parameter takes 2 bytes (16 bits = 2 bytes):1,000,000,000 parameters x 2 bytes = 2,000,000,000 bytes = ~2GB\n\nStep 2: LoRA adapter weights\n\nWith LoRA rank 8 targeting q_proj and v_proj, we are training roughly 0.1% of the model‚Äôs parameters. This adds a negligible amount of memory:0.1% x 2GB = ~0.002GB\n\nStep 3: Gradients and optimizer states\n\nSince LoRA freezes the base model, gradients and optimizer states only apply to the small set of trainable LoRA parameters, not the full model. This adds roughly:~0.1GB\n\nStep 4: Activations\n\nActivations scale with batch size and sequence length. With batch size 8 and max length 256:~0.3GB\n\nTotal estimate:2GB (base model) + 0.002GB (LoRA) + 0.1GB (gradients) + 0.3GB (activations) = ~2.5GB\n\nThis is why a 24GB NVIDIA L4 GPU is more than sufficient for this experiment. We are only using about 10% of its available VRAM. You could even run it on a smaller 16GB T4 with room to spare.\n\nNote: These are rough estimates. Actual memory usage can vary depending on the framework version, attention implementation, and other factors. It makes sense to run a quick test with a small number of samples before committing to a full training run.","type":"content","url":"/appendix/gpu-maths#worked-example-fine-tuning-gemma-3-1b-model-with-lora","position":5},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"What If You Were to Run This Without LoRA?"},"type":"lvl2","url":"/appendix/gpu-maths#what-if-you-were-to-run-this-without-lora","position":6},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"What If You Were to Run This Without LoRA?"},"content":"For comparison, here is what full fine-tuning of Gemma 3 1B in float32 would cost:1B parameters x 4 bytes (float32) = 4GB (model weights)\n+ 4GB (gradients)\n+ 8GB (Adam optimizer states)\n+ ~1GB (activations)\n= ~17GB total\n\nThis would not fit on a 16GB T4 and would be tight on a 24GB L4. This is exactly why LoRA is so valuable. It reduces the memory requirement from ~17GB to ~2.5GB for the same base model.","type":"content","url":"/appendix/gpu-maths#what-if-you-were-to-run-this-without-lora","position":7},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"Estimating GPU Hours"},"type":"lvl2","url":"/appendix/gpu-maths#estimating-gpu-hours","position":8},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"Estimating GPU Hours"},"content":"Once you know your experiment fits in memory, the next question is how long it will take. A simple estimate:GPU hours = (num_samples x num_epochs) / (steps_per_second x batch_size x 3600)\n\nFor our experiment on the L4, we use 3 steps/second as our estimate. This comes directly from the training logs we observed during the hands-on session, where the L4 consistently processed between 2.7 and 3.0 steps per second for this specific workload (Gemma 3 1B, LoRA rank 8, bfloat16, batch size 8, sequence length 256). We round up to 3 for a conservative estimate:(10,000 samples x 2 epochs) / (3 steps/second x 8 batch size x 3600 seconds)\n= 20,000 / 86,400\n= ~0.23 hours (~14 minutes)\n\nThis matches what we observed in practice, where the full training run completed in roughly 15 minutes.\n\nA few things that affect training speed:\n\nBatch size - larger batches process more samples per step, reducing total steps and training time. However, the tradeoff is that larger batch sizes require more GPU VRAM to store activations, so there is a practical upper limit based on how much memory your GPU has available\n\nSequence length - longer sequences increase the cost of each step due to the attention mechanism scaling quadratically with sequence length\n\nPrecision - bfloat16 is roughly 2x faster than float32 on modern GPUs\n\nGPU generation - an A100 is roughly 3 to 4x faster per step than an L4 for the same workload\n\nUse the \n\nGCP Pricing Calculator with your estimated GPU hours to get a cost estimate before starting a run.","type":"content","url":"/appendix/gpu-maths#estimating-gpu-hours","position":9},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"Common GCP GPUs and When to Use Them"},"type":"lvl2","url":"/appendix/gpu-maths#common-gcp-gpus-and-when-to-use-them","position":10},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"Common GCP GPUs and When to Use Them"},"content":"GPU\n\nVRAM\n\nBest For\n\nNVIDIA T4\n\n16GB\n\nSmall models up to 3B, inference, cost-sensitive runs\n\nNVIDIA L4\n\n24GB\n\nModels up to 7B with LoRA, good price/performance\n\nNVIDIA A100 40GB\n\n40GB\n\nModels up to 13B full fine-tune, large batch training\n\nNVIDIA A100 80GB\n\n80GB\n\nModels up to 70B with LoRA, large context lengths\n\nNVIDIA H100\n\n80GB\n\nLargest models, fastest training, highest cost","type":"content","url":"/appendix/gpu-maths#common-gcp-gpus-and-when-to-use-them","position":11},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"Practical Tips"},"type":"lvl2","url":"/appendix/gpu-maths#practical-tips","position":12},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"Practical Tips"},"content":"Start small and scale up. Run a quick test with a small number of training examples before committing to a full run. If it fits in memory, scale up.\n\nReduce batch size if you hit OOM. Halving the batch size roughly halves activation memory. Use gradient accumulation to compensate for effective batch size.\n\nUse bfloat16. Switching from float32 to bfloat16 halves model and gradient memory with minimal impact on training quality. This is why we set bf16=True in the training arguments.\n\nCheck memory usage during training. Run nvidia-smi in a separate terminal or tmux window during training to see live GPU memory utilization.","type":"content","url":"/appendix/gpu-maths#practical-tips","position":13},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"References and Further Reading"},"type":"lvl2","url":"/appendix/gpu-maths#references-and-further-reading","position":14},{"hierarchy":{"lvl1":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","lvl2":"References and Further Reading"},"content":"NVIDIA GPU Specifications - Full specs for all NVIDIA data center GPUs including VRAM and compute capacity\n\nGCP GPU Pricing - Full pricing breakdown for GPU VM instances on GCP\n\nHuggingFace Model Memory Calculator - A handy tool for estimating memory requirements for HuggingFace models\n\nLoRA: Low-Rank Adaptation of Large Language Models - The original LoRA paper by Hu et al.\n\nMixed Precision Training - The paper introducing bfloat16 and float16 training","type":"content","url":"/appendix/gpu-maths#references-and-further-reading","position":15},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP"},"content":"A hands-on tutorial for beginners who want to use Google Cloud Platform (GCP) for large-scale machine learning experiments or AI workloads.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üéØ What You‚Äôll Learn"},"type":"lvl2","url":"/#id-what-youll-learn","position":2},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üéØ What You‚Äôll Learn"},"content":"How to use the GCP console and the gcloud CLI\n\nHow to provision and manage virtual machines and GPU resources on GCP\n\nHow to run large-scale ML experiments (like fine-tuning large language models) on GCP VMs\n\nWhen to use virtual machines vs. managed services like Vertex AI\n\nHow to predict and manage GPU memory\n\nHow to avoid expensive cloud mistakes with cost monitoring and budget alerts","type":"content","url":"/#id-what-youll-learn","position":3},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üìã Prerequisites"},"type":"lvl2","url":"/#id-prerequisites","position":4},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üìã Prerequisites"},"content":"Required:\n\nPython programming\n\nComfort with terminal/command line\n\nBasic knowledge of machine learning/deep learning\n\nNot required:\n\nGCP experience (we start from scratch!)\n\nDeep learning expertise\n\nCloud computing background\n\nYou will also need:\n\nA Google Cloud account (there is a free tier available with $300 credits)\n\nA text editor (VS Code, Sublime Text, Cursor, etc.)\n\nTerminal access","type":"content","url":"/#id-prerequisites","position":5},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üï• Duration"},"type":"lvl2","url":"/#id-duration","position":6},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üï• Duration"},"content":"4-6 hours (can be split into multiple sessions)\n\nCore tutorial: 3-4 hours\n\nOptional appendices: 25-30 minutes additional","type":"content","url":"/#id-duration","position":7},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üìö Tutorial Structure"},"type":"lvl2","url":"/#id-tutorial-structure","position":8},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üìö Tutorial Structure"},"content":"","type":"content","url":"/#id-tutorial-structure","position":9},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl3":"Core Path","lvl2":"üìö Tutorial Structure"},"type":"lvl3","url":"/#core-path","position":10},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl3":"Core Path","lvl2":"üìö Tutorial Structure"},"content":"Part\n\nTitle\n\nTime\n\nLearning Objective\n\n1\n\nGCP Foundations\n\n30 mins\n\nUnderstand GCP and interact with it via the console and gcloud CLI\n\n2\n\nRunning Experiments on GCP VMs\n\n45 mins\n\nLearn how to run AI/ML experiments on GCP virtual machines with GPU support\n\n3\n\nVertex AI\n\n65 mins\n\nUnderstand how to use the managed Vertex AI service for training jobs and notebooks\n\n4\n\nCost Management\n\n30 mins\n\nUnderstand budget alerts and best practices for cost management","type":"content","url":"/#core-path","position":11},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl3":"Appendix (Optional)","lvl2":"üìö Tutorial Structure"},"type":"lvl3","url":"/#appendix-optional","position":12},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl3":"Appendix (Optional)","lvl2":"üìö Tutorial Structure"},"content":"Appendix\n\nTitle\n\nTime\n\nFocus\n\nA\n\nGPU Memory Maths\n\n45 mins\n\nUnderstand how to estimate GPU memory requirements, avoid OOM errors, and choose the right GPU for your needs","type":"content","url":"/#appendix-optional","position":13},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üí° What Makes This Tutorial Different"},"type":"lvl2","url":"/#id-what-makes-this-tutorial-different","position":14},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üí° What Makes This Tutorial Different"},"content":"This tutorial teaches you what you need to run AI/ML experiments on GCP, emphasizing:\n\nSystems thinking for ML infrastructure\n\nCost-conscious cloud practices\n\nHow to structure reproducible AI/ML experiments for real-world projects\n\nHands-on practice with every learned concept\n\nThe content is also structured to be used on-demand so you do not have to read the entire tutorial.","type":"content","url":"/#id-what-makes-this-tutorial-different","position":15},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üéì Who This is For"},"type":"lvl2","url":"/#id-who-this-is-for","position":16},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üéì Who This is For"},"content":"This tutorial was particularly designed for AIMS AI for Science students who do not have a background in cloud computing or software engineering, helping them use GCP more effectively for their research projects.\n\nHowever, it should be generally helpful for:\n\nBeginner AI/ML students and researchers\n\nAnyone curious about using GCP for running AI/ML experiments","type":"content","url":"/#id-who-this-is-for","position":17},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üß† How to Use This Tutorial"},"type":"lvl2","url":"/#id-how-to-use-this-tutorial","position":18},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üß† How to Use This Tutorial"},"content":"","type":"content","url":"/#id-how-to-use-this-tutorial","position":19},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl3":"Option 1: Linear (Recommended for beginners)","lvl2":"üß† How to Use This Tutorial"},"type":"lvl3","url":"/#option-1-linear-recommended-for-beginners","position":20},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl3":"Option 1: Linear (Recommended for beginners)","lvl2":"üß† How to Use This Tutorial"},"content":"Start from Part 1 and work through sequentially.","type":"content","url":"/#option-1-linear-recommended-for-beginners","position":21},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl3":"Option 2: Jump to What You Need","lvl2":"üß† How to Use This Tutorial"},"type":"lvl3","url":"/#option-2-jump-to-what-you-need","position":22},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl3":"Option 2: Jump to What You Need","lvl2":"üß† How to Use This Tutorial"},"content":"Need a quick-start guide to using GCP? ‚Üí Part 1\n\nWant to run ML experiments on a GPU VM? ‚Üí Part 2\n\nWant to learn how to use managed services like Vertex AI? ‚Üí Part 3\n\nNeed to understand how to monitor costs to manage your GCP credits properly? ‚Üí Part 4\n\nNeed to know how to choose a GPU for your experiment? ‚Üí Appendix A","type":"content","url":"/#option-2-jump-to-what-you-need","position":23},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"ü´±üèø‚Äçü´≤üèæ Contributing"},"type":"lvl2","url":"/#id-contributing","position":24},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"ü´±üèø‚Äçü´≤üèæ Contributing"},"content":"This tutorial is fully open-source! We welcome:\n\nüêõ Bug reports\n\nüí° Suggestions for improvements\n\nüìù Additional sections, examples, or exercises\n\nüåé Translations\n\nSee \n\nCONTRIBUTING.md for details.","type":"content","url":"/#id-contributing","position":25},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üìÑ License"},"type":"lvl2","url":"/#id-license","position":26},{"hierarchy":{"lvl1":"A Practical Guide to Running AI/ML Experiments on GCP","lvl2":"üìÑ License"},"content":"This tutorial is released under the CC-BY-4.0 License. See LICENSE for details.\n\nLet‚Äôs get started! ‚Üí \n\nPart 1: GCP Foundations","type":"content","url":"/#id-license","position":27},{"hierarchy":{"lvl1":"GCP Core Concepts"},"type":"lvl1","url":"/part1-gcp-foundations/understanding-gcp","position":0},{"hierarchy":{"lvl1":"GCP Core Concepts"},"content":"Before diving in, it‚Äôs essential to develop a clear intuition for what the Google Cloud Platform (GCP) is, its core components, and how it organizes and manages resources. This section also introduces the core ideas that will guide how we use GCP throughout this tutorial. This foundation will help you navigate GCP confidently and avoid common pitfalls.","type":"content","url":"/part1-gcp-foundations/understanding-gcp","position":1},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"What Is the Google Cloud Platform (GCP)?"},"type":"lvl2","url":"/part1-gcp-foundations/understanding-gcp#what-is-the-google-cloud-platform-gcp","position":2},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"What Is the Google Cloud Platform (GCP)?"},"content":"GCP is a suite of cloud computing services, including computing, storage, networking, data analytics, machine learning, etc., that runs on the same infrastructure Google uses for products like Search, Gmail, and YouTube. It allows individuals and organizations/enterprises to use Google‚Äôs computing infrastructure without owning or managing physical hardware.\n\nAt a high level, GCP provides a way to create, configure, and manage computing resources using software in an on-demand manner. Machines, storage, and networks are no longer physical objects, but configurable resources that can be created, modified, and removed as needed.\n\nNote\n\nIf the term cloud computing is unfamiliar, it may be helpful to pause here and review the general idea.\n\nYou do not need a deep understanding of cloud computing to follow this tutorial, but having a rough mental model can make later sections easier to reason about.\n\nHere are some helpful links:\n\nWhat is Cloud Computing? - Google Cloud (Blog)\n\nI Explain Cloud Computing in About 10 Minutes - Cloud Computing Insider (YouTube)","type":"content","url":"/part1-gcp-foundations/understanding-gcp#what-is-the-google-cloud-platform-gcp","position":3},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"Why Should You Care About GCP?"},"type":"lvl2","url":"/part1-gcp-foundations/understanding-gcp#why-should-you-care-about-gcp","position":4},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"Why Should You Care About GCP?"},"content":"For AI/ML researchers, GCP provides:\n\nCompute resources: Virtual machines with GPUs and TPUs.\n\nStorage: Persistent disks for storing datasets, model artifacts, etc.\n\nNetworking: High-speed connections between resources.\n\nFlexible pricing: Pay-per-second billing.\n\nIdentity and access management: Fine-grained control over who can create, view, or modify resources.","type":"content","url":"/part1-gcp-foundations/understanding-gcp#why-should-you-care-about-gcp","position":5},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"What is a Resource in GCP?"},"type":"lvl2","url":"/part1-gcp-foundations/understanding-gcp#what-is-a-resource-in-gcp","position":6},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"What is a Resource in GCP?"},"content":"In GCP, a resource is any manageable entity that you can create, configure, and pay fore.\n\nA resource represents something concrete, such as:\n\nA virtual machine\n\nA disk or storage bucket\n\nA network or firewall rule\n\nA GPU or TPU attached to a virtual machine\n\nA managed database service instance\n\nResources have a lifecycle: they can be created, modified, stopped, deleted, and monitored. Creating resources in GCP incurs cost.\n\nImportant\n\nOnce you understand that everything you use in GCP is a resource, the hierarchical organization of resources in GCP becomes intuitive.\n\nNote\n\nA resource is different from a service. A service in GCP is a managed feature or platform that provides a specific type of functionality, such as computing, storage, or machine learning, allowing you to create and manage resources without worrying about the underlying infrastructure. While a resource is a concrete instance created using that service.\n\nFor example, Cloud SQL is a service that lets you create and manage databases, and each database instance you create within it is a resource.","type":"content","url":"/part1-gcp-foundations/understanding-gcp#what-is-a-resource-in-gcp","position":7},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"GCP Resource Hierarchy"},"type":"lvl2","url":"/part1-gcp-foundations/understanding-gcp#gcp-resource-hierarchy","position":8},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"GCP Resource Hierarchy"},"content":"GCP organizes resources hierarchically. This structure is crucial for managing access, billing, and organization.Organization (e.g. *aims.ac.za*)\n‚îî‚îÄ‚îÄ Folders (optional groupings)\n        ‚îî‚îÄ‚îÄ Projects (your main workspace)\n            ‚îî‚îÄ‚îÄ Resources (virtual machines, storage, etc.)","type":"content","url":"/part1-gcp-foundations/understanding-gcp#gcp-resource-hierarchy","position":9},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"Organization","lvl2":"GCP Resource Hierarchy"},"type":"lvl3","url":"/part1-gcp-foundations/understanding-gcp#organization","position":10},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"Organization","lvl2":"GCP Resource Hierarchy"},"content":"The Organization sits at the top of the GCP resource hierarchy, representing an institution or company.\n\nNote\n\nFor AIMS Students: You‚Äôre working within the aims.ac.za organization. This means:\n\nBilling is managed centrally by AIMS\n\nYou have specific quotas and permissions\n\nYou create projects under this organization\n\nAdministrative policies are set at the organization level","type":"content","url":"/part1-gcp-foundations/understanding-gcp#organization","position":11},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"Folders (Optional)","lvl2":"GCP Resource Hierarchy"},"type":"lvl3","url":"/part1-gcp-foundations/understanding-gcp#folders-optional","position":12},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"Folders (Optional)","lvl2":"GCP Resource Hierarchy"},"content":"Folders group projects together. An organization (e.g., AIMS) may use folders to organize projects by cohort, program, or research group.\n\nYou typically won‚Äôt create folders yourself - focus on projects.","type":"content","url":"/part1-gcp-foundations/understanding-gcp#folders-optional","position":13},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"Projects","lvl2":"GCP Resource Hierarchy"},"type":"lvl3","url":"/part1-gcp-foundations/understanding-gcp#projects","position":14},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"Projects","lvl2":"GCP Resource Hierarchy"},"content":"Projects are the core organizational unit. Everything you create lives inside a project.\n\nKey Characteristics:\n\nEach project has a unique Project ID (immutable, globally unique)\n\nExample: trump-project\n\nEach project has a Project Name (can be changed)\n\nEach project has a Project Number (assigned by Google Cloud)\n\nAll resources belong to exactly one project\n\nBilling is tracked per project\n\nAccess control (IAM) is managed at the project level\n\nNote\n\nFor AIMS students, projects are already created for you by AIMS. You do not need to create a new project. You will work inside an existing project that has billing and permissions preconfigured.","type":"content","url":"/part1-gcp-foundations/understanding-gcp#projects","position":15},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"Core GCP Concepts"},"type":"lvl2","url":"/part1-gcp-foundations/understanding-gcp#core-gcp-concepts","position":16},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"Core GCP Concepts"},"content":"","type":"content","url":"/part1-gcp-foundations/understanding-gcp#core-gcp-concepts","position":17},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"1. Billing Accounts","lvl2":"Core GCP Concepts"},"type":"lvl3","url":"/part1-gcp-foundations/understanding-gcp#id-1-billing-accounts","position":18},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"1. Billing Accounts","lvl2":"Core GCP Concepts"},"content":"A billing account is linked to a payment method and pays for resource usage across projects.\n\nNote\n\nFor AIMS students: Billing is managed by AIMS centrally. You do not need to:\n\nAdd a credit card\n\nManage billing accounts directly\n\nWorry about unexpected charges (within approved quotas)\n\nHowever, you should still:\n\nMonitor your resource usage\n\nClean up resources when done\n\nFollow cost-conscious practices","type":"content","url":"/part1-gcp-foundations/understanding-gcp#id-1-billing-accounts","position":19},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"2. Quotas","lvl2":"Core GCP Concepts"},"type":"lvl3","url":"/part1-gcp-foundations/understanding-gcp#id-2-quotas","position":20},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"2. Quotas","lvl2":"Core GCP Concepts"},"content":"Quotas limit resource usage in a project. Google Cloud implements quotas to:\n\nPrevent accidental overspending\n\nEnsure fair resource allocation/distribution\n\nProtect Google Cloud against abuse\n\nCommon quotas:\n\nGPU quotas: Number of GPUs per region\n\nCPU quotas: Number of vCPUs\n\nPersistent disk: Total disk space\n\nAPI rate limits: Requests per minute\n\nImportant\n\nWhy Quotas matter?: You might try to launch a GPU virtual machine and get an error like ‚ÄúQuota ‚ÄòGPUS_ALL_REGIONS‚Äô exceeded‚Äù even though you have permissions. You‚Äôll need to request a quota increase.\n\nImportant\n\nTo check your quotas on the GCP console:\n\nGo to Console ‚Üí Navigation Menu ‚Üí IAM & Admin ‚Üí Quotas and system limits.\n\nUse the filter to select a specific API or service, for example, the Compute Engine API.\n\nOnce selected, you will see the available quotas for that service, such as the number of A100 80GB GPUs, CPUs, or other resources.","type":"content","url":"/part1-gcp-foundations/understanding-gcp#id-2-quotas","position":21},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"3. Regions and Zones","lvl2":"Core GCP Concepts"},"type":"lvl3","url":"/part1-gcp-foundations/understanding-gcp#id-3-regions-and-zones","position":22},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"3. Regions and Zones","lvl2":"Core GCP Concepts"},"content":"GCP resources are deployed in specific geographic locations.\n\nRegion: A specific, independent, and secure geographic location used to deploy resources. Examples include us-central1 (Iowa, USA), europe-west4 (Netherlands), asia-southeast1 (Singapore).\n\nZones: A deployment area within a region. A region is a collection of zones. Zones have high-bandwidth, low-latency network connections to other zones in the same region. For example, the us-central1 region can have zones like us-central1-a, us-central1-b, us-central1-c.\n\nWhy this matters?\n\nLatency: Always choose regions closer to you\n\nHardware availability: Not all GPUs are available everywhere.\n\nPricing: Costs vary by region. Check \n\npricing calculator for exact costs.\n\nQuotas: Quotas are set by region. You might have a GPU quota in us-central1 but not in europe-west4.\n\nReliability: Deploying across multiple zones within a region improves fault tolerance and reduces downtime.\n\nRecommended regions for AIMS students:\n\nRegion\n\nLocation\n\nPros\n\nCons\n\neurope-west4\n\nNetherlands\n\nGood latency from Africa, good GPU availability\n\nSlightly higher cost\n\nus-central1\n\nIowa, USA\n\nBest GPU availability, often cheaper\n\nHigher latency from Africa\n\nasia-southeast1\n\nSingapore\n\nModerate latency, good availability\n\nModerate pricing\n\nStart with us-central1 for this tutorial - it has the best GPU availability for learning.","type":"content","url":"/part1-gcp-foundations/understanding-gcp#id-3-regions-and-zones","position":23},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"4. APIs and Services","lvl2":"Core GCP Concepts"},"type":"lvl3","url":"/part1-gcp-foundations/understanding-gcp#id-4-apis-and-services","position":24},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"4. APIs and Services","lvl2":"Core GCP Concepts"},"content":"GCP functionality is accessed through APIs. Before using a service, you need to enable its API for your project.\n\nCommon APIs you may need to enable:\n\nCompute Engine API: To create virtual machines\n\nVertex AI API: To use managed ML services\n\nCloud Storage API: To use object storage services\n\nCloud Build API: To use container building services.\n\nTo enable an API:\n\nGo to Console ‚Üí APIs & Services ‚Üí Enable APIs and Services\n\nFilter to select the specific API e.g. Vertex AI API\n\nClick on the Enable API button to enable the API.","type":"content","url":"/part1-gcp-foundations/understanding-gcp#id-4-apis-and-services","position":25},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"5. Identity and Access Management (IAM)","lvl2":"Core GCP Concepts"},"type":"lvl3","url":"/part1-gcp-foundations/understanding-gcp#id-5-identity-and-access-management-iam","position":26},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl3":"5. Identity and Access Management (IAM)","lvl2":"Core GCP Concepts"},"content":"IAM controls who can do what in GCP.\n\nKey concepts:\n\nPrincipal: Who (user, service account)\n\nExample: you@aims.ac.za\n\nRole: What permissions\n\nOwner: Full control\n\nEditor: Can modify resources\n\nViewer: Read-only access\n\nResource: Where (project, VM, bucket)\n\nNote\n\nFor AIMS students: Your permissions are managed by AIMS IT. If you need additional permissions, please contact AIMS IT support.\n\nBest practices:\n\nUse least privilege (use only the minimum permissions needed)\n\nDon‚Äôt share credentials\n\nUse service accounts for automated tasks","type":"content","url":"/part1-gcp-foundations/understanding-gcp#id-5-identity-and-access-management-iam","position":27},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"üîë Key Takeaways"},"type":"lvl2","url":"/part1-gcp-foundations/understanding-gcp#id-key-takeaways","position":28},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"üîë Key Takeaways"},"content":"‚úÖ Projects are your main workspace\n\n‚úÖ Quotas can block you even with permissions - check and request quota increases early\n\n‚úÖ Regions matter for latency, availability, and cost\n\n‚úÖ APIs must be enabled before use\n\n‚úÖ AIMS manages billing - you focus on research, not payment setup.","type":"content","url":"/part1-gcp-foundations/understanding-gcp#id-key-takeaways","position":29},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"üöÄ What‚Äôs Next"},"type":"lvl2","url":"/part1-gcp-foundations/understanding-gcp#id-whats-next","position":30},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"üöÄ What‚Äôs Next"},"content":"Now that you understand GCP‚Äôs core concepts, let‚Äôs explore the console interface in detail.","type":"content","url":"/part1-gcp-foundations/understanding-gcp#id-whats-next","position":31},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"üìö References & Further Reading"},"type":"lvl2","url":"/part1-gcp-foundations/understanding-gcp#id-references-further-reading","position":32},{"hierarchy":{"lvl1":"GCP Core Concepts","lvl2":"üìö References & Further Reading"},"content":"Google Cloud Resource Hierarchy\n\nUnderstanding Projects\n\nCloud Quotas\n\nRegions and Zones\n\nIAM Overview\n\nCloud computing is the on-demand delivery of IT resources like storage and processing power over the internet on a pay-as-you-go basis, allowing businesses and individuals to reduce costs and scale rapidly without owning or maintaining physical infrastructure.","type":"content","url":"/part1-gcp-foundations/understanding-gcp#id-references-further-reading","position":33},{"hierarchy":{"lvl1":"The GCP Console"},"type":"lvl1","url":"/part1-gcp-foundations/gcp-console","position":0},{"hierarchy":{"lvl1":"The GCP Console"},"content":"The GCP Console is a web-based graphical interface used to deploy, manage, and monitor all your cloud resources and services from a central dashboard. It provides a user-friendly way to handle everything from virtual machines and databases to billing and security without needing to use command-line tools.","type":"content","url":"/part1-gcp-foundations/gcp-console","position":1},{"hierarchy":{"lvl1":"The GCP Console","lvl2":"Accessing the Console"},"type":"lvl2","url":"/part1-gcp-foundations/gcp-console#accessing-the-console","position":2},{"hierarchy":{"lvl1":"The GCP Console","lvl2":"Accessing the Console"},"content":"The GCP console can be accessed at: \n\nhttps://‚Äãconsole‚Äã.cloud‚Äã.google‚Äã.com\n\nNote\n\nFor AIMS students, an account has already been created and linked to your @aims.ac.za email address; simply log in using your institutional credentials. If you are not an AIMS student and do not have a Google Cloud account, navigate to \n\ncloud.google.com and click ‚ÄúGet started for free‚Äù to create one.","type":"content","url":"/part1-gcp-foundations/gcp-console#accessing-the-console","position":3},{"hierarchy":{"lvl1":"The GCP Console","lvl2":"Console Layout"},"type":"lvl2","url":"/part1-gcp-foundations/gcp-console#console-layout","position":4},{"hierarchy":{"lvl1":"The GCP Console","lvl2":"Console Layout"},"content":"This is an overview of what the GCP console looks like:\n\n\n\nScreenshot of my GCP console","type":"content","url":"/part1-gcp-foundations/gcp-console#console-layout","position":5},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Top Navigation Bar","lvl2":"Console Layout"},"type":"lvl3","url":"/part1-gcp-foundations/gcp-console#top-navigation-bar","position":6},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Top Navigation Bar","lvl2":"Console Layout"},"content":"‚ò∞ Navigation menu (left): Access all GCP services\n\nProject selector (top-left): Switch between projects\n\nSearch bar (center): Find services or resources (shortcut: /)\n\nCloud Shell (>_) (right): Browser-based terminal\n\nAccount icon (right): Settings and logout","type":"content","url":"/part1-gcp-foundations/gcp-console#top-navigation-bar","position":7},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Navigation Menu","lvl2":"Console Layout"},"type":"lvl3","url":"/part1-gcp-foundations/gcp-console#navigation-menu","position":8},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Navigation Menu","lvl2":"Console Layout"},"content":"The navigation menu is the primary way to discover and switch between the 100+ services offered by Google Cloud. You can access this by clicking the three horizontal lines (‚ò∞) in the top-left corner of the console.\n\nYou can find services like Billing, IAM & Admin, Vertex AI, and Compute Engine in the Products section of the menu. To keep your most-used tools at the top of the sidebar, you can click the Star icon (‚òÜ) next to any service to add it to your Favorites list for instant access.","type":"content","url":"/part1-gcp-foundations/gcp-console#navigation-menu","position":9},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Home Dashboard","lvl2":"Console Layout"},"type":"lvl3","url":"/part1-gcp-foundations/gcp-console#home-dashboard","position":10},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Home Dashboard","lvl2":"Console Layout"},"content":"What you see:\n\nProject name and ID\n\nNavigation menu\n\nQuick action buttons e.g. ‚ÄúCreate VM‚Äù, ‚ÄúRun a query in BigQuery‚Äù, ‚ÄúDeploy an application‚Äù, ‚ÄúCreate a storage bucket‚Äù\n\nQuick access section for commonly accessed services e.g. ‚ÄúAPIs and services‚Äù, ‚ÄúIAM and admin‚Äù, ‚ÄúBilling‚Äù, ‚ÄúCompute Engine‚Äù, ‚ÄúCloud Storage‚Äù, ‚ÄúBigQuery‚Äù, ‚ÄúVPC Network‚Äù, ‚ÄúKubernetes Engine‚Äù","type":"content","url":"/part1-gcp-foundations/gcp-console#home-dashboard","position":11},{"hierarchy":{"lvl1":"The GCP Console","lvl2":"Examples"},"type":"lvl2","url":"/part1-gcp-foundations/gcp-console#examples","position":12},{"hierarchy":{"lvl1":"The GCP Console","lvl2":"Examples"},"content":"","type":"content","url":"/part1-gcp-foundations/gcp-console#examples","position":13},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Enabling APIs and Services","lvl2":"Examples"},"type":"lvl3","url":"/part1-gcp-foundations/gcp-console#enabling-apis-and-services","position":14},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Enabling APIs and Services","lvl2":"Examples"},"content":"Before using most Google Cloud tools, you must enable their specific API.\n\nOpen the navigation menu (‚ò∞) and select APIs & Services > Enabled APIs & services\n\nClick on the API you want to enable e.g. ‚ÄúCompute Engine API‚Äù\n\nClick on the ‚ÄúEnable API‚Äù button to enable the API","type":"content","url":"/part1-gcp-foundations/gcp-console#enabling-apis-and-services","position":15},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Accessing the Compute Engine Service","lvl2":"Examples"},"type":"lvl3","url":"/part1-gcp-foundations/gcp-console#accessing-the-compute-engine-service","position":16},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Accessing the Compute Engine Service","lvl2":"Examples"},"content":"To access the compute engine service on the console:\n\nClick on ‚ÄúCompute Engine‚Äù via the navigation menu or quick access section\n\nClick on ‚ÄúVM instances‚Äù on the sidebar to see your list of virtual machine instances\n\nClick on ‚ÄúCreate instance‚Äù to create a new VM instance\n\nHere, you can also start/stop/delete VMs, SSH into your VMs, configure and monitor resources.","type":"content","url":"/part1-gcp-foundations/gcp-console#accessing-the-compute-engine-service","position":17},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Accessing Vertex AI","lvl2":"Examples"},"type":"lvl3","url":"/part1-gcp-foundations/gcp-console#accessing-vertex-ai","position":18},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Accessing Vertex AI","lvl2":"Examples"},"content":"For machine learning and AI development:\n\nNavigate to ‚ÄúVertex AI‚Äù from the navigation menu.\n\nClick on ‚ÄúDashboard‚Äù to see your recent activity and current costs for models like Gemini 3.0 Pro.\n\nUse the sidebar to access specialized tools like Vertex AI Studio for prompting or Colab Enterprise for coding in notebooks.\n\nHere, you can also create and manage datasets, run experiments, deploy models, generate API keys specifically for model deployments, etc.","type":"content","url":"/part1-gcp-foundations/gcp-console#accessing-vertex-ai","position":19},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Managing Billing","lvl2":"Examples"},"type":"lvl3","url":"/part1-gcp-foundations/gcp-console#managing-billing","position":20},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Managing Billing","lvl2":"Examples"},"content":"To keep track of your AIMS GCP credits and spending:\n\nSelect Billing from the navigation menu or quick access tiles.\n\nNote\n\nFor AIMS students, you will need to click on ‚ÄúGo to linked billing account‚Äù because the billing account linked to your project is managed by AIMS.\n\nClick on the Reports tab in the Cost Management section of the sidebar for a detailed visual breakdown of which services are using your budget.\n\nClick on the Budgets & alerts tab in the sidebar to create budgets and alerts. This can be used to receive an email notification if your spending reaches a specific threshold (e.g., 10% of your credits.)","type":"content","url":"/part1-gcp-foundations/gcp-console#managing-billing","position":21},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"IAM and Admin","lvl2":"Examples"},"type":"lvl3","url":"/part1-gcp-foundations/gcp-console#iam-and-admin","position":22},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"IAM and Admin","lvl2":"Examples"},"content":"This section controls ‚ÄúIdentity and Access Management‚Äù (who can do what).\n\nNavigate to IAM & Admin > IAM from the navigation menu\n\nYou can see the principals and their roles.\n\nClick Grant Access to add a new Principal to your project using their email.\n\nAssign a Role (like Viewer, Editor, or Owner) to that principal to define their level of access.\n\nHere, you can also: create labels and tags (accessible from the sidebar), create service accounts for automated scripts, create roles and policies, view audit logs to see who made changes, and check Quotas and system limits to see the resource limits for your project.","type":"content","url":"/part1-gcp-foundations/gcp-console#iam-and-admin","position":23},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Using the Cloud Shell","lvl2":"Examples"},"type":"lvl3","url":"/part1-gcp-foundations/gcp-console#using-the-cloud-shell","position":24},{"hierarchy":{"lvl1":"The GCP Console","lvl3":"Using the Cloud Shell","lvl2":"Examples"},"content":"Cloud Shell is a browser-based Linux terminal in your console that comes with Cloud SDK gcloud, Cloud Code, an online Code Editor and other utilities pre-installed, fully authenticated and up to date, and is free for all users.\n\nYou can click on the terminal icon (>_) in the top-right toolbar to open the Cloud Shell. The shell will open from the bottom of the page.","type":"content","url":"/part1-gcp-foundations/gcp-console#using-the-cloud-shell","position":25},{"hierarchy":{"lvl1":"The GCP Console","lvl4":"Testing your Cloud Shell","lvl3":"Using the Cloud Shell","lvl2":"Examples"},"type":"lvl4","url":"/part1-gcp-foundations/gcp-console#testing-your-cloud-shell","position":26},{"hierarchy":{"lvl1":"The GCP Console","lvl4":"Testing your Cloud Shell","lvl3":"Using the Cloud Shell","lvl2":"Examples"},"content":"To see your active account (Principal):gcloud auth list\n\n*This should show your @aims.ac.za email with an asterisk next to ACTIVE, confirming you are the active user.\n\nTo print your current Compute zone:gcloud config get-value compute/zone\n\nNote: If this returns nothing, it just means a default zone hasn‚Äôt been set yet, which is normal for new projects.\n\nTo list available regions:gcloud compute regions list\n\n\n\nScreenshot of Cloud Shell on GCP\n\nTip\n\nWith the Cloud Shell, you can also:\n\nYou can use other command-line tools like gsutil to move files.\n\nYou can use the Cloud Shell Editor (a VSCode-like editor) within the shell for editing scripts and configuration files.\n\nEvery student gets 5 GB of persistent home directory storage, meaning your files and settings stay there even after you close the session.\n\nYou can run a web application (like a Flask or FastAPI app) in the shell and preview it on a secure URL using the Web Preview button.","type":"content","url":"/part1-gcp-foundations/gcp-console#testing-your-cloud-shell","position":27},{"hierarchy":{"lvl1":"The GCP Console","lvl2":"üîë Key Takeaways"},"type":"lvl2","url":"/part1-gcp-foundations/gcp-console#id-key-takeaways","position":28},{"hierarchy":{"lvl1":"The GCP Console","lvl2":"üîë Key Takeaways"},"content":"‚úÖ The GCP console can be used to interact with your GCP account, services, and resources in a user-friendly and intuitive manner.\n\n‚úÖ Most services will not work until you explicitly Enable the API in the ‚ÄúAPIs & Services‚Äù section.\n\n‚úÖ Cloud Shell is your ‚ÄúPower Tool‚Äù. It is more than just a command line; it includes a built-in Code Editor, 5 GB of persistent storage, and pre-authenticated command-line utilities like gcloud and gsutil","type":"content","url":"/part1-gcp-foundations/gcp-console#id-key-takeaways","position":29},{"hierarchy":{"lvl1":"The GCP Console","lvl2":"üöÄ What‚Äôs Next?"},"type":"lvl2","url":"/part1-gcp-foundations/gcp-console#id-whats-next","position":30},{"hierarchy":{"lvl1":"The GCP Console","lvl2":"üöÄ What‚Äôs Next?"},"content":"Now that you can navigate the Console, let‚Äôs learn the gcloud CLI for command-line control.","type":"content","url":"/part1-gcp-foundations/gcp-console#id-whats-next","position":31},{"hierarchy":{"lvl1":"The GCP Console","lvl2":"References & Further Reading"},"type":"lvl2","url":"/part1-gcp-foundations/gcp-console#references-further-reading","position":32},{"hierarchy":{"lvl1":"The GCP Console","lvl2":"References & Further Reading"},"content":"Google Cloud Console Overview\n\nCloud Shell Documentation\n\nGoogle Cloud Documentation\n\nKeyboard Shortcuts for Google Cloud\n\nA principal is an entity that can be granted access to a Google Cloud resource. When you ‚ÄúGrant Access‚Äù in IAM, you are essentially telling Google Cloud which Principal has permission to perform specific actions.","type":"content","url":"/part1-gcp-foundations/gcp-console#references-further-reading","position":33},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)"},"type":"lvl1","url":"/part1-gcp-foundations/the-gcloud-cli","position":0},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)"},"content":"While the GCP Console is great for interacting with GCP through a graphical interface, the gcloud CLI (Command Line Interface) is the primary tool for automating tasks and managing resources from your terminal.\n\nIn a nutshell, it allows you to interact with GCP services through text commands instead of clicking through the web interface. The gcloud CLI is also part of the Google Cloud SDK.","type":"content","url":"/part1-gcp-foundations/the-gcloud-cli","position":1},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"Why Use the CLI?"},"type":"lvl2","url":"/part1-gcp-foundations/the-gcloud-cli#why-use-the-cli","position":2},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"Why Use the CLI?"},"content":"Speed: Perform complex tasks with a single command rather than clicking through multiple menus.\n\nAutomation: Write scripts to automate repetitive tasks like deploying dozens of virtual machines or storage buckets at once with scripts instead of manual clicking.\n\nConsistency: Ensure that resources are created with the exact same configurations every time.","type":"content","url":"/part1-gcp-foundations/the-gcloud-cli#why-use-the-cli","position":3},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"When To Use the CLI?"},"type":"lvl2","url":"/part1-gcp-foundations/the-gcloud-cli#when-to-use-the-cli","position":4},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"When To Use the CLI?"},"content":"‚úÖ Automation and scripting: Perfect for repetitive tasks that you don‚Äôt want to do by hand.\n\n‚úÖ Reproducible infrastructure setups: Ensure your environment is the same every time you deploy.\n\n‚úÖ Batch operations: Manage multiple resources (like 50 VMs) with a single command.\n\n‚úÖ Version-controlled infrastructure: Save your setup commands in a Git repository.\n\n‚úÖ Integration with other tools: Easily connect your cloud resources with Python scripts, CI/CD pipelines, or Git.\n\nNote\n\nUsing the console might, however, be more intuitive for beginners or more efficient for one-off tasks, such as learning and exploring GCP, quickly checking a single billing report, creating a single virtual machine instance, creating a single storage bucket, debugging and troubleshooting, etc.","type":"content","url":"/part1-gcp-foundations/the-gcloud-cli#when-to-use-the-cli","position":5},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"Where to Use It?"},"type":"lvl2","url":"/part1-gcp-foundations/the-gcloud-cli#where-to-use-it","position":6},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"Where to Use It?"},"content":"Cloud Shell: The gcloud tool is pre-installed and authenticated in the browser-based Cloud Shell terminal we explored in the previous chapter. No installation required.\n\nLocal Machine: You can install the Google Cloud SDK on your own laptop (Windows, macOS, or Linux) to manage your cloud projects without opening a browser.","type":"content","url":"/part1-gcp-foundations/the-gcloud-cli#where-to-use-it","position":7},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"Key Capabilities"},"type":"lvl2","url":"/part1-gcp-foundations/the-gcloud-cli#key-capabilities","position":8},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"Key Capabilities"},"content":"Manage resources: Create, modify, delete resources like VMs, storage, and networks.\n\nConfigure projects: Quickly switch between projects or set a default project, region, and zone.\n\nAuthenticate: Log in and manage credentials for yourself or for Service Accounts.\n\nAutomate workflows: Write Bash or Python scripts to automate repetitive cloud infrastructure tasks.\n\nQuery resources: List and filter resources (e.g., ‚ÄúFind all VMs that are currently stopped‚Äù) and format the output as JSON, YAML, or a table.\n\nControl permissions: Manage IAM roles and policies to decide who has access to what.","type":"content","url":"/part1-gcp-foundations/the-gcloud-cli#key-capabilities","position":9},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"Anatomy of a gcloud Command"},"type":"lvl2","url":"/part1-gcp-foundations/the-gcloud-cli#anatomy-of-a-gcloud-command","position":10},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"Anatomy of a gcloud Command"},"content":"Most commands follow a simple structure:gcloud [GROUP] [SUBGROUP] [ACTION] [NAME] --[FLAGS]\n\ngcloud: The base command that calls the Google Cloud SDK.\n\n[GROUP]: The broad service or resource category you are targeting (e.g., compute, storage, auth, projects).\n\n[SUBGROUP]: (Optional) A specific type of resource within that group. For example, under compute group, you might target instances, disks, or networks.\n\n[ACTION]: The ‚Äúverb‚Äù or what you want to do to the resource (e.g., create, list, delete, describe, stop).\n\n[NAME]: The specific identity of the resource you are acting upon (e.g., the name of your VM instance).\n\n--[FLAGS]: Optional parameters that modify the command, usually starting with double dashes. These specify details like --zone, --machine-type, or --format.","type":"content","url":"/part1-gcp-foundations/the-gcloud-cli#anatomy-of-a-gcloud-command","position":11},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl3":"Example in Action","lvl2":"Anatomy of a gcloud Command"},"type":"lvl3","url":"/part1-gcp-foundations/the-gcloud-cli#example-in-action","position":12},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl3":"Example in Action","lvl2":"Anatomy of a gcloud Command"},"content":"If you want to create a virtual machine, the command looks like this:\n\nPart\n\nCommand Fragment\n\nPurpose\n\nGroup\n\ncompute\n\nWe are working with the compute engine service\n\nSubgroup\n\ninstances\n\nSpecifically, we want a virtual machine\n\nAction\n\ncreate\n\nWe want to create a new virtual machine instance\n\nName\n\naims-vm\n\nThis is the name we are giving the instance\n\nFlag\n\n--zone=us-central1-a\n\nThis tells GCP what zone to create the instance\n\nThe full command is:gcloud compute instances create aims-vm --zone=us-central1-a","type":"content","url":"/part1-gcp-foundations/the-gcloud-cli#example-in-action","position":13},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"üîë Key Takeaways"},"type":"lvl2","url":"/part1-gcp-foundations/the-gcloud-cli#id-key-takeaways","position":14},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"üîë Key Takeaways"},"content":"While the console is great for exploring, the CLI is the superior choice for automating repetitive tasks, batch operations, and creating reproducible environments.\n\nYou don‚Äôt need to install anything immediately; the Cloud Shell provides a fully authenticated, pre-configured gcloud environment directly in your browser.\n\nEvery command follows a logical structure with gcloud (the tool), Group (the service), Subgroup (the resource), and Action (the verb).","type":"content","url":"/part1-gcp-foundations/the-gcloud-cli#id-key-takeaways","position":15},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"üöÄ What‚Äôs Next?"},"type":"lvl2","url":"/part1-gcp-foundations/the-gcloud-cli#id-whats-next","position":16},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"üöÄ What‚Äôs Next?"},"content":"We are moving into the practical hands-on phase of the GCP foundations part. In the next two chapters, you will complete structured hands-on tutorials:\n\nHands-on Tutorial 1: Using the Console\n\nCreate a VM: Manually configure and launch a virtual machine using the Compute Engine dashboard.\n\nBrowser-based SSH: Connect to your instance directly through the Google Cloud console.\n\nHands-on Tutorial 2: Using the gcloud CLI\n\nLocal Setup: Install and initialize the Google Cloud SDK on your own computer.\n\nAutomation: Use gcloud to create VMs, manage SSH keys from the terminal, and set up a Python environment.\n\nScripting: Write Bash scripts to create and stop multiple VMs at once to manage your credits efficiently.","type":"content","url":"/part1-gcp-foundations/the-gcloud-cli#id-whats-next","position":17},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"üìö References & Further Reading"},"type":"lvl2","url":"/part1-gcp-foundations/the-gcloud-cli#id-references-further-reading","position":18},{"hierarchy":{"lvl1":"The Google Cloud CLI (gcloud)","lvl2":"üìö References & Further Reading"},"content":"gcloud CLI Overview - The official documentation. A great starting point.\n\ngcloud CLI Cheat Sheet - A high-value reference for the most common commands.\n\nCloud Shell Guide - Deeper details on the browser-based terminal.\n\nSDK Installation Steps - Official guides for Windows, macOS, and Linux.","type":"content","url":"/part1-gcp-foundations/the-gcloud-cli#id-references-further-reading","position":19},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console"},"type":"lvl1","url":"/part1-gcp-foundations/hands-on-gcp-console","position":0},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console"},"content":"In this hands-on tutorial, you will create your first GPU-powered virtual machine using the GCP Console web interface.","type":"content","url":"/part1-gcp-foundations/hands-on-gcp-console","position":1},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl2":"Step 1: Create a Virtual Private Cloud (VPC) Network"},"type":"lvl2","url":"/part1-gcp-foundations/hands-on-gcp-console#step-1-create-a-virtual-private-cloud-vpc-network","position":2},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl2":"Step 1: Create a Virtual Private Cloud (VPC) Network"},"content":"Every virtual machine requires a network to function. A \n\nVirtual Private Cloud (VPC) is a secure, isolated private network hosted within Google Cloud. It acts as the virtual equivalent of a physical network in a traditional data center. Specifically, a VPC network does the following:\n\nProvides connectivity for your Compute Engine virtual machine (VM) instances.\n\nEnables communication between your VMs, other Google Cloud services, and the public internet.\n\nActs as a secure boundary, allowing you to control exactly what traffic can enter or leave your network using firewall rules.\n\nTo ensure your VM has the correct network configuration and to prevent creation errors, we will manually create a VPC first.\n\nBasic Cloud Networking Concepts\n\nBefore diving into creating the VPC and virtual machine, let‚Äôs clarify a few essential concepts that are key to cloud networking, as understanding these will ensure your VM is both secure and accessible:\n\nVPC: The overarching network for your Google Cloud project. VPC networks, including their associated routes and firewall rules, are global resources. They are not associated with any particular region or zone.\n\nSubnet (Subnetwork): A division of your VPC. Subnets are regional resources. When you create a VM, you attach it to a specific subnet, which assigns the VM its internal IP address from a defined range.\n\nFirewall Rules: Security policies that control what traffic is allowed into (ingress) or out of (egress) your VPC. By default, VPC networks block all incoming traffic.\n\nCloud NAT (Network Address Translation): In enterprise environments, VMs are often kept fully private without external IP addresses for security. Cloud NAT allows these strictly private VMs to reach the internet to download software updates without exposing them to incoming internet traffic.\n\nCloud Router: Works alongside Cloud NAT to manage the dynamic routing of traffic between your private subnets and the outside world.\n\nFor this specific tutorial, we will assign a temporary external IP directly to our VM so we can connect to it easily. Therefore, we do not need to configure Cloud NAT or Cloud Router here.\n\nOpen the Navigation menu (‚ò∞) and go to VPC network ‚Üí VPC networks.\n\nClick the Create VPC Network button\n\nName: Enter demo-vpc, or whatever name you want.\n\nSubnet creation mode: Select Automatic. This instructs Google Cloud to automatically create a subnet with a predefined IP range in every available region.\n\nFirewall rules: Select the checkboxes for allow-ssh, allow-icmp, and allow-custom. Opening the SSH port (port 22) is absolutely required so you can log into your VM later.\n\nClick Create and wait for the status icon to show it is ready.\n\n\n\nScreenshot of VPC creation step A (configure VPC details and subnet creation mode)\n\n\n\nScreenshot: VPC creation step B (configure firewall rules and click ‚Äúcreate‚Äù)","type":"content","url":"/part1-gcp-foundations/hands-on-gcp-console#step-1-create-a-virtual-private-cloud-vpc-network","position":3},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl2":"Step 2: Navigate to Compute Engine on the GCP Console"},"type":"lvl2","url":"/part1-gcp-foundations/hands-on-gcp-console#step-2-navigate-to-compute-engine-on-the-gcp-console","position":4},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl2":"Step 2: Navigate to Compute Engine on the GCP Console"},"content":"Open the Navigation menu (‚ò∞) in top-left\n\nGo to Compute Engine ‚Üí VM instances\n\nIf the Compute Engine API is not enabled, click Enable and wait a few moments. You can also enable the Compute Engine API from the APIs & Services page.\n\n\n\nScreenshot of ‚ÄúCreate instance‚Äù page","type":"content","url":"/part1-gcp-foundations/hands-on-gcp-console#step-2-navigate-to-compute-engine-on-the-gcp-console","position":5},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl2":"Step 3: Create and Configure the Instance"},"type":"lvl2","url":"/part1-gcp-foundations/hands-on-gcp-console#step-3-create-and-configure-the-instance","position":6},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl2":"Step 3: Create and Configure the Instance"},"content":"Click the Create instance button at the top of the Compute Engine dashboard. This will take you to the instance configuration page where you can configure the machine configuration, operating system (OS) and storage, networking, security, etc.","type":"content","url":"/part1-gcp-foundations/hands-on-gcp-console#step-3-create-and-configure-the-instance","position":7},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl3":"For Machine Configuration","lvl2":"Step 3: Create and Configure the Instance"},"type":"lvl3","url":"/part1-gcp-foundations/hands-on-gcp-console#for-machine-configuration","position":8},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl3":"For Machine Configuration","lvl2":"Step 3: Create and Configure the Instance"},"content":"Enter a name for the virtual machine instance i.e. aims-gcp-tutorial-1 and select a region i.e. us-central1 (Iowa) and zone.\n\nTo configure the machine type, select the GPUs tab and choose NVIDIA T4 under GPU type and 1 under the Number of GPUs dropdown to use 1 GPU. This means we will be working with a single NVIDIA T4 GPU.\n\nNote\n\nThe console categorizes hardware into ‚ÄúMachine Families‚Äù based on your needs:\n\nGeneral-purpose (E2, N2, N1): The best price-performance ratio for everyday balanced workloads.\n\nCompute-optimized: Designed for high-performance computing (HPC) and CPU-intensive tasks.\n\nMemory-optimized: Ideal for memory-intensive workloads like large in-memory databases.\n\nStorage-optimized: Best for high-density storage and high-throughput data access.\n\nGPUs: Essential to access GPU hardware accelerators for AI/ML experiments.\n\n\n\nScreenshot of ‚ÄúMachine configuration‚Äù page","type":"content","url":"/part1-gcp-foundations/hands-on-gcp-console#for-machine-configuration","position":9},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl3":"For OS and Storage Configuration","lvl2":"Step 3: Create and Configure the Instance"},"type":"lvl3","url":"/part1-gcp-foundations/hands-on-gcp-console#for-os-and-storage-configuration","position":10},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl3":"For OS and Storage Configuration","lvl2":"Step 3: Create and Configure the Instance"},"content":"On the sidebar, click on OS and Storage. This will allow you to configure the operating system.\n\nClick on Switch Image to configure the OS image for the boot disk.\n\nIn the Operating system dropdown under ‚ÄúPublic images‚Äù, select Deep Learning on Linux to use a\n\nIn the Version dropdown, select Deep Learning VM with CUDA 12.4 M124. You can also select a different CUDA version.\n\nSet the Boot disk type to Balanced persistent disk. This provides the best cost-to-performance ratio.\n\nClick Select to continue.\n\n\n\nScreenshot: ‚ÄúOS and Storage‚Äù configuration window","type":"content","url":"/part1-gcp-foundations/hands-on-gcp-console#for-os-and-storage-configuration","position":11},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl3":"Networking, Security, and Advanced Settings","lvl2":"Step 3: Create and Configure the Instance"},"type":"lvl3","url":"/part1-gcp-foundations/hands-on-gcp-console#networking-security-and-advanced-settings","position":12},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl3":"Networking, Security, and Advanced Settings","lvl2":"Step 3: Create and Configure the Instance"},"content":"Before clicking the Create button on the bottom of the page to create your first VM instance on GCP, click on Networking in the sidebar to link your VM to the network you created in Step 2.\n\nUnder the Firewall section, check Allow HTTP traffic and Allow HTTPS traffic. Checking these boxes opens ports 80 and 443, permitting inbound HTTP and HTTPS traffic to your VM. This is essential for exposing web-based applications hosted on your VM to the internet, allowing you to access tools like Jupyter Notebooks or web demos remotely via your browser.\n\nUnder Network interfaces, click the dropdown.\n\nYou can select the VPC network you just created i.e. demo-vpc for the network interface‚Äôs Network.\n\nEnsure External IPv4 address is set to Ephemeral. This assigns a public IP address so you can reach the VM from the internet.\n\n\n\nScreenshot: ‚ÄúNetworking‚Äù configuration page\n\nSecurity & Advanced: Here you can manage SSH keys or add startup scripts. For most tasks, the default service account settings are sufficient.\n\nTip\n\nAt the top right, look for the Equivalent Code button. This opens a panel showing the exact gcloud CLI command for the settings you just chose. This is a great way to learn the CLI syntax by seeing how the Console translates your clicks into code.\n\nAfter completing the configuration steps above, you can then click the Create button to create your first VM instance in GCP using the Console ü•≥. The creation process will take about ~1-2 minutes.","type":"content","url":"/part1-gcp-foundations/hands-on-gcp-console#networking-security-and-advanced-settings","position":13},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl2":"Step 4: The VM Instances Page"},"type":"lvl2","url":"/part1-gcp-foundations/hands-on-gcp-console#step-4-the-vm-instances-page","position":14},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl2":"Step 4: The VM Instances Page"},"content":"Once you click Create, your VM will appear on the list of VMs.\n\n\n\nScreenshot: VM instances list page with the SSH button outlined in red.\n\nNotice two critical pieces of information:\n\nInternal IP: Used for communication between different Google Cloud resources in your project. It is private and carries no data transfer costs.\n\nExternal IP: The public address used to reach your VM from the internet (e.g., via your laptop terminal).","type":"content","url":"/part1-gcp-foundations/hands-on-gcp-console#step-4-the-vm-instances-page","position":15},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl3":"Connecting to your VM via SSH","lvl2":"Step 4: The VM Instances Page"},"type":"lvl3","url":"/part1-gcp-foundations/hands-on-gcp-console#connecting-to-your-vm-via-ssh","position":16},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl3":"Connecting to your VM via SSH","lvl2":"Step 4: The VM Instances Page"},"content":"To access the VM, click the SSH button in the ‚ÄúConnect‚Äù column of the VMs list.\n\nA browser-based terminal window will open. Click Authorize when prompted to allow the connection.\n\nBecause we used a Deep Learning image, the system will automatically detect the GPU hardware. You will see a prompt: ‚ÄúWould you like to install the Nvidia driver? [y/n]‚Äù.\n\nType y and press Enter. The installation will take 3‚Äì5 minutes.\n\nOnce finished, verify the setup by running pwd to see your home directory and nvidia-smi to view the GPU status.\n\n\n\nScreenshot: SSH browser terminal showing the output of the pwd command and the nvidia-smi command","type":"content","url":"/part1-gcp-foundations/hands-on-gcp-console#connecting-to-your-vm-via-ssh","position":17},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl2":"Step 5: Stopping or Deleting your VM Istance"},"type":"lvl2","url":"/part1-gcp-foundations/hands-on-gcp-console#step-5-stopping-or-deleting-your-vm-istance","position":18},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl2":"Step 5: Stopping or Deleting your VM Istance"},"content":"As you may have noticed, the monthly estimate for keeping a this VM you just created running 24/7 is approximately 200 (at an hourly rate of about 0.35 for an N1 machine with a T4 GPU). This is quite a lot of money to lose.\n\nTo manage your budget, you must proactively manage the state of your VM from the VM instances page by stopping or deleting unused VMs.\n\nTo stop or delete a VM:\n\nGo to the VM instances page in the GCP Console.\n\nCheck the box next to your VM name.\n\nClick the Stop button to stop the VM or the Delete button to delete the VM.\n\nImportant\n\nStopping a VM is like turning off your laptop. Google stops charging you for the GPU and CPU usage. However, you will still incur a small monthly charge for the Disk storage (the ‚ÄúBalanced Persistent Disk‚Äù) because Google must keep your data saved so it‚Äôs there when you ‚ÄúTurn on‚Äù the VM again.\n\nDeleting a VM wipes the instance and its disk completely. You stop paying for everything. This is the best option once you have finished an experiment and have downloaded your results. Note: Once deleted, your code and data cannot be recovered unless you have backed them up elsewhere.","type":"content","url":"/part1-gcp-foundations/hands-on-gcp-console#step-5-stopping-or-deleting-your-vm-istance","position":19},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl2":"üöÄ What‚Äôs Next?"},"type":"lvl2","url":"/part1-gcp-foundations/hands-on-gcp-console#id-whats-next","position":20},{"hierarchy":{"lvl1":"Hands-On 01: Create a VM via GCP Console","lvl2":"üöÄ What‚Äôs Next?"},"content":"You have successfully created a GPU-enabled VM via the GCP Console using the Compute Engine service ü•≥. Beyond creating VMs, the console allows you to interact with a wide range of GCP services and resources.\n\nNow that you have mastered the interface, we will move to the terminal to learn how to automate these tasks using the command line","type":"content","url":"/part1-gcp-foundations/hands-on-gcp-console#id-whats-next","position":21},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)"},"type":"lvl1","url":"/part1-gcp-foundations/hands-on-gcloud-cli","position":0},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)"},"content":"While the GCP Console is great for clicking through settings, using the gcloud command-line interface (CLI) is much more powerful. It allows your work to be repeatable and scriptable, meaning you can recreate your exact environment with a single command instead of dozens of manual clicks. It also enables automating tasks, such as starting or deleting multiple machines at once.","type":"content","url":"/part1-gcp-foundations/hands-on-gcloud-cli","position":1},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl2":"Installing the Google Cloud CLI"},"type":"lvl2","url":"/part1-gcp-foundations/hands-on-gcloud-cli#installing-the-google-cloud-cli","position":2},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl2":"Installing the Google Cloud CLI"},"content":"You need the Google Cloud CLI installed on your local machine to use gcloud\n\nFollow the official guides to download and install it:\n\nInstall for Windows\n\nInstall for Linux\n\nInstall for macOS\n\nNote\n\nFor AIMS Students, The Google Cloud CLI is already installed on all AIMS Campus desktops. If you run into any issues, kindly contact the IT team.\n\nAfter installation, run the gcloud init command in your terminal to log in and select your project.gcloud init\n\nTry these basic commands to make sure your terminal is connected to your GCP account:# See which project you are currently using\ngcloud config list project\n\n# List the VMs you currently have\ngcloud compute instances list","type":"content","url":"/part1-gcp-foundations/hands-on-gcloud-cli#installing-the-google-cloud-cli","position":3},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Creating a Virtual Machine (VM)","lvl2":"Installing the Google Cloud CLI"},"type":"lvl3","url":"/part1-gcp-foundations/hands-on-gcloud-cli#creating-a-virtual-machine-vm","position":4},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Creating a Virtual Machine (VM)","lvl2":"Installing the Google Cloud CLI"},"content":"","type":"content","url":"/part1-gcp-foundations/hands-on-gcloud-cli#creating-a-virtual-machine-vm","position":5},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 1: Create a Secure Network (VPC)","lvl2":"Installing the Google Cloud CLI"},"type":"lvl3","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-1-create-a-secure-network-vpc","position":6},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 1: Create a Secure Network (VPC)","lvl2":"Installing the Google Cloud CLI"},"content":"This creates a Virtual Private Cloud (VPC) to isolate your computing resources within the project.export AIMSUSERNAME=\"your_aims_username\" # This is for AIMS students\ngcloud compute networks create ${AIMSUSERNAME}-vpc --subnet-mode=auto\n\nFor Windows Users\n\nThe commands in this tutorial are written for bash. If you are using a Windows computer, we highly recommend installing Git Bash or Ubuntu via WSL (Windows Subsystem for Linux) so you can run these commands directly without modification.\n\nIf you prefer to use your native Windows command shell, you must adapt how environment variables are set and retrieved for the commands/scripts in this tutorial. The Windows equivalent for the commands above would be:set AIMSUSERNAME=your_aims_username\ngcloud compute networks create %AIMSUSERNAME%-vpc --subnet-mode=auto","type":"content","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-1-create-a-secure-network-vpc","position":7},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 2: Create a Firewall Rule for SSH","lvl2":"Installing the Google Cloud CLI"},"type":"lvl3","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-2-create-a-firewall-rule-for-ssh","position":8},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 2: Create a Firewall Rule for SSH","lvl2":"Installing the Google Cloud CLI"},"content":"By default, all incoming traffic is blocked. This rule allows traffic on Port 22, which is required for SSH connections.gcloud compute firewall-rules create ${AIMSUSERNAME}-fw-ssh \\\n    --network=${AIMSUSERNAME}-vpc \\\n    --allow=tcp:22","type":"content","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-2-create-a-firewall-rule-for-ssh","position":9},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 3: Set Up Internet Access (Router and NAT)","lvl2":"Installing the Google Cloud CLI"},"type":"lvl3","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-3-set-up-internet-access-router-and-nat","position":10},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 3: Set Up Internet Access (Router and NAT)","lvl2":"Installing the Google Cloud CLI"},"content":"This allows your VM to access external resources (such as HuggingFace to download a model) without assigning a public External IP address to the instance.export REGION=\"europe-west4\" # This is your region\n\n# Create the Cloud Router\ngcloud compute routers create ${AIMSUSERNAME}-router-${REGION} \\\n    --network=${AIMSUSERNAME}-vpc \\\n    --region=${REGION}\n\n# Create the NAT configuration\ngcloud compute routers nats create ${AIMSUSERNAME}-nat-config \\\n    --router-region=${REGION} \\\n    --router=${AIMSUSERNAME}-router-${REGION} \\\n    --nat-all-subnet-ip-ranges \\\n    --auto-allocate-nat-external-ips","type":"content","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-3-set-up-internet-access-router-and-nat","position":11},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 4: Create the VM","lvl2":"Installing the Google Cloud CLI"},"type":"lvl3","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-4-create-the-vm","position":12},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 4: Create the VM","lvl2":"Installing the Google Cloud CLI"},"content":"This command provisions a virtual machine with an NVIDIA L4 GPU. The L4 is a modern, cost-effective mid-range GPU with 24GB RAM.export ZONE=\"europe-west4-a\"\nexport VM_NAME=\"${AIMSUSERNAME}-l4-vm\" # Any other name works!\n\ngcloud compute instances create ${VM_NAME} \\\n    --zone=${ZONE} \\\n    --machine-type=g2-standard-4 \\\n    --accelerator=\"type=nvidia-l4,count=1\" \\\n    --image-family=common-cu128-ubuntu-2204-nvidia-570 \\\n    --image-project=deeplearning-platform-release \\\n    --maintenance-policy=TERMINATE \\\n    --network=${AIMSUSERNAME}-vpc\n\nNote\n\nIf you need a more powerful GPU like an A100, you only need to change two flags in the command above:\n\n--machine-type: Change from g2-standard-4 to an A2 machine type (e.g., a2-highgpu-1g).\n\n--accelerator: Change from type=nvidia-l4,count=1 to type=nvidia-tesla-a100,count=1.\n\nQuery the VM status:\nVerify that the instance has been provisioned and is in the RUNNING state:gcloud compute instances list --filter=\"name=${VM_NAME}\"\n\n\n\nScreenshot: L4 GPU VM created and queried successfully using gcloud","type":"content","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-4-create-the-vm","position":13},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 5: Connecting to your VM via SSH","lvl2":"Installing the Google Cloud CLI"},"type":"lvl3","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-5-connecting-to-your-vm-via-ssh","position":14},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 5: Connecting to your VM via SSH","lvl2":"Installing the Google Cloud CLI"},"content":"You can access the VM securely through the private network via SSH using Identity-Aware Proxy (IAP). We include a port-forwarding flag (-L 8080:localhost:8080) so you can open JupyterLab in your local browser:gcloud compute ssh ${VM_NAME} \\\n    --zone=${ZONE} \\\n    --tunnel-through-iap \\\n    -- -L 8080:localhost:8080\n\nNow you can run commands like nvidia-smi on your VM to check the GPU information.\n\n\n\nScreenshot: L4 GPU VM successfully connected to via SSH using gcloud","type":"content","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-5-connecting-to-your-vm-via-ssh","position":15},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 6: Automating Multiple VMs with a Bash Script","lvl2":"Installing the Google Cloud CLI"},"type":"lvl3","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-6-automating-multiple-vms-with-a-bash-script","position":16},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 6: Automating Multiple VMs with a Bash Script","lvl2":"Installing the Google Cloud CLI"},"content":"The gcloud CLI enables automating large-scale tasks. You can use a bash script to launch multiple VM instances simultaneously for parallel experiments.\n\nScript to create 3 VMs:# This loop provisions 3 instances in the background\nfor i in {1..3}\ndo\n   gcloud compute instances create \"experiment-vm-$i\" \\\n       --zone=europe-west4-a \\\n       --machine-type=g2-standard-4 \\\n       --accelerator=\"type=nvidia-l4,count=1\" \\\n       --image-family=common-cu128-ubuntu-2204-nvidia-570 \\\n       --image-project=deeplearning-platform-release \\\n       --maintenance-policy=TERMINATE \\\n       --network=${AIMSUSERNAME}-vpc &\ndone\nwait\necho \"All VM instances have been deployed.\"\n\n\n\nScreenshot: Multiple L4 GPU VMs created and queried successfully using gcloud","type":"content","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-6-automating-multiple-vms-with-a-bash-script","position":17},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 7: Resource Cleanup","lvl2":"Installing the Google Cloud CLI"},"type":"lvl3","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-7-resource-cleanup","position":18},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl3":"Step 7: Resource Cleanup","lvl2":"Installing the Google Cloud CLI"},"content":"Danger\n\nTo terminate all charges and return your project to an empty state, you must delete the resources in the correct order.\n\nThis is important for cost management to avoid burning your GCP credits.\n\nPart A: Delete all VM instancesgcloud compute instances delete $(gcloud compute instances list --format=\"value(name)\") --quiet --zone=${ZONE}\n\n\n\nScreenshot: VMs deleted successfully.\n\nPart B: Delete Networking Infrastructure# 1. Delete the NAT and Cloud Router\ngcloud compute routers nats delete ${AIMSUSERNAME}-nat-config --router=${AIMSUSERNAME}-router-${REGION} --region=${REGION} --quiet\ngcloud compute routers delete ${AIMSUSERNAME}-router-${REGION} --region=${REGION} --quiet\n\n# 2. Delete the Firewall rule\ngcloud compute firewall-rules delete ${AIMSUSERNAME}-fw-ssh --quiet\n\n# 3. Delete the VPC Network\ngcloud compute networks delete ${AIMSUSERNAME}-vpc --quiet","type":"content","url":"/part1-gcp-foundations/hands-on-gcloud-cli#step-7-resource-cleanup","position":19},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl2":"üîë Key Takeaways"},"type":"lvl2","url":"/part1-gcp-foundations/hands-on-gcloud-cli#id-key-takeaways","position":20},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl2":"üîë Key Takeaways"},"content":"Using the gcloud CLI makes infrastructure deployment repeatable and scriptable, allowing you to provision complex networking and high-performance compute resources with a single command.\n\nUsing variables like AIMSUSERNAME, REGION, and ZONE ensures your scripts are flexible and easily adaptable.\n\nTo avoid residual charges for cost management, you must return the project to a blank state by deleting not only the VM instance but also the associated networking components like the Cloud NAT, Router, and VPC.","type":"content","url":"/part1-gcp-foundations/hands-on-gcloud-cli#id-key-takeaways","position":21},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl2":"üöÄ What‚Äôs Next"},"type":"lvl2","url":"/part1-gcp-foundations/hands-on-gcloud-cli#id-whats-next","position":22},{"hierarchy":{"lvl1":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","lvl2":"üöÄ What‚Äôs Next"},"content":"In the next part, we will run a full AI experiment on a GPU VM to fine-tune a small language model with 1 billion parameters (Gemma 1B). This hands-on session will introduce you to leveraging GCP services  to manage end-to-end machine learning workflows.","type":"content","url":"/part1-gcp-foundations/hands-on-gcloud-cli#id-whats-next","position":23},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/part1-gcp-foundations","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"Welcome to Part 1! In this section, you‚Äôll understand the core Google Cloud Platform (GCP) concepts, understand organizes resources, and learn to interact with GCP using both the web console and command-line interface.","type":"content","url":"/part1-gcp-foundations","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"üéØ Learning Objectives"},"type":"lvl2","url":"/part1-gcp-foundations#id-learning-objectives","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"üéØ Learning Objectives"},"content":"By the end of this part, you will:\n\nUnderstand how GCP works on a high-level and why it is necessary.\n\nUnderstand how GCP organizes resources (projects, billing, quotas).\n\nNavigate the GCP console confidently.\n\nCreate your first GCP project.\n\nInstall and configure the gcloud CLI.\n\nRun common commands using the gcloud CLI.","type":"content","url":"/part1-gcp-foundations#id-learning-objectives","position":3},{"hierarchy":{"lvl1":"Introduction","lvl2":"üìö What‚Äôs in This Part"},"type":"lvl2","url":"/part1-gcp-foundations#id-whats-in-this-part","position":4},{"hierarchy":{"lvl1":"Introduction","lvl2":"üìö What‚Äôs in This Part"},"content":"","type":"content","url":"/part1-gcp-foundations#id-whats-in-this-part","position":5},{"hierarchy":{"lvl1":"Introduction","lvl3":"Concepts","lvl2":"üìö What‚Äôs in This Part"},"type":"lvl3","url":"/part1-gcp-foundations#concepts","position":6},{"hierarchy":{"lvl1":"Introduction","lvl3":"Concepts","lvl2":"üìö What‚Äôs in This Part"},"content":"Understanding GCP Core Concepts: Learn how GCP thinks about resources, projects, billing accounts, quotas, and regions. Understanding these fundamentals will prevent confusion later.\n\nThe GCP Console: Get familiar with the web interface where you‚Äôll manage your cloud resources. We‚Äôll tour the key sections you‚Äôll use throughout this tutorial.\n\nThe gcloud CLI: Discover the command-line tool that gives you programmatic control over GCP. This is essential for automation and reproducible steps.","type":"content","url":"/part1-gcp-foundations#concepts","position":7},{"hierarchy":{"lvl1":"Introduction","lvl3":"Hands-On Practice","lvl2":"üìö What‚Äôs in This Part"},"type":"lvl3","url":"/part1-gcp-foundations#hands-on-practice","position":8},{"hierarchy":{"lvl1":"Introduction","lvl3":"Hands-On Practice","lvl2":"üìö What‚Äôs in This Part"},"content":"Hands-On: Set Up Your First Project in GCP Console: Create your first GCP project, enable necessary APIs, and verify your quotas. This is where your cloud journey begins!\n\nHands-On: Install and Use gcloud CLI: Install the gcloud command-line tool on your machine, authenticate, and run your first commands to explore GCP resources.\n\nReady? Let‚Äôs go üöÄ","type":"content","url":"/part1-gcp-foundations#hands-on-practice","position":9},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM"},"type":"lvl1","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms","position":0},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM"},"content":"In this hands-on session, we will fine-tune Gemma 3 1B (a small language model with 1 billion parameters) on the Dolly-15k instruction dataset using LoRA (Low-Rank Adaptation), track our experiments with Weights & Biases, and persist outputs and model artifacts to Google Cloud Storage.","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms","position":1},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Overview"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#overview","position":2},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Overview"},"content":"By the end of this hands-on session, you will have:\n\nProvisioned an L4 GPU VM using the Compute Engine service via gcloud CLI\n\nSet up a Python environment using uv\n\nCloned the experiment repository from GitHub\n\nRun the scripts to fine-tune Gemma 3 1B using LoRA on the Dolly 15k dataset\n\nCreated persistent SSH sessions using tmux\n\nLearnt how to monitor training in real-time using Weights & Biases\n\nPersisted outputs and model artifacts to Google Cloud Storage (GCS)\n\nCleaned up all resources to avoid unnecessary charges","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#overview","position":3},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Background"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#background","position":4},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Background"},"content":"Click here for more background information about this hands-on session.","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#background","position":5},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl3":"Why This Experiment?","lvl2":"Background"},"type":"lvl3","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#why-this-experiment","position":6},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl3":"Why This Experiment?","lvl2":"Background"},"content":"Fine-tuning takes an already capable model and adapts it to a specific task at a fraction of the cost of training from scratch, which is how most real-world LLM applications are built.\n\nWe use Gemma 3 1B because fine-tuning a model at this scale of one billion parameters introduces you to the practical realities of running large model training on cloud infrastructure, including GPU memory constraints, checkpoint management, and experiment tracking.\n\nAt 1B parameters, Gemma is large enough to produce meaningful, coherent responses and representative of the models used in real applications, but small enough to fine-tune on a single L4 GPU in under 30 minutes, making it a practical starting point.","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#why-this-experiment","position":7},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl3":"Why LoRA?","lvl2":"Background"},"type":"lvl3","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#why-lora","position":8},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl3":"Why LoRA?","lvl2":"Background"},"content":"Even at 1 Billion parameters, fully fine-tuning Gemma 1B would require updating every weight in the model on every training step, which is memory-intensive and slow on a single GPU.\n\nLoRA solves this by freezing the original model weights and injecting small trainable matrices into specific layers, training less than the full parameters while still achieving meaningful adaptation.\n\nThe key hyperparameter is the rank (r), which controls the size of these trainable matrices. A higher rank means more expressive power but more memory and compute.","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#why-lora","position":9},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl3":"Why Dolly-15k?","lvl2":"Background"},"type":"lvl3","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#why-dolly-15k","position":10},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl3":"Why Dolly-15k?","lvl2":"Background"},"content":"Dolly-15k is an open-source instruction-following dataset created by Databricks, containing 15,000 human-generated prompt-response pairs.\n\nIt covers a wide range of tasks including question answering, summarization, classification, and creative writing.\n\nWe use it because it is freely available, well-structured, and a standard dataset for instruction fine-tuning, making results easy to compare and reproduce.","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#why-dolly-15k","position":11},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl3":"Why tmux?","lvl2":"Background"},"type":"lvl3","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#why-tmux","position":12},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl3":"Why tmux?","lvl2":"Background"},"content":"Training a 1 Billion parameter model takes time. Keeping an SSH connection open for the entire duration is fragile and impractical.\n\nIf your SSH session disconnects, any process running in that terminal is killed, meaning your training run is lost and you have to start over.\n\ntmux keeps your training process running on the VM independently of your SSH connection, so a dropped connection does not affect the job.\n\nIt also means you can disconnect intentionally, close your laptop, and reconnect later to check progress without interrupting anything.","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#why-tmux","position":13},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl3":"Why Weights & Biases?","lvl2":"Background"},"type":"lvl3","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#why-weights-biases","position":14},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl3":"Why Weights & Biases?","lvl2":"Background"},"content":"Training runs on a remote VM, so you need a way to monitor progress without staying connected via SSH.\n\nWandB logs metrics like loss, learning rate, and gradient norms in real time and makes them accessible from any browser, meaning you can close your laptop and check the dashboard later.\n\nIt also keeps a history of every run, making it easy to compare experiments and share results with others.","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#why-weights-biases","position":15},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 1: Create the VM"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-1-create-the-vm","position":16},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 1: Create the VM"},"content":"Create a new shell script file called create_vm.sh and copy the script below into it. Before running the script, update the AIMSUSERNAME variable at the top to your username:nano create_vm.sh\n\nPaste the following script:#!/bin/bash\n\nexport AIMSUSERNAME=\"<your_aims_username>\" # Change this to your own username\nexport REGION=\"europe-west4\"\nexport ZONE=\"europe-west4-a\"\n\n# Create VPC if it doesn't exist\nif ! gcloud compute networks describe ${AIMSUSERNAME}-vpc &>/dev/null; then\n    gcloud compute networks create ${AIMSUSERNAME}-vpc --subnet-mode=auto\nelse\n    echo \"VPC ${AIMSUSERNAME}-vpc already exists, skipping.\"\nfi\n\n# Create firewall rule if it doesn't exist\nif ! gcloud compute firewall-rules describe ${AIMSUSERNAME}-fw-ssh &>/dev/null; then\n    gcloud compute firewall-rules create ${AIMSUSERNAME}-fw-ssh \\\n        --network=${AIMSUSERNAME}-vpc \\\n        --allow=tcp:22\nelse\n    echo \"Firewall rule ${AIMSUSERNAME}-fw-ssh already exists, skipping.\"\nfi\n\n# Create Cloud Router if it doesn't exist\nif ! gcloud compute routers describe ${AIMSUSERNAME}-router-${REGION} --region=${REGION} &>/dev/null; then\n    gcloud compute routers create ${AIMSUSERNAME}-router-${REGION} \\\n        --network=${AIMSUSERNAME}-vpc \\\n        --region=${REGION}\nelse\n    echo \"Router ${AIMSUSERNAME}-router-${REGION} already exists, skipping.\"\nfi\n\n# Create NAT if it doesn't exist\nif ! gcloud compute routers nats describe ${AIMSUSERNAME}-nat-config \\\n    --router=${AIMSUSERNAME}-router-${REGION} \\\n    --region=${REGION} &>/dev/null; then\n    gcloud compute routers nats create ${AIMSUSERNAME}-nat-config \\\n        --router-region=${REGION} \\\n        --router=${AIMSUSERNAME}-router-${REGION} \\\n        --nat-all-subnet-ip-ranges \\\n        --auto-allocate-nat-external-ips\nelse\n    echo \"NAT ${AIMSUSERNAME}-nat-config already exists, skipping.\"\nfi\n\nexport VM_NAME=\"${AIMSUSERNAME}-l4-vm\"\n\ngcloud compute instances create ${VM_NAME} \\\n    --zone=${ZONE} \\\n    --machine-type=g2-standard-4 \\\n    --accelerator=\"type=nvidia-l4,count=1\" \\\n    --image-family=common-cu128-ubuntu-2204-nvidia-570 \\\n    --image-project=deeplearning-platform-release \\\n    --maintenance-policy=TERMINATE \\\n    --network=${AIMSUSERNAME}-vpc \\\n    --scopes=storage-full,cloud-platform\n\nThen save and run the script:chmod +x create_vm.sh\n./create_vm.sh\n\nThe script creates all necessary networking infrastructure (VPC, firewall, router, NAT) and provisions a VM with an NVIDIA L4 GPU.\n\nNote\n\nNotice the --scopes=storage-full,cloud-platform flag in the gcloud compute instances create command for creating the VM. This gives the VM permission to read from and write to GCS, which we will need later when uploading the dataset and syncing experiment outputs and model artifacts.\n\nWarning\n\nIf you see an error like The zone does not have enough resources available, the NVIDIA L4 GPU is out of stock in that zone. Try changing ZONE in the script to a nearby zone like europe-west4-b or europe-west4-c or changing the region REGION in the script to another region e.g. us-central1.\n\nThen verify the VM is running:gcloud compute instances list","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-1-create-the-vm","position":17},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 2: SSH into the VM"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-2-ssh-into-the-vm","position":18},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 2: SSH into the VM"},"content":"gcloud compute ssh <YOUR_AIMS_USERNAME>-l4-vm --zone=europe-west4-a --tunnel-through-iap\n\nReplace <YOUR_AIMS_USERNAME> and the --zone flag with the values you used when running the script.\n\nWhen you have successfully ‚ÄúSSH-ed‚Äù into your server, verify the GPU is available.nvidia-smi\n\nYou should see the NVIDIA L4 with 24GB VRAM listed. All subsequent commands are run inside this SSH session.","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-2-ssh-into-the-vm","position":19},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 3: Install uv"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-3-install-uv","position":20},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 3: Install uv"},"content":"uv is a fast Python package and project manager that handles creating virtual environments, installing dependencies, and locking package versions. We use it because it guarantees the exact same environment can be recreated on any machine with a single command.\n\nNote\n\nThe VM image we provisioned already comes with Python pre-installed, so we do not need to set up Python manually. uv will use the existing Python installation and manage everything else for us.\n\nInstall uv:curl -LsSf https://astral.sh/uv/install.sh | sh\nsource $HOME/.local/bin/env\n\nVerify uv installation:uv --version","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-3-install-uv","position":21},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 4: Clone the GitHub Repository"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-4-clone-the-github-repository","position":22},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 4: Clone the GitHub Repository"},"content":"Clone the GitHub repository containing the \n\ncode for this hands-on session into your VM:git clone https://github.com/rexsimiloluwah/finetuning-gemma-1b-aims-gcp-tutorial.git\ncd finetuning-gemma-1b-aims-gcp-tutorial","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-4-clone-the-github-repository","position":23},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 5: Set Up the Python Environment"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-5-set-up-the-python-environment","position":24},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 5: Set Up the Python Environment"},"content":"To setup the Python virtual environment using uv:uv venv --python 3.12\nuv sync\n\nuv sync reads the uv.lock file and installs all dependencies at the exact versions used during development, ensuring the environment on the VM is identical to what was used locally.","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-5-set-up-the-python-environment","position":25},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 6: Configure Environment Variables"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-6-configure-environment-variables","position":26},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 6: Configure Environment Variables"},"content":"You need two API keys before running anything in this hands-on session: a HuggingFace API token to access the Gemma model and a Weights & Biases API key to access the WandB platform for experiment tracking.\n\nGetting your WandB API key:\n\nGo to \n\nwandb.ai and create a free account if you don‚Äôt have one\n\nOnce logged in, click Create a new project and give it a name, for example gemma-finetuning-gcp. This is where all your training runs and metrics will be logged.\n\nNavigate to \n\nwandb.ai/authorize\n\nCopy your API key\n\n\n\nScreenshot of ‚ÄúCreate a new project‚Äù step in WandB\n\nGetting your HuggingFace token\n\nGo to \n\nhuggingface.co and create a free account if you don‚Äôt have one\n\nNavigate to \n\nhuggingface‚Äã.co‚Äã/settings‚Äã/tokens\n\nCreate a new token with read permissions and copy it\n\nWarning\n\nGemma 3 is a gated model on HuggingFace. Before your token will work, you must visit \n\nhuggingface‚Äã.co‚Äã/google‚Äã/gemma‚Äã-3‚Äã-1b‚Äã-it and accept the license agreement. Without this step, the training script will fail with a 401 error.\n\n\n\nScreenshot of ‚ÄúAccept Gemma 3 1B License Agreement on HuggingFace‚Äù step\n\nNow copy the example env file .env.example to create your .env file and fill in your credentials:cp .env.example .env\nnano .env\n\nFill in your values:HF_TOKEN=your_huggingface_token\nWANDB_API_KEY=your_wandb_api_key\nWANDB_PROJECT=your_wandb_project_name\n\nTo exit the nano editor: press Ctrl+O then Enter to save, then Ctrl+X\n\nBefore moving on, please confirm the following:\n\nCreated a WandB account and a new project\n\nCopied your WandB API key from \n\nwandb.ai/authorize\n\nCreated a HuggingFace account and generated a read token\n\nAccepted the Gemma 3 license agreement at \n\nhuggingface‚Äã.co‚Äã/google‚Äã/gemma‚Äã-3‚Äã-1b‚Äã-it\n\nFilled in .env file with your HF_TOKEN, WANDB_API_KEY, and WANDB_PROJECT","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-6-configure-environment-variables","position":27},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 7: Upload the Dataset to Google Cloud Storage (GCS) Bucket (Optional)"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-7-upload-the-dataset-to-google-cloud-storage-gcs-bucket-optional","position":28},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 7: Upload the Dataset to Google Cloud Storage (GCS) Bucket (Optional)"},"content":"Rather than downloading the dataset from HuggingFace every time you run an experiment, we upload it once to GCS (GCP‚Äôs scalable cloud object storage service) and read from there in all subsequent runs. This is faster, avoids rate limits, and means your experiment does not depend on an external service being available. On real projects with large datasets, this pattern becomes essential.\n\nRun the upload script:uv run bash scripts/download_and_upload_gcs.sh\n\n# You can optionally pass a custom bucket name:\nuv run bash scripts/download_and_upload_gcs.sh <BUCKET_NAME>\n\nThis script downloads the Dolly-15k dataset from HuggingFace, creates a new GCS bucket, and uploads the downloaded dataset to the GCS bucket.\n\n\n\nScreenshot of ‚ÄúUpload Dataset to GCS Bucket‚Äù step\n\nVerify the data is in the GCS bucket:gcloud storage ls gs://<BUCKET_NAME>/data/raw/","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-7-upload-the-dataset-to-google-cloud-storage-gcs-bucket-optional","position":29},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 8: Start a tmux Session"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-8-start-a-tmux-session","position":30},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 8: Start a tmux Session"},"content":"Training this 1 Billion parameter model using LoRA takes about 25-40 minutes. Rather than keeping your SSH connection open the entire time, we use tmux to run training in a persistent terminal session. This means if your SSH connection closes, training keeps running on the VM.\n\nInstall tmux and start a new session:sudo apt-get install -y tmux \ntmux new -s finetunegemma1b\n\nThis creates a new session named ‚Äúfinetunegemma1b‚Äù. You should now be inside the created tmux session.\n\nTo detach the session without killing it:\n\nPress Ctrl+B then the D key\n\nAlternatively, you can run the command from another shell for this server:tmux detach -s <session_name>\n\nTo return to a detached session layer:tmux attach -t <session_name>","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-8-start-a-tmux-session","position":31},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 9: Run the Training Script"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-9-run-the-training-script","position":32},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 9: Run the Training Script"},"content":"In the tmux session shell, run the script below to start finetuning the model:uv run python -m scripts.run_train \\\n    data.source=hf \\\n    data.max_train_samples=10000 \\\n    training.num_epochs=2 \\\n    training.batch_size=8 \\\n    experiment_id=exp_lora_r8\n\nThis trains on 10,000 samples for 2 epochs with LoRA rank 8, giving us 10,000 / 8 = 1,250 steps per epoch and 2,500 steps total. On the NVIDIA L4 GPU, this should take roughly 20-30 minutes.\n\nYou can now detach from the tmux session and monitor training progress from your WandB dashboard at \n\nwandb.ai. You will see train/loss, eval/loss, train/grad_norm, and train/learning_rate updating every 10 steps.\n\n\n\nScreenshot of WandB dashboard for monitoring training progress and metrics","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-9-run-the-training-script","position":33},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 10: Run Evaluation"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-10-run-evaluation","position":34},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 10: Run Evaluation"},"content":"Once training finishes, reattach to the tmux session if you detached:tmux attach -t <session_name>\n\nRun the evaluation script. Replace <experiment_id> and <checkpoint_folder> with the actual paths from your outputs/ directory:uv run python -m scripts.run_evaluate \\\n    --model_path outputs/<experiment_id>/<checkpoint_folder> \\\n    --eval_file data/eval/eval_prompts.jsonl\n\nThis computes perplexity and repetition rate on the evaluation set and logs the results to WandB.\n\n\n\nScreenshot of WandB dashboard for monitoring evaluation results\n\nNote\n\nDo not worry too much about the evaluation results. The goal of this tutorial is not to produce a state-of-the-art model but to walk through the complete workflow of fine-tuning a large language model on a GCP VM end to end.","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-10-run-evaluation","position":35},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 11: Run Inference (Optional)"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-11-run-inference-optional","position":36},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 11: Run Inference (Optional)"},"content":"Test the model on a single instructionuv run python -m scripts.run_inference \\\n    --model_path outputs/<experiment_id>/<checkpoint_folder> \\\n    --instruction \"Explain what machine learning is in simple terms\"\n\nYou can also run in interactive mode to keep sending instructions:uv run python -m scripts.run_inference \\\n    --model_path outputs/<experiment_id>/<checkpoint_folder>\n\n\n\nScreenshot of inference result using the fine-tuned Gemma 3 1B model","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-11-run-inference-optional","position":37},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 12: Sync Outputs to GCS"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-12-sync-outputs-to-gcs","position":38},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 12: Sync Outputs to GCS"},"content":"Before deleting the VM, sync your checkpoints and logs to your GCS bucket so they persist:gcloud storage cp -r outputs/ gs://<BUCKET_NAME>/outputs/\n\nVerify the GCS bucket content from the GCS dashboard or from the CLI:\n\n\n\nScreenshot of GCS bucket on the GCP console showing the ‚Äúdata‚Äù and synced ‚Äúoutputs‚Äù folder.gcloud storage ls gs://<BUCKET_NAME>/outputs/\n\nExit the tmux session:exit","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-12-sync-outputs-to-gcs","position":39},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 13: Resource Cleanup"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-13-resource-cleanup","position":40},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"Step 13: Resource Cleanup"},"content":"Danger\n\nDelete the VM when you are done to avoid incurring charges. GPU VMs are billed by the second and the NVIDIA L4 GPU VM using the g2-standard-4 machine configuration costs roughly $0.70/hour even when idle.\n\nRun the script below and replace <YOUR_AIMSUSERNAME> and <ZONE> with the values you used in the VM creation script.gcloud compute instances delete <YOUR_AIMSUSERNAME>-l4-vm --zone=<ZONE> --quiet\n\nVerify it has been deleted:gcloud compute instances list","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#step-13-resource-cleanup","position":41},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"üîë Key Takeways"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#id-key-takeways","position":42},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"üîë Key Takeways"},"content":"Using a package manager like uv ensures your Python environment is reproducible across any machine. A single uv sync installs all dependencies at the exact versions pinned in the lock file, eliminating environment mismatch issues.\n\nUploading your dataset to GCS once and reading from there on every run is more reliable and faster than downloading from external sources each time.\n\ntmux is essential for long-running jobs on remote VMs. It keeps your process alive even if your SSH connection is destroyed.\n\nWandB gives you full visibility into training progress and metrics from any browser, without needing to stay connected to the VM.\n\nAlways sync outputs to GCS before deleting your VM. VM disks are ephemeral and deleted with the instance.\n\nAlways delete your VM after a session to avoid unexpected charges.","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#id-key-takeways","position":43},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"üöÄ What‚Äôs Next?"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#id-whats-next","position":44},{"hierarchy":{"lvl1":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","lvl2":"üöÄ What‚Äôs Next?"},"content":"In the next session, we will look at Vertex AI, Google Cloud‚Äôs fully managed ML platform and how it can simplify running experiments like this without provisioning or managing any infrastructure yourself.","type":"content","url":"/part2-running-experiments-on-gcp-vms/hands-on-running-ai-experiments-on-gcp-vms#id-whats-next","position":45},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/part2-running-experiments-on-gcp-vms","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"","type":"content","url":"/part2-running-experiments-on-gcp-vms","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Running AI Experiments on GCP VMs"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms#running-ai-experiments-on-gcp-vms","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Running AI Experiments on GCP VMs"},"content":"When a machine learning experiment outgrows your laptop, because the model is too large, training takes too long, or you need a GPU, the natural next step is to move it to the cloud.\nGCP virtual machines (VMs) give you on-demand access to powerful hardware, including modern GPUs like the NVIDIA L4, without the cost of owning physical hardware.\n\nRunning experiments on a remote VM is conceptually simple: it is just a Linux machine you can SSH into and run Python code on. But there are a few things worth understanding before you get started.","type":"content","url":"/part2-running-experiments-on-gcp-vms#running-ai-experiments-on-gcp-vms","position":3},{"hierarchy":{"lvl1":"Introduction","lvl3":"Reproducible Environments","lvl2":"Running AI Experiments on GCP VMs"},"type":"lvl3","url":"/part2-running-experiments-on-gcp-vms#reproducible-environments","position":4},{"hierarchy":{"lvl1":"Introduction","lvl3":"Reproducible Environments","lvl2":"Running AI Experiments on GCP VMs"},"content":"Code that works locally often fails on a VM due to different Python versions or package versions.\n\nHence it is important to use package managers like uv or poetry to automate the installation, configuration, and management of software dependencies, ensuring consistency and reproducibility.","type":"content","url":"/part2-running-experiments-on-gcp-vms#reproducible-environments","position":5},{"hierarchy":{"lvl1":"Introduction","lvl3":"Persistent Storage","lvl2":"Running AI Experiments on GCP VMs"},"type":"lvl3","url":"/part2-running-experiments-on-gcp-vms#persistent-storage","position":6},{"hierarchy":{"lvl1":"Introduction","lvl3":"Persistent Storage","lvl2":"Running AI Experiments on GCP VMs"},"content":"VM disks are ephemeral. When you delete the VM, everything else is deleted.\n\nIt is important to always write outputs, checkpoints, and logs to a persistent object storage like Google Cloud Storage (GCS) during or after training so they persist independently of the VM.\n\nThink of the VM as temporary compute and GCS as permanent storage.","type":"content","url":"/part2-running-experiments-on-gcp-vms#persistent-storage","position":7},{"hierarchy":{"lvl1":"Introduction","lvl3":"Long-Running Jobs","lvl2":"Running AI Experiments on GCP VMs"},"type":"lvl3","url":"/part2-running-experiments-on-gcp-vms#long-running-jobs","position":8},{"hierarchy":{"lvl1":"Introduction","lvl3":"Long-Running Jobs","lvl2":"Running AI Experiments on GCP VMs"},"content":"Training machine learning models can take anywhere from minutes, for small models like XGBoost, to hours or even days for models like transformers or large language models.\n\nKeeping an SSH connection open for the entire duration is fragile. A dropped connection can kill the training process.\n\nYou can use a tool like tmux to keep your process running on the VM even if your SSH session disconnects.","type":"content","url":"/part2-running-experiments-on-gcp-vms#long-running-jobs","position":9},{"hierarchy":{"lvl1":"Introduction","lvl3":"Experiment Monitoring and Logging","lvl2":"Running AI Experiments on GCP VMs"},"type":"lvl3","url":"/part2-running-experiments-on-gcp-vms#experiment-monitoring-and-logging","position":10},{"hierarchy":{"lvl1":"Introduction","lvl3":"Experiment Monitoring and Logging","lvl2":"Running AI Experiments on GCP VMs"},"content":"Experiment monitoring is the practice of logging and visualizing metrics as a model trains, giving you a live record of how your experiment is progressing.\n\nOn a remote VM, you cannot just watch your terminal, so you need a tool that logs metrics to a dashboard you can access from anywhere.\n\nTools like Weights & Biases, Tensorboard, or MLFlow automatically track metrics like loss, accuracy, and learning rate, hardware utilization, and maintain a history of every experiment you run.\n\nThis gives you a centralized record of all your experiments, making it easy to compare runs, spot regressions, and share results with others.","type":"content","url":"/part2-running-experiments-on-gcp-vms#experiment-monitoring-and-logging","position":11},{"hierarchy":{"lvl1":"Introduction","lvl3":"Cost Management","lvl2":"Running AI Experiments on GCP VMs"},"type":"lvl3","url":"/part2-running-experiments-on-gcp-vms#cost-management","position":12},{"hierarchy":{"lvl1":"Introduction","lvl3":"Cost Management","lvl2":"Running AI Experiments on GCP VMs"},"content":"VMs are billed by the second. An idle VM costs the same as a busy one.\n\nAlways stop/delete your VM when an experiment is done.\n\nNote\n\nGPU VMs are significantly more expensive than CPU VMs. An NVIDIA L4 instance costs roughly $0.70/hour. Always double-check that your VM has been deleted after a session by running gcloud compute instances list.\n\nWarning\n\nNever store API keys or tokens directly in your code or commit them to GitHub. Use a .env file listed in .gitignore and load credentials at runtime.","type":"content","url":"/part2-running-experiments-on-gcp-vms#cost-management","position":13},{"hierarchy":{"lvl1":"Introduction","lvl2":"üöÄ What‚Äôs Next?"},"type":"lvl2","url":"/part2-running-experiments-on-gcp-vms#id-whats-next","position":14},{"hierarchy":{"lvl1":"Introduction","lvl2":"üöÄ What‚Äôs Next?"},"content":"In the hands-on session that follows, you will run a complete end-to-end ML experiment on a GCP VM with an NVIDIA L4 GPU. This will involve:\n\nFine-tuning Gemma 3 1B on the Dolly-15k instruction dataset using LoRA.\n\nTracking experiments in real time with Weights & Biases\n\nPersisting checkpoints and outputs to GCS\n\nRunning evaluation and inference on the trained model","type":"content","url":"/part2-running-experiments-on-gcp-vms#id-whats-next","position":15},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs"},"type":"lvl1","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job","position":0},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs"},"content":"","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job","position":1},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Overview"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#overview","position":2},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Overview"},"content":"In the previous hands-on session, we ran the Gemma 1B fine-tuning experiment on a raw GCP VM. We had to provision the VM, SSH in, set up the environment, and manually manage the entire process. In this session we will run the exact same experiment as a Vertex AI Custom Training Job, where Vertex AI handles all the infrastructure for us. We submit a job spec, Vertex AI provisions the hardware, runs the job, and terminates everything automatically when done.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#overview","position":3},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Learning Objectives"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#learning-objectives","position":4},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Learning Objectives"},"content":"By the end of this session, you will be able to:\n\nEnable the Vertex AI API and set up the required permissions\n\nWrite a Vertex AI Custom Training Job config file\n\nSubmit and monitor a training job using the gcloud CLI\n\nNote\n\nThis hands-on uses the same fine-tuning experiment from the previous session. If you want to understand the experiment in detail, including the model, dataset, and training code, refer back to \n\nHands-On: Fine-Tuning Gemma 1B on a GCP VM.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#learning-objectives","position":5},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Prerequisites"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#prerequisites","position":6},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Prerequisites"},"content":"gcloud CLI installed and configured on your local machine\n\nA WandB account and API key\n\nA HuggingFace account and API token with the Gemma 3 1B model license agreement accepted.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#prerequisites","position":7},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 1: Enable the Vertex AI API"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-1-enable-the-vertex-ai-api","position":8},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 1: Enable the Vertex AI API"},"content":"Run this from your local machine to enable the Vertex AI API:export PROJECT_ID=$(gcloud config get-value project)\n\ngcloud services enable aiplatform.googleapis.com \\\n    --project=${PROJECT_ID}\n\nVerify it has been enabled:gcloud services list --enabled \\\n    --filter=\"name:aiplatform.googleapis.com\" \\\n    --project=${PROJECT_ID}","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-1-enable-the-vertex-ai-api","position":9},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 2: Grant the Required Permissions"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-2-grant-the-required-permissions","position":10},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 2: Grant the Required Permissions"},"content":"Vertex AI training jobs run under a service account. Grant it the permissions it needs to read and write to GCS.export SERVICE_ACCOUNT=$(gcloud iam service-accounts list \\\n    --filter=\"displayName:Compute Engine default service account\" \\\n    --format=\"value(email)\" \\\n    --project=${PROJECT_ID})\n\ngcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n    --member=\"serviceAccount:${SERVICE_ACCOUNT}\" \\\n    --role=\"roles/storage.admin\"\n\ngcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n    --member=\"serviceAccount:${SERVICE_ACCOUNT}\" \\\n    --role=\"roles/aiplatform.user\"","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-2-grant-the-required-permissions","position":11},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 3: Create the Job Config"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-3-create-the-job-config","position":12},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 3: Create the Job Config"},"content":"Vertex AI Custom Training Jobs are defined by a YAML config file that specifies the machine type, container image, and the command to run. Create a file called vertex_job.yaml in your project folder.\n\nPaste the following code, replacing the placeholder values for the environment variables (YOUR_HF_TOKEN, YOUR_WANDB_API_KEY, YOUR_WANDB_PROJECT, YOUR_GCS_BUCKET_NAME) with the actual values.workerPoolSpecs:\n  - machineSpec:\n      machineType: g2-standard-4\n      acceleratorType: NVIDIA_L4\n      acceleratorCount: 1\n    replicaCount: 1\n    containerSpec:\n      imageUri: us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-gpu.2-0.py310\n      command:\n        - bash\n        - -c\n      args:\n        - |\n          curl -LsSf https://astral.sh/uv/install.sh | sh &&\n          source $HOME/.local/bin/env &&\n          git clone https://github.com/rexsimiloluwah/finetuning-gemma-1b-aims-gcp-tutorial.git &&\n          cd finetuning-gemma-1b-aims-gcp-tutorial &&\n          uv sync &&\n          uv run python -m scripts.run_train data.source=gcs data.max_train_samples=10000 training.num_epochs=2 training.batch_size=8 experiment_id=exp_lora_r8_vertex\n          gsutil -m cp -r outputs/ gs://$BUCKET_NAME/runs/exp_lora_r8_vertex\n      env:\n        - name: HF_TOKEN\n          value: <YOUR_HF_TOKEN>\n        - name: WANDB_API_KEY\n          value: <YOUR_WANDB_API_KEY>\n        - name: WANDB_PROJECT\n          value: <YOUR_WANDB_PROJECT>\n        - name: BUCKET_NAME\n          value: <YOUR_GCS_BUCKET_NAME>\n\nThis configuration does the following:\n\nAutomatically spins up a Google Cloud VM equipped with an NVIDIA L4 GPU and the necessary CUDA drivers.\n\nAutomates environment setup: clones the training code, installs the uv package manager, and installs all Python dependencies.\n\nRuns the Gemma 1B fine-tuning script and automatically persists the resulting model weights and logs to your GCS bucket.\n\nWhy this container image?\n\nThe pytorch-gpu.2-0.py310 image comes with PyTorch, CUDA, and common ML libraries pre-installed. This means uv sync only needs to install your smaller dependencies like transformers, peft, and wandb, rather than downloading PyTorch (873MB) and CUDA libraries from scratch on every run.\n\nWhy gsutil cp command at the end?\n\nHuggingFace Trainer saves checkpoints locally inside the container, which is ephemeral and lost when the job finishes. The gsutil cp command at the end explicitly syncs the local outputs/ directory to GCS after training completes, ensuring your checkpoints persist. This is the standard pattern for persisting outputs from Vertex AI training jobs when using HuggingFace Trainer.\n\nWarning\n\nDo not commit the vertex_job.yaml file to GitHub with your API keys in it. Add it to .gitignore to keep your credentials safe.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-3-create-the-job-config","position":13},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 4: Submit the Training Job"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-4-submit-the-training-job","position":14},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 4: Submit the Training Job"},"content":"This command uses the gcloud CLI to submit a Vertex AI Custom Training Job based on the specifications in vertex_job.yaml.export REGION=\"europe-west4\" # change to any region you want\nexport AIMSUSERNAME=\"<YOUR_AIMS_USERNAME>\"\nexport JOB_NAME=\"${AIMSUSERNAME}-gemma-finetune-$(date +%Y%m%d%H%M%S)\"\n\ngcloud ai custom-jobs create \\\n    --region=${REGION} \\\n    --display-name=${JOB_NAME} \\\n    --config=vertex_job.yaml \\\n    --project=${PROJECT_ID}\n\nYou should see an output confirming that the job was successfully created with a job ID.\n\n\n\nScreenshot: Terminal output showing successfully Vertex AI training job creation with job ID","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-4-submit-the-training-job","position":15},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 5: Monitor the Job"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-5-monitor-the-job","position":16},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 5: Monitor the Job"},"content":"Check the job status from the CLI:gcloud ai custom-jobs list \\\n    --region=${REGION} \\\n    --project=${PROJECT_ID}\n\nStream the logs in real time:# Ger the Job ID using the Display Name\nJOB_ID=$(gcloud ai custom-jobs list \\     \n    --region=${REGION} \\\n    --filter=\"displayName=${JOB_NAME}\" \\\n    --format=\"value(name.scope(customJobs))\")\n\n# Stream the logs using the ID\ngcloud ai custom-jobs stream-logs ${JOB_ID} --region=${REGION}\n\nYou can also monitor the job from the GCP Console.\n\nGo to Vertex AI page on your GCP Console\n\nNavigate to the Custom Jobs tab. Ensure the region you used when submitting the job is selected.\n\nClick on the job you want to monitor\n\nClick on View logs to view the logs for the training job.\n\n\n\nScreenshot: GCP Console showing the custom jobs list with job status\n\n\n\nScreenshot: GCP Console showing details for a custom training job with the ‚ÄúView logs‚Äù link\n\n\n\nScreenshot: GCP Console showing the custom training job logs streaming in real time\n\nAs with the VM-based run, you can also monitor training metrics in real time from your WandB dashboard at \n\nwandb.ai. You will see. train/loss, eval/loss, train/grad_norm, train/learning_rate, etc., updating every 10 steps.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-5-monitor-the-job","position":17},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 6: Verify the Outputs in GCS"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-6-verify-the-outputs-in-gcs","position":18},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 6: Verify the Outputs in GCS"},"content":"Once the job status changes to FINISHED, we can verify that the outputs were synced to your GCS bucket successfully using the GCP Console.\n\n\n\nScreenshot: GCS Bucket in GCP Console showing synced output folder","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-6-verify-the-outputs-in-gcs","position":19},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 7: Resource Cleanup"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-7-resource-cleanup","position":20},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"Step 7: Resource Cleanup"},"content":"Vertex AI Custom Training Jobs terminate automatically when the job completes, so there is no VM delete. The only resource to clean up is the GCS bucket if you no longer need the outputs:\n\nDanger\n\nOnly delete the bucket if you no longer need the checkpoints. If you want to keep your trained model, skip this step or download the outputs locally first.gcloud storage rm --recursive gs://<YOUR_GCS_BUCKET_NAME>","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#step-7-resource-cleanup","position":21},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"üîë Key Takeaways"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#id-key-takeaways","position":22},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"üîë Key Takeaways"},"content":"Vertex AI Custom Training Jobs eliminate all infrastructure management. You submit a job spec, Vertex AI provisions the hardware, runs the job, and terminates everything automatically when done.\n\nYou pay only for the duration of the job. There are no idle costs unlike a raw VM that keeps running until you manually delete it.\n\nThe training code itself does not change at all. The only difference from the VM-based approach is how you submit and monitor the job.\n\nUsing a pre-built container image with PyTorch already installed significantly speeds up job startup time by avoiding large dependency downloads.\n\nAlways sync outputs to GCS explicitly at the end of the job since HuggingFace Trainer saves checkpoints locally inside the ephemeral container.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#id-key-takeaways","position":23},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"üöÄ What‚Äôs Next"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#id-whats-next","position":24},{"hierarchy":{"lvl1":"Hands-On 01: Vertex AI Custom Training Jobs","lvl2":"üöÄ What‚Äôs Next"},"content":"In the next hands-on session, we will run the same experiment interactively using Vertex AI Colab Enterprise, a managed notebook environment accessible directly from your browser with no SSH or infrastructure setup required.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-custom-training-job#id-whats-next","position":25},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)"},"type":"lvl1","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise","position":0},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)"},"content":"","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise","position":1},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Overview"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#overview","position":2},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Overview"},"content":"In this hands on session, we will run the same Gemma 1B fine-tuning experiment using Colab Enterprise, Vertex AI‚Äôs managed notebook environment. Unlike the Custom Training Job approach where you submit a job specification and wait, Colab Enterprise gives you an interactive Jupyter notebook (similar to Google Colab) that runs directly in your browser with no SSH or infrastructure setup required.\n\nNote\n\nThis hands-on tutorial uses the same Gemma 1B fine-tuning experiment from the previous chapter. If you want to understand the experiment in detail, refer back to Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#overview","position":3},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Learning Objectives"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#learning-objectives","position":4},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Learning Objectives"},"content":"By the end of this session, you will be able to:\n\nCreate a runtime template with a GPU configuration in Colab Enterprise\n\nCreate and connect a runtime to a notebook\n\nRun the fine-tuning experiment interactively in a notebook\n\nClean up all resources after the session.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#learning-objectives","position":5},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Prerequisites"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#prerequisites","position":6},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Prerequisites"},"content":"A WandB account and API key\n\nA HuggingFace account and API token with the Gemma 3 1B model license agreement accepted.\n\nImportant\n\nTo follow this tutorial, you need a GCP project with the Vertex AI API enabled. Kindly navigate to the APIs and services page to enable the API for Vertex AI, if it is not already enabled.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#prerequisites","position":7},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Step 1: Open Colab Enterprise"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#step-1-open-colab-enterprise","position":8},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Step 1: Open Colab Enterprise"},"content":"Go to your \n\nGCP Console\n\nIn the left sidebar, navigate to Vertex AI\n\nUnder the Notebooks section, click Colab Enterprise.\n\n\n\nScreenshot of the Colab enterprise page in Vertex AI","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#step-1-open-colab-enterprise","position":9},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Step 2: Create a Runtime Template"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#step-2-create-a-runtime-template","position":10},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Step 2: Create a Runtime Template"},"content":"A runtime template defines the hardware configuration that your notebook will run on. We create it once and can reuse it across multiple notebooks.\n\nIn the left sidebar, click Runtime templates, then click Create a new runtime template.\n\n\n\nScreenshot: Create a new runtime template\n\nThe runtime template creation has four sections.\n\nRuntime basics\n\nEnter a name for the template, for example nvidia-l4-gpu-template\n\nSelect the region for the runtime template, for example europe-west4\n\n\n\nScreenshot: Runtime basics (edit display name and select region)\n\nConfigure compute\n\nThis is where we configure the compute associated with the runtime template.\n\nUnder Machine Type, select g2-standard-4\n\nUnder Accelerator type, select NVIDIA_L4 and set Accelerator count to 1\n\n\n\nScreenshot: Configure machine configuration for the runtime template\n\nConfigure Python Environment\n\nSelect the Python environment under the Environment input (you can use Python 3.12)\n\nUnder the Environment variables, add your environment variables:\n\nHF_TOKEN = Your HuggingFace API token\n\nWANDB_API_KEY = Your WandB API key\n\nWANDB_PROJECT = Your WandB project name\n\nBUCKET_NAME = Your GCS bucket name\n\n\n\nScreenshot: Configure Python environment for runtime template with environment variables filled in\n\nNote\n\nAdding environment variables here means they are automatically available in every notebook that connects to this runtime. You do not need to set them manually in your notebook cells.\n\nWarning\n\nDo not share your runtime template with others as it contains your API keys. Keep it private to your GCP project.\n\nNetworking and security\n\nThis is where we choose the networking configuration.\n\nUnder Network input, select the VPC you created earlier (similoluwa-vpc) or create a new VPC.\n\nSelect the same VPC under the Subnetwork input.\n\n\n\nScreenshot: Networking and security section with VPC selected\n\nFinally, click the Create button to create the runtime template.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#step-2-create-a-runtime-template","position":11},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Step 3: Create a Runtime"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#step-3-create-a-runtime","position":12},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Step 3: Create a Runtime"},"content":"Now, we can create a runtime using the runtime template that was just created.\n\nIn the left sidebar, click Runtimes\n\nClick Create new runtime\n\nSelect the template you just created i.e. nvidia-l4-gpu-template, under the Runtime template input\n\nEnter a name for your runtime in the Runtime name input\n\nClick Create and wait for it to provision. This takes a few seconds.\n\n\n\nScreenshot: Create a new runtime using the created runtime template","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#step-3-create-a-runtime","position":13},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Step 4: Create a Notebook and Connect to the Runtime"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#step-4-create-a-notebook-and-connect-to-the-runtime","position":14},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Step 4: Create a Notebook and Connect to the Runtime"},"content":"Now, we can create a Vertex AI notebook using the newly created runtime.\n\nOn the Colab Enterprise main page, click the Create notebook button in Quick actions section\n\nOnce the notebook opens, click the Connect dropdown in the top right\n\nClick Connect to a runtime\n\nIn the Connect to Vertex AI runtime dialog:\n\nSelect the Connect to an existing runtime option\n\nSelect the runtime you just created under the Select an existing runtime input\n\nFinally, click Create to connect to the runtime.\n\n\n\nScreenshot: Create Colab Enterprise notebook\n\n\n\nScreenshot: Connect to a runtime dropdown\n\n\n\nScreenshot: Connect to a runtime dialog\n\nOnce connected, the runtime immediately becomes active and you can start using notebook.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#step-4-create-a-notebook-and-connect-to-the-runtime","position":15},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Step 5: Using the Notebook"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#step-5-using-the-notebook","position":16},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Step 5: Using the Notebook"},"content":"In this section, we will use the notebook to fine-tune the Gemma 3 1B model.\n\nVerify the GPU is Available\n\nTo verify the GPU is available:%%bash\nnvidia-smi\n\n\n\nScreenshot: Testing nvidia-smi command in Vertex notebook\n\nSet Up the Python Environment\n\nIn the next cell, install uv and clone the repository:%%bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nsource $HOME/.local/bin/env\ngit clone https://github.com/rexsimiloluwah/finetuning-gemma-1b-aims-gcp-tutorial.git\ncd finetuning-gemma-1b-aims-gcp-tutorial\nuv sync\n\nRun Training\n\nIn the next cell, run the fine-tuning script:%%bash\nsource $HOME/.local/bin/env\ncd finetuning-gemma-1b-aims-gcp-tutorial\nuv run python -m scripts.run_train data.source=gcs data.max_train_samples=10000 training.num_epochs=2 training.batch_size=8 experiment_id=exp_lora_r8_colab\n\nTraining takes roughly 20-30 minutes on the L4. You can monitor progress in real time from your WandB dashboard at \n\nwandb.ai while the cell is running.\n\n\n\nScreenshot: Run the training script in the Vertex notebook. Here, it was trained for only 1 epoch using 500 samples for demonstration purposes. Do not expect any good results during evaluation.\n\nNote\n\nUnlike the VM-based session, you do not need tmux here. The notebook cell keeps running even if you close your browser tab, as long as the runtime stays active. You can come back later and see the output when it completes.\n\nRun Evaluation\n\nOnce training finishes, run evaluation in the next cell:%%bash\nsource $HOME/.local/bin/env\ncd finetuning-gemma-1b-aims-gcp-tutorial\nuv run python -m scripts.run_evaluate \\\n    --model_path outputs/exp_lora_r8_colab/checkpoint-2500 \\\n    --eval_file data/eval/eval_prompts.jsonl \\\n    --max_eval_samples 200\n\n\n\nScreenshot: Run the evaluation script.\n\nThis computes perplexity and repetition rate on 200 eval examples and logs the results to WandB.\n\nNote\n\nDo not worry about the evaluation results. The goal of this session is to walk through the complete workflow, not to produce a state-of-the-art model.\n\nRun Inference\n\nTest the trained model on a single instruction in the next cell:%%bash\nsource $HOME/.local/bin/env\ncd finetuning-gemma-1b-aims-gcp-tutorial\nuv run python -m scripts.run_inference \\\n    --model_path outputs/exp_lora_r8_colab/checkpoint-2500 \\\n    --instruction \"Explain what machine learning is in simple terms\"\n\n\n\nScreenshot: Run the inference script.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#step-5-using-the-notebook","position":17},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Step 6: Resource Cleanup"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#step-6-resource-cleanup","position":18},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"Step 6: Resource Cleanup"},"content":"Danger\n\nColab Enterprise runtimes are billed by the hour even when idle. Always delete the runtime when you are done to avoid unexpected charges.\n\nDelete the runtime from the GCP Console:\n\nIn the left sidebar, click Runtimes\n\nFind your runtime in the list\n\nClick the three dots menu next to it\n\nClick Delete\n\nTip\n\nYou can alternatively click Stop if you want to retain your data/environment for later use.\n\n\n\nScreenshot: Delete the Vertex AI runtime.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#step-6-resource-cleanup","position":19},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"üîë Key Takeaways"},"type":"lvl2","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#id-key-takeaways","position":20},{"hierarchy":{"lvl1":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","lvl2":"üîë Key Takeaways"},"content":"Colab Enterprise gives you an interactive GPU notebook environment with no infrastructure to manage, accessible directly from your browser\n\nCreating a runtime template first and reusing it across notebooks saves time and ensures a consistent hardware configuration across sessions\n\nAdding environment variables to the runtime template means credentials are automatically available in every notebook that connects to it, without hardcoding them in notebook cells\n\nUnlike the VM-based approach, you do not need tmux since notebook cells keep running even if you close your browser, as long as the runtime stays active\n\nFor long training runs, a Custom Training Job is more reliable since it runs entirely unattended. Colab Enterprise is better suited for interactive experimentation and debugging.","type":"content","url":"/part3-vertex-ai/hands-on-vertex-ai-notebook-colab-enterprise#id-key-takeaways","position":21},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/part3-vertex-ai","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"","type":"content","url":"/part3-vertex-ai","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"What is Vertex AI?"},"type":"lvl2","url":"/part3-vertex-ai#what-is-vertex-ai","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"What is Vertex AI?"},"content":"When you run a machine learning experiment on a raw virtual machine (VM), you are responsible for provisioning the machine, managing Python environments, monitoring your training jobs, and remembering to shut everything down when you are done. This works fine for one or two experiments, but it becomes a real burden as your workload scales.\n\nVertex AI is Google Cloud‚Äôs fully managed, end-to-end ML platform. It abstracts away the infrastructure so you can focus entirely on your models and data. Instead of thinking about virtual machines and infrastructure setup, you work with higher-level concepts like jobs, datasets, experiments, and pipelines.\n\nNote\n\n‚ÄúFully managed‚Äù means Google handles the underlying infrastructure, including provisioning hardware, installing drivers, scaling resources, and cleaning up after jobs complete.","type":"content","url":"/part3-vertex-ai#what-is-vertex-ai","position":3},{"hierarchy":{"lvl1":"Introduction","lvl2":"Core Capabilities of Vertex AI"},"type":"lvl2","url":"/part3-vertex-ai#core-capabilities-of-vertex-ai","position":4},{"hierarchy":{"lvl1":"Introduction","lvl2":"Core Capabilities of Vertex AI"},"content":"Vertex AI is not a single tool. It is a suite of services that covers the entire ML workflow:\n\nCustom Training Jobs: Enables you to submit training jobs by specifying your code, container image, and machine configuration. Vertex AI provisions the required compute resources, runs the job, streams logs, and automatically shuts down the infrastructure when training completes. You are billed only for the time the job is running.\n\nVertex AI Workbench: Provides managed notebook environments for development and experimentation. It offers:\n\nWorkbench Instances (formerly Managed Notebooks) ‚Äî A managed JupyterLab environment with optional GPU support and pre-installed ML libraries, accessible directly from your browser without manual infrastructure setup.\n\nColab Enterprise ‚Äî A serverless notebook experience similar to Google Colab, but running within your Google Cloud project. It integrates directly with Google Cloud Storage and other Google Cloud services, without requiring you to provision or manage infrastructure.\n\nModel Registry: A centralized service for storing, versioning, and managing trained models. After training, a model can be registered as a versioned artifact that you can deploy, compare against other versions, or roll back to when needed.\n\nVertex AI Pipelines: Enables you to define reproducible and automated workflows that connect steps such as data preparation, training, evaluation, and deployment. Once defined, a pipeline can be triggered programmatically to ensure consistent and repeatable ML processes.\n\nPrediction and Serving: Allows you to deploy trained models as managed REST endpoints with automatic scaling. Vertex AI handles load balancing and scales resources up or down based on traffic patterns.\n\nVertex AI Experiments: Provides built-in experiment tracking that logs parameters, metrics, and artifacts across training runs, helping you compare results and maintain reproducibility within the platform.\n\nVertex AI Model Garden: A curated catalog of foundation and open models that can be deployed or fine-tuned within your project, offering streamlined access to large models on Google Cloud without complex infrastructure setup.","type":"content","url":"/part3-vertex-ai#core-capabilities-of-vertex-ai","position":5},{"hierarchy":{"lvl1":"Introduction","lvl2":"How Vertex AI Fits Into GCP?"},"type":"lvl2","url":"/part3-vertex-ai#how-vertex-ai-fits-into-gcp","position":6},{"hierarchy":{"lvl1":"Introduction","lvl2":"How Vertex AI Fits Into GCP?"},"content":"Vertex AI does not work in isolation. It is deeply integrated with the rest of GCP:\n\nGCS: Training jobs read data from and write checkpoints to GCS buckets automatically.\n\nCloud Logging: Job logs are streamed to Cloud Logging and accessible from the Console or CLI.\n\nIAM: Access control for jobs, models, and endpoints is managed through the same IAM roles as the rest of GCP.\n\nArtifact Registry: Custom training containers are stored in Artifact Registry and referenced by training jobs.\n\nNote\n\nOne of the main benefits of Vertex AI is its seamless integration with Google Cloud. If your data is already in GCS and your team uses GCP, Vertex AI can be used immediately without setting up separate infrastructure.","type":"content","url":"/part3-vertex-ai#how-vertex-ai-fits-into-gcp","position":7},{"hierarchy":{"lvl1":"Introduction","lvl2":"Vertex AI vs Raw VMs on GCP"},"type":"lvl2","url":"/part3-vertex-ai#vertex-ai-vs-raw-vms-on-gcp","position":8},{"hierarchy":{"lvl1":"Introduction","lvl2":"Vertex AI vs Raw VMs on GCP"},"content":"Both raw virtual machines (VMs) and Vertex AI can run the same experiments, but they differ in operational overhead and management:\n\nRaw VM: Offers full control but requires more setup. You must provision the machine, configure environments, monitor jobs, and clean up manually. Best suited for one-off experiments or when you need custom system-level configuration.\n\nVertex AI Custom Training: Provides less control but greatly reduces operational overhead. You submit a job specification and the platform handles provisioning, execution, logging, and cleanup automatically. Ideal for repeated experiments, team workflows, and large-scale projects.\n\nVertex AI Workbench Instances: Interactive, browser-based JupyterLab environments running on managed hardware. The instance remains active until you stop it, similar to a VM, but you don‚Äôt need to manage drivers or environment setup.\n\nColab Enterprise: Fully serverless notebooks. No infrastructure to manage, making it perfect for quick experimentation and data exploration.\n\nWarning\n\nVertex AI requires more initial setup compared to SSH-ing into a VM. For a single quick experiment, a raw VM can be faster to start. Vertex AI becomes advantageous when running multiple experiments, collaborating with a team, or building reproducible pipelines.\n\nFeature\n\nRaw VM\n\nVertex AI Custom Training Job\n\nVertex AI Workbench\n\nManagement\n\nManual\n\nFully managed\n\nSemi-managed\n\nIdle costs\n\nYes, until deleted\n\nNo, auto-terminates\n\nYes, until stopped\n\nAccess\n\nSSH / Terminal\n\nCLI / Console logs\n\nBrowser (Jupyter)\n\nSetup complexity\n\nLow\n\nMedium\n\nLow","type":"content","url":"/part3-vertex-ai#vertex-ai-vs-raw-vms-on-gcp","position":9},{"hierarchy":{"lvl1":"Introduction","lvl2":"üîë Key Takeaways"},"type":"lvl2","url":"/part3-vertex-ai#id-key-takeaways","position":10},{"hierarchy":{"lvl1":"Introduction","lvl2":"üîë Key Takeaways"},"content":"Vertex AI is a fully managed machine learning platform on Google Cloud. You define what to run, and the platform manages the underlying infrastructure required to execute it.\n\nIt spans the end-to-end ML workflow, including training, experiment tracking, model versioning, pipelines, and model serving.\n\nVertex AI Workbench is the umbrella for managed notebook environments:\n\nWorkbench Instances provide a persistent JupyterLab environment running on managed compute.\n\nColab Enterprise provides a serverless notebook experience within your Google Cloud project.\n\nModel Garden provides curated access to foundation models that can be deployed or fine-tuned directly within your project, reducing the need for custom infrastructure setup.\n\nVertex AI integrates natively with services such as Google Cloud Storage (GCS), IAM, and Cloud Logging, making it a natural fit for teams already operating within Google Cloud.\n\nCompared to raw virtual machines, the primary tradeoff is control versus operational overhead. Vertex AI requires more initial configuration, but it reduces manual infrastructure management and eliminates idle costs for managed training jobs.","type":"content","url":"/part3-vertex-ai#id-key-takeaways","position":11},{"hierarchy":{"lvl1":"Introduction","lvl2":"üöÄ What Next?"},"type":"lvl2","url":"/part3-vertex-ai#id-what-next","position":12},{"hierarchy":{"lvl1":"Introduction","lvl2":"üöÄ What Next?"},"content":"In the next two hands-on sessions, we will run the same Gemma 1B fine-tuning experiment on Vertex AI in two different ways, focusing on the developer experience.\n\nHands-On 1: Vertex AI Custom Training Job: Submit the experiment as a managed batch training job using the gcloud CLI. This demonstrates how Vertex AI provisions compute, executes the job, streams logs, and automatically cleans up when training completes.\n\nHands-On 2: Vertex AI Notebook (Colab Enterprise): Run the same experiment interactively inside a Vertex AI notebook using Colab Enterprise. This provides a fully serverless, browser-based notebook experience within your Google Cloud project, allowing you to experiment and iterate without managing infrastructure.\n\nThese sessions are designed to help you understand the workflow of working within Vertex AI.","type":"content","url":"/part3-vertex-ai#id-what-next","position":13},{"hierarchy":{"lvl1":"Introduction","lvl2":"üìö References & Further Reading"},"type":"lvl2","url":"/part3-vertex-ai#id-references-further-reading","position":14},{"hierarchy":{"lvl1":"Introduction","lvl2":"üìö References & Further Reading"},"content":"Vertex AI Documentation - Official Google Cloud documentation covering all Vertex AI services.\n\nVertex AI Custom Training Overview - Detailed guide on submitting and managing custom training jobs.\n\nVertex AI Workbench Documentation - Guide to creating and using managed Workbench Instances.\n\nColab Enterprise Documentation - Guide to using serverless Colab Enterprise notebooks on Vertex AI.\n\nVertex AI Model Garden - Browse and deploy foundation models directly from GCP.\n\nVertex AI Pricing - Full pricing breakdown for training, prediction, and notebook instances.","type":"content","url":"/part3-vertex-ai#id-references-further-reading","position":15},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets"},"type":"lvl1","url":"/part4-cost-management/hands-on-cost-management","position":0},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets"},"content":"","type":"content","url":"/part4-cost-management/hands-on-cost-management","position":1},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"Overview"},"type":"lvl2","url":"/part4-cost-management/hands-on-cost-management#overview","position":2},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"Overview"},"content":"In this hands-on session, we will set up a GCP billing budget and alert so you are notified before you overspend your credits. We will also use the GCP Pricing Calculator to estimate the cost of the experiments we ran in the previous hands-on sessions.","type":"content","url":"/part4-cost-management/hands-on-cost-management#overview","position":3},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"Learning Objectives"},"type":"lvl2","url":"/part4-cost-management/hands-on-cost-management#learning-objectives","position":4},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"Learning Objectives"},"content":"By the end of this session, you will be able to:\n\nSet up a billing budget and alert in GCP\n\nUse the GCP Pricing Calculator to estimate experiment costs\n\nUnderstand the cost breakdown of the resources used in this tutorial","type":"content","url":"/part4-cost-management/hands-on-cost-management#learning-objectives","position":5},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"Step 1: Open the Billing Console"},"type":"lvl2","url":"/part4-cost-management/hands-on-cost-management#step-1-open-the-billing-console","position":6},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"Step 1: Open the Billing Console"},"content":"Go to \n\nconsole‚Äã.cloud‚Äã.google‚Äã.com\n\nIn the top left, click the navigation menu\n\nClick Billing\n\nFor AIMS Students:\n\nWhen you click Billing you will see a page saying ‚ÄúYou have multiple billing accounts‚Äù with the message ‚ÄúBilling account \n\naims.ac.za Billing Account is linked to this project‚Äù. This is expected since your project is linked to the AIMS institutional billing account.\n\nClick Go to linked billing account to access the billing dashboard for your project and continue with the steps below.","type":"content","url":"/part4-cost-management/hands-on-cost-management#step-1-open-the-billing-console","position":7},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"Step 2: Create a Budget"},"type":"lvl2","url":"/part4-cost-management/hands-on-cost-management#step-2-create-a-budget","position":8},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"Step 2: Create a Budget"},"content":"In the left sidebar, click Budgets and alerts, then click Create budget to create a new budget.\n\n\n\nScreenshot: Create a budget.\n\nWarning\n\nNotice the banner on the Budgets and alerts page that says ‚ÄúSetting a budget does not cap resource or API consumption.‚Äù This is important. GCP will send you an email alert when you reach your threshold but will not stop your resources automatically. You are still responsible for deleting resources when you are done.\n\nTo create a budget, we need to set up the scope (which projects and services to monitor), the amount (the total budget limit), and the actions (how you want to be notified when you hit your limit).\n\nScope:\n\nEnter a name for the budget, for example gcp-aims-tutorial-budget\n\nUnder Time range, leave it as Monthly\n\nUnder Projects, make sure your project is selected\n\nLeave all services selected so the budget covers all GCP services\n\nClick Next\n\n\n\nScreenshot: Set up the budget scope\n\nAmount\n\nUnder Budget type, select Specified amount\n\nEnter your target amount i.e. $10.\n\nNote for AIMS students: While your total allocation might be $2,000, setting a low monthly target like $10 or $20 helps you catch accidental ‚Äúresource leaks‚Äù early.\n\nClick Next\n\n\n\nScreenshot: Set up the budget amount\n\nActions\n\nBy default, Google Cloud creates three alert thresholds at 50%, 90%, and 100% of your budget. Keep these defaults:\n\n50%: Acts as an early warning that your experiment is halfway through its monthly allowance.\n\n90%: Serves as a critical ‚Äúwrap up‚Äù signal to finish your runs.\n\n100%: Notifies you that you have fully reached your limit.\n\nUnder Manage Notifications, make sure the box ‚ÄúEmail alerts to billing admins and users‚Äù is checked. This ensures you receive an immediate email notification the moment a threshold is crossed.\n\nClick Finish to finally create the budget.\n\n\n\nScreenshot: Set up the budget actions","type":"content","url":"/part4-cost-management/hands-on-cost-management#step-2-create-a-budget","position":9},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"Step 3: Verify the Budget"},"type":"lvl2","url":"/part4-cost-management/hands-on-cost-management#step-3-verify-the-budget","position":10},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"Step 3: Verify the Budget"},"content":"You should not see the budget listed on the Budget and alerts page with the spend progress bar and configured thresholds.\n\n\n\nScreenshot: Budgets and alerts page showing the newly created budget with spend progress.\n\nYou will receive an email notification when your spending reaches 50%, 90%, and 100% of the budget amount.\n\nApproximate costs for this tutorial\n\nYou can use the \n\nGCP Pricing Calculator to estimate costs before running any experiment.\n\nBased on europe-west4 pricing, the approximate costs for the resources used in this tutorial are:\n\nL4 GPU VM (g2-standard-4): ~$0.70/hour\n\nVertex AI Custom Training Job (same machine): ~$0.75/hour\n\nVertex AI Colab Enterprise L4 runtime: ~$0.70/hour\n\nGCS Standard Storage: ~$0.02/GB/month\n\nThe full tutorial from start to finish, including all hands-on sessions, should cost roughly $4-5 in total if you clean up resources promptly after each session.\n\nThe biggest risk is forgetting to delete a VM or runtime. A 20-minute training run on an NVIDIA L4 GPU costs roughly $0.25, but leaving a VM running overnight costs ~$16.","type":"content","url":"/part4-cost-management/hands-on-cost-management#step-3-verify-the-budget","position":11},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"üîë Key Takeaways"},"type":"lvl2","url":"/part4-cost-management/hands-on-cost-management#id-key-takeaways","position":12},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"üîë Key Takeaways"},"content":"Always set up billing budget and alert before running experiments, not after.\n\nGCP billing alerts notify you but do not stop your resources automatically. Manual cleanup of resources is always required.\n\nThe biggest cost driver is usually GPU compute time. Always delete VMs and runtimes when you are done.\n\nJob-based compute like Vertex AI Custom Training is almost always cheaper than always-on VMs for batch training since it terminates automatically when done.","type":"content","url":"/part4-cost-management/hands-on-cost-management#id-key-takeaways","position":13},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"üìö References and Further Reading"},"type":"lvl2","url":"/part4-cost-management/hands-on-cost-management#id-references-and-further-reading","position":14},{"hierarchy":{"lvl1":"Hands-On: Setting Up Billing Alerts and Budgets","lvl2":"üìö References and Further Reading"},"content":"GCP Billing Documentation - Official documentation on managing billing and budgets\n\nGCP Pricing Calculator - Estimate costs for any GCP service before running experiments\n\nCompute Engine Pricing - Full pricing breakdown for GPU VM instances\n\nVertex AI Pricing - Full pricing breakdown for Vertex AI training and serving\n\nGCS Pricing - Storage and data transfer pricing for Google Cloud Storage","type":"content","url":"/part4-cost-management/hands-on-cost-management#id-references-and-further-reading","position":15},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/part4-cost-management","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"","type":"content","url":"/part4-cost-management","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Cost Management for Machine Learning on Google Cloud"},"type":"lvl2","url":"/part4-cost-management#cost-management-for-machine-learning-on-google-cloud","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Cost Management for Machine Learning on Google Cloud"},"content":"Running machine learning experiments in the cloud is powerful, but it can get expensive fast. High-performance hardware like GPUs and TPUs are billed at a premium rate. To keep your budget under control, you need to be intentional about how you provision, use, and tear down your infrastructure.","type":"content","url":"/part4-cost-management#cost-management-for-machine-learning-on-google-cloud","position":3},{"hierarchy":{"lvl1":"Introduction","lvl3":"Core Cost Management Strategies","lvl2":"Cost Management for Machine Learning on Google Cloud"},"type":"lvl3","url":"/part4-cost-management#core-cost-management-strategies","position":4},{"hierarchy":{"lvl1":"Introduction","lvl3":"Core Cost Management Strategies","lvl2":"Cost Management for Machine Learning on Google Cloud"},"content":"1. Implement a ‚ÄúProvision-Run-Delete‚Äù Workflow\n\nGPU-enabled VMs and Notebook instances are billed for compute resources every second they are in a ‚ÄúRunning‚Äù state. Make it a habit to save your results to Google Cloud Storage (GCS) and delete the compute resource immediately after the task completes.\n\nUse gcloud compute instances list to verify that no active machines are still running and costing you money.\n\n2. Configure Budget Alerts and Quotas\n\nSet up Cloud Billing alerts to get email notifications when your spending reaches specific thresholds, like 25% or 50% of your budget.\n\nFor an extra layer of protection, you can also limit your GPU Quota in the IAM settings to prevent anyone in the project from accidentally launching a massive, expensive cluster.\n\n3. Prioritize Managed Training Jobs\n\nUse Vertex AI Custom Training for batch workloads. Unlike interactive notebooks or VMs, these jobs automatically provision the hardware, run your code, and shut down the infrastructure the moment the process finishes.\n\nThis removes the risk of human error where a VM is accidentally left running over a weekend.\n\n4. Clean Up Orphaned Persistent Disks\n\nWhen you delete a VM, the storage disk attached to it is often kept by default. These orphaned disks continue to accrue costs even though they are not attached to anything.\n\nPeriodically check the ‚ÄúDisks‚Äù section of the Cloud Console and delete any volumes that are no longer associated with an active VM.\n\n5. Utilize Spot Virtual Machines\n\nFor experiments that are not time-critical, use Spot VMs. These offer the same hardware at a discount of up to 90% compared to standard prices.\n\nThe trade-off is that Google can reclaim these instances if they need the capacity elsewhere. Just ensure your code saves checkpoints to GCS frequently so you do not lose progress.\n\n6. Avoid Over-Provisioning\n\nAvoid over-provisioning. High-end GPUs like the A100 or H100 are impressive but expensive. Start with smaller, cost-effective GPUs like the L4 or T4 for debugging and initial testing.\n\nOnly scale up to larger accelerators when your model architecture or data size specifically requires the extra memory.\n\n7. Offload Data to Cloud Storage\n\nStandard persistent disks are much more expensive per gigabyte than Google Cloud Storage. Store your large datasets and final model weights in GCS buckets instead of leaving them on a VM disk.\n\nThis allows you to delete expensive compute resources while keeping your data accessible for a fraction of the cost.\n\n8. Enable Idle Shutdown for Notebooks\n\nIf you use managed notebooks, always enable the Idle Shutdown feature. This acts as a safety net by automatically powering down the instance after a set period of inactivity, such as two hours, ensuring you do not pay for hardware you are not actively using.","type":"content","url":"/part4-cost-management#core-cost-management-strategies","position":5},{"hierarchy":{"lvl1":"Introduction","lvl2":"üìö References & Further Reading"},"type":"lvl2","url":"/part4-cost-management#id-references-further-reading","position":6},{"hierarchy":{"lvl1":"Introduction","lvl2":"üìö References & Further Reading"},"content":"To learn more about optimizing your Google Cloud expenses and managing AI infrastructure, you can explore the following resources:\n\nGoogle Cloud Billing Documentation: Learn how to set up budgets, alerts, and export billing data to BigQuery for advanced analysis.\n\nVertex AI Pricing Details: A comprehensive breakdown of costs for training jobs, notebook instances, and prediction endpoints.\n\nBest Practices for Cost Optimization on GCP: Part of the Google Cloud Architecture Framework, this guide covers organization-wide strategies for reducing waste.\n\nManaging GPU Quotas: Instructions on how to view and request changes to your hardware limits to prevent unexpected scaling.\n\nCloud Storage Storage Classes: Understand the difference between Standard, Nearline, and Coldline storage to optimize your data lifecycle costs.","type":"content","url":"/part4-cost-management#id-references-further-reading","position":7}]}