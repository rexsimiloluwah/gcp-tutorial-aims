<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment - AIMS GCP ML Tutorial</title><meta property="og:title" content="Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment - AIMS GCP ML Tutorial"/><meta name="generator" content="mystmd"/><meta name="description" content="A no-fluff guide to using GCP for ML experiments and research"/><meta property="og:description" content="A no-fluff guide to using GCP for ML experiments and research"/><meta name="keywords" content="GCP, AI, GPU, Machine Learning, Research, Experiments, MLOps"/><link rel="stylesheet" href="/gcp-tutorial-aims/build/_assets/app-2K3KGISG.css"/><link rel="stylesheet" href="/gcp-tutorial-aims/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><link rel="icon" href="/gcp-tutorial-aims/favicon.ico"/><link rel="stylesheet" href="/gcp-tutorial-aims/myst-theme.css"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="myst-skip-to-article fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="myst-skip-to-link block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="myst-skip-to-link block px-2 py-1 text-black underline">Skip to article content</a></div><dialog id="myst-no-css" style="position:fixed;left:0px;top:0px;width:100%;height:100vh;font-size:4rem;padding:1rem;color:black;background:white"><strong>Site not loading correctly?</strong><p>This may be due to an incorrect <code>BASE_URL</code> configuration. See<!-- --> <a href="https://mystmd.org/guide/deployment#deploy-base-url">the MyST Documentation</a> <!-- -->for reference.</p><script>
    (() => {
            // Test for has-styling variable set by the MyST stylesheet
            const node = document.currentScript.parentNode;
            const hasCSS = window.getComputedStyle(node).getPropertyValue("--has-styling");
            if (hasCSS === ""){
                    node.showModal();
            }

    })()
</script></dialog><div class="myst-top-nav bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-full top-0 z-30 h-[60px]"><nav class="myst-top-nav-bar flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><div class="block xl:hidden"><button class="myst-top-nav-menu-button flex items-center justify-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100 w-10 h-10"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 0 1 3.75 6h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 6.75ZM3 12a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 12Zm0 5.25a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75a.75.75 0 0 1-.75-.75Z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="myst-home-link flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/gcp-tutorial-aims/"><span class="text-md sm:text-xl tracking-tight sm:mr-5">AIMS GCP ML Tutorial</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R75cp:" data-state="closed" class="myst-search-bar flex items-center h-10 aspect-square sm:w-64 text-left text-gray-600 dark:text-gray-300 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 myst-search-bar-disabled hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="myst-search-text-placeholder hidden sm:block grow">Search</span><div aria-hidden="true" class="myst-search-shortcut items-center hidden mx-1 font-mono text-sm text-gray-600 dark:text-gray-300 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">⌘</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="myst-theme-button theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-10 h-10 mx-3" title="Toggle theme between light and dark mode" aria-label="Toggle theme between light and dark mode"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="myst-theme-moon-icon h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="myst-theme-sun-icon h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"></div><div class="hidden sm:block"></div></div></nav></div><div class="myst-primary-sidebar fixed xl:article-grid grid-gap xl:w-full xl:pointer-events-none overflow-auto max-xl:w-[75vw] max-xl:max-w-[350px] max-xl:!top-0 max-xl:h-screen hidden z-10" style="top:60px"><div class="myst-primary-sidebar-pointer pointer-events-auto xl:col-margin-left flex-col overflow-hidden max-xl:h-full hidden xl:flex"><div class="myst-primary-sidebar-nav flex-grow py-6 overflow-y-auto primary-scrollbar"><nav aria-label="Navigation" class="myst-primary-sidebar-topnav overflow-y-hidden transition-opacity lg:hidden ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="w-full px-1 dark:text-white font-medium"></div></nav><div class="my-3 border-b-2 lg:hidden"></div><nav aria-label="Table of Contents" class="myst-primary-sidebar-toc flex-grow overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="myst-toc w-full px-1 dark:text-white"><a title="AIMS GCP Tutorial" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold" href="/gcp-tutorial-aims/">AIMS GCP Tutorial</a><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 pl-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Part 1 - GCP Foundations" class="block break-words rounded py-2 grow cursor-pointer">Part 1 - GCP Foundations</div><button class="self-stretch flex items-center flex-none px-1 rounded-l-md group hover:bg-slate-300/30 focus-visible:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rmpsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rmpsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 pl-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Part 2 - Running Experiments On GCP VMs" class="block break-words rounded py-2 grow cursor-pointer">Part 2 - Running Experiments On GCP VMs</div><button class="self-stretch flex items-center flex-none px-1 rounded-l-md group hover:bg-slate-300/30 focus-visible:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rupsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rupsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 pl-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Part 3 - Vertex AI" class="block break-words rounded py-2 grow cursor-pointer">Part 3 - Vertex AI</div><button class="self-stretch flex items-center flex-none px-1 rounded-l-md group hover:bg-slate-300/30 focus-visible:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R16psp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R16psp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 pl-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Part 4 - Cost Management" class="block break-words rounded py-2 grow cursor-pointer">Part 4 - Cost Management</div><button class="self-stretch flex items-center flex-none px-1 rounded-l-md group hover:bg-slate-300/30 focus-visible:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1epsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1epsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 pl-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Appendix" class="block break-words rounded py-2 grow cursor-pointer">Appendix</div><button class="self-stretch flex items-center flex-none px-1 rounded-l-md group hover:bg-slate-300/30 focus-visible:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1mpsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1mpsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div></div></nav></div><div class="myst-primary-sidebar-footer flex-none py-6 transition-all duration-700 translate-y-6 opacity-0 ml-3 xl:ml-0 mr-3 max-w-[350px]"><a class="myst-made-with-myst flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><main class="article-grid grid-gap"><article class="article-grid subgrid-gap col-screen article content"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="myst-fm-block mb-8 pt-9"><div class="myst-fm-block-header flex items-center mb-5 h-6 text-sm font-light"><div class="flex-grow"></div><div class="myst-fm-block-badges"><a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer" class="myst-fm-license-cc-badge opacity-50 hover:opacity-100 text-inherit hover:text-inherit myst-fm-license-content" aria-label="Content License: Creative Commons Attribution 4.0 International (CC-BY-4.0)"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="myst-fm-license-cc-icon myst-fm-license-cc-icon-main inline-block mx-1"><title>Content License: Creative Commons Attribution 4.0 International (CC-BY-4.0)</title><path d="M12 2.2c2.7 0 5 1 7 2.9.9.9 1.6 2 2.1 3.1.5 1.2.7 2.4.7 3.8 0 1.3-.2 2.6-.7 3.8-.5 1.2-1.2 2.2-2.1 3.1-1 .9-2 1.7-3.2 2.2-1.2.5-2.5.7-3.7.7s-2.6-.3-3.8-.8c-1.2-.5-2.2-1.2-3.2-2.1s-1.6-2-2.1-3.2-.8-2.4-.8-3.7c0-1.3.2-2.5.7-3.7S4.2 6 5.1 5.1C7 3.2 9.3 2.2 12 2.2zM12 4c-2.2 0-4.1.8-5.6 2.3C5.6 7.1 5 8 4.6 9c-.4 1-.6 2-.6 3s.2 2.1.6 3c.4 1 1 1.8 1.8 2.6S8 19 9 19.4c1 .4 2 .6 3 .6s2.1-.2 3-.6c1-.4 1.9-1 2.7-1.8 1.5-1.5 2.3-3.3 2.3-5.6 0-1.1-.2-2.1-.6-3.1-.4-1-1-1.8-1.7-2.6C16.1 4.8 14.2 4 12 4zm-.1 6.4l-1.3.7c-.1-.3-.3-.5-.5-.6-.2-.1-.4-.2-.6-.2-.9 0-1.3.6-1.3 1.7 0 .5.1.9.3 1.3.2.3.5.5 1 .5.6 0 1-.3 1.2-.8l1.2.6c-.3.5-.6.9-1.1 1.1-.5.3-1 .4-1.5.4-.9 0-1.6-.3-2.1-.8-.5-.6-.8-1.3-.8-2.3 0-.9.3-1.7.8-2.2.6-.6 1.3-.8 2.1-.8 1.2 0 2.1.4 2.6 1.4zm5.6 0l-1.3.7c-.1-.3-.3-.5-.5-.6-.2-.1-.4-.2-.6-.2-.9 0-1.3.6-1.3 1.7 0 .5.1.9.3 1.3.2.3.5.5 1 .5.6 0 1-.3 1.2-.8l1.2.6c-.3.5-.6.9-1.1 1.1-.4.2-.9.3-1.4.3-.9 0-1.6-.3-2.1-.8s-.8-1.3-.8-2.2c0-.9.3-1.7.8-2.2.5-.5 1.2-.8 2-.8 1.2 0 2.1.4 2.6 1.4z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="myst-fm-license-cc-icon myst-fm-license-cc-icon-by inline-block mr-1"><title>Credit must be given to the creator</title><path d="M12 2.2c2.7 0 5 .9 6.9 2.8 1.9 1.9 2.8 4.2 2.8 6.9s-.9 5-2.8 6.8c-2 1.9-4.3 2.9-7 2.9-2.6 0-4.9-1-6.9-2.9-1.8-1.7-2.8-4-2.8-6.7s1-5 2.9-6.9C7 3.2 9.3 2.2 12 2.2zM12 4c-2.2 0-4.1.8-5.6 2.3C4.8 8 4 9.9 4 12c0 2.2.8 4 2.4 5.6C8 19.2 9.8 20 12 20c2.2 0 4.1-.8 5.7-2.4 1.5-1.5 2.3-3.3 2.3-5.6 0-2.2-.8-4.1-2.3-5.7C16.1 4.8 14.2 4 12 4zm2.6 5.6v4h-1.1v4.7h-3v-4.7H9.4v-4c0-.2.1-.3.2-.4.1-.2.2-.2.4-.2h4c.2 0 .3.1.4.2.2.1.2.2.2.4zm-4-2.5c0-.9.5-1.4 1.4-1.4s1.4.5 1.4 1.4c0 .9-.5 1.4-1.4 1.4s-1.4-.5-1.4-1.4z"></path></svg></a><a href="https://github.com/rexsimiloluwa/aims-ai-gcp-tutorial" title="GitHub Repository: rexsimiloluwa/aims-ai-gcp-tutorial" target="_blank" rel="noopener noreferrer" class="myst-fm-github-link text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="myst-fm-github-icon inline-block mr-1 opacity-60 hover:opacity-100"><path d="M12 2.5c-5.4 0-9.8 4.4-9.8 9.7 0 4.3 2.8 8 6.7 9.2.5.1.7-.2.7-.5v-1.8c-2.4.5-3.1-.6-3.3-1.1-.1-.3-.6-1.1-1-1.4-.3-.2-.8-.6 0-.6s1.3.7 1.5 1c.9 1.5 2.3 1.1 2.8.8.1-.6.3-1.1.6-1.3-2.2-.2-4.4-1.1-4.4-4.8 0-1.1.4-1.9 1-2.6-.1-.2-.4-1.2.1-2.6 0 0 .8-.3 2.7 1 .8-.2 1.6-.3 2.4-.3.8 0 1.7.1 2.4.3 1.9-1.3 2.7-1 2.7-1 .5 1.3.2 2.3.1 2.6.6.7 1 1.5 1 2.6 0 3.7-2.3 4.6-4.4 4.8.4.3.7.9.7 1.8V21c0 .3.2.6.7.5 3.9-1.3 6.6-4.9 6.6-9.2 0-5.4-4.4-9.8-9.8-9.8z"></path></svg></a></div><a href="https://github.com/rexsimiloluwa/aims-ai-gcp-tutorial/edit/main/appendix/01-gpu-maths.md" title="Edit This Page" target="_blank" rel="noopener noreferrer" class="myst-fm-edit-link text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="myst-fm-edit-icon inline-block mr-1 opacity-60 hover:opacity-100"><path stroke-linecap="round" stroke-linejoin="round" d="m16.862 4.487 1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897l8.932-8.931Zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10"></path></svg></a><div class="myst-fm-downloads-dropdown relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="myst-fm-downloads-button relative ml-2 -mr-1" id="headlessui-menu-button-:Rs8ucp:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="myst-fm-downloads-icon"><title>Download</title><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="myst-fm-block-title mb-0">Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment</h1><header class="myst-fm-authors-affiliations mt-4 not-prose"><div class="myst-fm-authors-list"><span class="myst-fm-author font-semibold text-sm myst-fm-author-item inline-block"><button class="myst-fm-author-popover focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R78ucp:" data-state="closed"><span class="myst-fm-author-name">AIMS TtT Team</span></button></span></div></header></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div id="skip-to-article"></div><p>Before provisioning a GPU VM or submitting a Vertex AI training job, it is worth spending a few minutes estimating whether your chosen GPU can actually fit your experiment. Running out of GPU memory mid-training is one of the most common and frustrating errors in ML, and it is entirely avoidable with some simple upfront calculation.</p><h2 id="what-determines-gpu-memory-usage" class="relative group"><span class="heading-text">What Determines GPU Memory Usage?</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#what-determines-gpu-memory-usage" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>The GPU memory required for a training run is determined by four main factors:</p><ul><li><p><strong>Model parameters</strong> - the weights of the model itself. The memory cost depends on how many parameters the model has and the precision they are stored in</p></li><li><p><strong>Gradients</strong> - a copy of the model parameters computed during the backward pass. In full fine-tuning these match the size of the model weights</p></li><li><p><strong>Optimizer states</strong> - the Adam optimizer stores two additional copies of the parameters (momentum and variance), roughly doubling the gradient memory cost</p></li><li><p><strong>Activations</strong> - intermediate values computed during the forward pass. These scale with batch size and sequence length</p></li></ul><h2 id="worked-example-fine-tuning-gemma-3-1b-model-with-lora" class="relative group"><span class="heading-text">Worked Example: Fine-tuning Gemma 3 1B Model with LoRA</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#worked-example-fine-tuning-gemma-3-1b-model-with-lora" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Let us walk through how to estimate the GPU memory required for fine-tuning Gemma 3 1B with LoRA in <code>bfloat16</code>, which is exactly the experiment we used in this tutorial.</p><p><strong>Step 1: Base model weights</strong></p><p>Gemma 3 1B has 1 billion parameters. We load it in <code>bfloat16</code>, where each parameter takes 2 bytes (16 bits = 2 bytes):</p><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-stone-200/10"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-text" style="white-space:pre">1,000,000,000 parameters x 2 bytes = 2,000,000,000 bytes = ~2GB</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><p><strong>Step 2: LoRA adapter weights</strong></p><p>With LoRA rank 8 targeting <code>q_proj</code> and <code>v_proj</code>, we are training roughly 0.1% of the model’s parameters. This adds a negligible amount of memory:</p><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-stone-200/10"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-text" style="white-space:pre">0.1% x 2GB = ~0.002GB</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><p><strong>Step 3: Gradients and optimizer states</strong></p><p>Since LoRA freezes the base model, gradients and optimizer states only apply to the small set of trainable LoRA parameters, not the full model. This adds roughly:</p><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-stone-200/10"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-text" style="white-space:pre">~0.1GB</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><p><strong>Step 4: Activations</strong></p><p>Activations scale with batch size and sequence length. With batch size 8 and max length 256:</p><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-stone-200/10"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-text" style="white-space:pre">~0.3GB</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><p><strong>Total estimate:</strong></p><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-stone-200/10"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-text" style="white-space:pre">2GB (base model) + 0.002GB (LoRA) + 0.1GB (gradients) + 0.3GB (activations) = ~2.5GB</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><p>This is why a 24GB NVIDIA L4 GPU is more than sufficient for this experiment. We are only using about 10% of its available VRAM. You could even run it on a smaller 16GB T4 with room to spare.</p><blockquote><p><strong>Note:</strong> These are rough estimates. Actual memory usage can vary depending on the framework version, attention implementation, and other factors. It makes sense to run a quick test with a small number of samples before committing to a full training run.</p></blockquote><h2 id="what-if-you-were-to-run-this-without-lora" class="relative group"><span class="heading-text">What If You Were to Run This Without LoRA?</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#what-if-you-were-to-run-this-without-lora" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>For comparison, here is what full fine-tuning of Gemma 3 1B in float32 would cost:</p><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-stone-200/10"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-text" style="white-space:pre">1B parameters x 4 bytes (float32) = 4GB (model weights)
+ 4GB (gradients)
+ 8GB (Adam optimizer states)
+ ~1GB (activations)
= ~17GB total</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><p>This would not fit on a 16GB T4 and would be tight on a 24GB L4. This is exactly why LoRA is so valuable. It reduces the memory requirement from ~17GB to ~2.5GB for the same base model.</p><h2 id="estimating-gpu-hours" class="relative group"><span class="heading-text">Estimating GPU Hours</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#estimating-gpu-hours" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Once you know your experiment fits in memory, the next question is how long it will take. A simple estimate:</p><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-stone-200/10"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-text" style="white-space:pre">GPU hours = (num_samples x num_epochs) / (steps_per_second x batch_size x 3600)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><p>For our experiment on the L4, we use <strong>3 steps/second</strong> as our estimate. This comes directly from the training logs we observed during the hands-on session, where the L4 consistently processed between 2.7 and 3.0 steps per second for this specific workload (Gemma 3 1B, LoRA rank 8, bfloat16, batch size 8, sequence length 256). We round up to 3 for a conservative estimate:</p><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-stone-200/10"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-text" style="white-space:pre">(10,000 samples x 2 epochs) / (3 steps/second x 8 batch size x 3600 seconds)
= 20,000 / 86,400
= ~0.23 hours (~14 minutes)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><p>This matches what we observed in practice, where the full training run completed in roughly 15 minutes.</p><p>A few things that affect training speed:</p><ul><li><p><strong>Batch size</strong> - larger batches process more samples per step, reducing total steps and training time. However, the tradeoff is that larger batch sizes require more GPU VRAM to store activations, so there is a practical upper limit based on how much memory your GPU has available</p></li><li><p><strong>Sequence length</strong> - longer sequences increase the cost of each step due to the attention mechanism scaling quadratically with sequence length</p></li><li><p><strong>Precision</strong> - bfloat16 is roughly 2x faster than float32 on modern GPUs</p></li><li><p><strong>GPU generation</strong> - an A100 is roughly 3 to 4x faster per step than an L4 for the same workload</p></li></ul><p>Use the <a target="_blank" rel="noreferrer" href="https://cloud.google.com/products/calculator" class="link whitespace-nowrap"><span class="link-text whitespace-normal">GCP Pricing Calculator</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> with your estimated GPU hours to get a cost estimate before starting a run.</p><h2 id="common-gcp-gpus-and-when-to-use-them" class="relative group"><span class="heading-text">Common GCP GPUs and When to Use Them</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#common-gcp-gpus-and-when-to-use-them" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><table class=""><tbody><tr class=""><th class="">GPU</th><th class="">VRAM</th><th class="">Best For</th></tr><tr class=""><td class="">NVIDIA T4</td><td class="">16GB</td><td class="">Small models up to 3B, inference, cost-sensitive runs</td></tr><tr class=""><td class="">NVIDIA L4</td><td class="">24GB</td><td class="">Models up to 7B with LoRA, good price/performance</td></tr><tr class=""><td class="">NVIDIA A100 40GB</td><td class="">40GB</td><td class="">Models up to 13B full fine-tune, large batch training</td></tr><tr class=""><td class="">NVIDIA A100 80GB</td><td class="">80GB</td><td class="">Models up to 70B with LoRA, large context lengths</td></tr><tr class=""><td class="">NVIDIA H100</td><td class="">80GB</td><td class="">Largest models, fastest training, highest cost</td></tr></tbody></table><h2 id="practical-tips" class="relative group"><span class="heading-text">Practical Tips</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#practical-tips" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><ul><li><p><strong>Start small and scale up.</strong> Run a quick test with a small number of training examples before committing to a full run. If it fits in memory, scale up.</p></li><li><p><strong>Reduce batch size if you hit OOM.</strong> Halving the batch size roughly halves activation memory. Use gradient accumulation to compensate for effective batch size.</p></li><li><p><strong>Use bfloat16.</strong> Switching from float32 to bfloat16 halves model and gradient memory with minimal impact on training quality. This is why we set <code>bf16=True</code> in the training arguments.</p></li><li><p><strong>Check memory usage during training.</strong> Run <code>nvidia-smi</code> in a separate terminal or tmux window during training to see live GPU memory utilization.</p></li></ul><h2 id="references-and-further-reading" class="relative group"><span class="heading-text">References and Further Reading</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#references-and-further-reading" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><ul><li><p><a target="_blank" rel="noreferrer" href="https://www.nvidia.com/en-us/data-center/products/gpus/" class="link whitespace-nowrap"><span class="link-text whitespace-normal">NVIDIA GPU Specifications</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> - Full specs for all NVIDIA data center GPUs including VRAM and compute capacity</p></li><li><p><a target="_blank" rel="noreferrer" href="https://cloud.google.com/compute/gpus-pricing" class="link whitespace-nowrap"><span class="link-text whitespace-normal">GCP GPU Pricing</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> - Full pricing breakdown for GPU VM instances on GCP</p></li><li><p><a target="_blank" rel="noreferrer" href="https://huggingface.co/spaces/hf-accelerate/model-memory-usage" class="link whitespace-nowrap"><span class="link-text whitespace-normal">HuggingFace Model Memory Calculator</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> - A handy tool for estimating memory requirements for HuggingFace models</p></li><li><p><a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/2106.09685" class="link whitespace-nowrap"><span class="link-text whitespace-normal">LoRA: Low-Rank Adaptation of Large Language Models</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> - The original LoRA paper by Hu et al.</p></li><li><p><a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/1710.03740" class="link whitespace-nowrap"><span class="link-text whitespace-normal">Mixed Precision Training</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> - The paper introducing bfloat16 and float16 training</p></li></ul><div class="myst-backmatter-parts"></div><div class="myst-footer-links flex pt-10 mb-10 space-x-4"><a class="myst-footer-link flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700 myst-footer-link-prev" href="/gcp-tutorial-aims/part4-cost-management/hands-on-cost-management"><div class="flex h-full align-middle"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="myst-footer-link-icon self-center transition-transform group-hover:-translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M10.5 19.5 3 12m0 0 7.5-7.5M3 12h18"></path></svg><div class="flex-grow text-right"><div class="myst-footer-link-group text-xs text-gray-500 dark:text-gray-400">Part 4 - Cost Management</div>Hands-On: Setting Up Billing Alerts and Budgets</div></div></a></div></article></main><script>((a,l)=>{if(!window.history.state||!window.history.state.key){let u=Math.random().toString(32).slice(2);window.history.replaceState({key:u},"")}try{let d=JSON.parse(sessionStorage.getItem(a)||"{}")[l||window.history.state.key];typeof d=="number"&&window.scrollTo(0,d)}catch(u){console.error(u),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/gcp-tutorial-aims/build/entry.client-PCJPW7TK.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-AQ2CODAG.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-JJXTQVMA.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-OZE3FFNP.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-G62B6HZR.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-C4DFGG5C.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-J7TUH54J.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-FZ2S7OYD.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-JEM6JXYA.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-34XIY2DH.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-KQM5FBHR.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-7HNKBP4B.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-CUKUDK3R.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-3EBOCCHJ.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-O4VQNZ62.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-4OEDG4JQ.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/root-PMP5BIHC.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/_shared/chunk-IX5KPAHP.js"/><link rel="modulepreload" href="/gcp-tutorial-aims/build/routes/$-5ZLZ2O3Y.js"/><script>window.__remixContext = {"url":"/appendix/gpu-maths","state":{"loaderData":{"root":{"config":{"version":3,"myst":"1.8.1","title":"AIMS GCP ML Tutorial","options":{"favicon":"/gcp-tutorial-aims/build/favicon-95849f84c6f92f7b96b6606b5f8b6f36.ico","logo_text":"AIMS GCP ML Tutorial","folders":true},"nav":[],"actions":[],"projects":[{"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true}},"title":"AIMS GCP Tutorial","description":"A no-fluff guide to using GCP for ML experiments and research","authors":[{"id":"AIMS TtT Team","name":"AIMS TtT Team"}],"github":"https://github.com/rexsimiloluwa/aims-ai-gcp-tutorial","keywords":["GCP","AI","GPU","Machine Learning","Research","Experiments","MLOps"],"id":"7eddb7b1-9986-44d1-8417-fe2b2bb4b498","toc":[{"file":"index.md"},{"children":[{"file":"part1-gcp-foundations/index.md"},{"file":"part1-gcp-foundations/01-understanding-gcp.md"},{"file":"part1-gcp-foundations/02-gcp-console.md"},{"file":"part1-gcp-foundations/03-the-gcloud-cli.md"},{"file":"part1-gcp-foundations/04-hands-on-gcp-console.md"},{"file":"part1-gcp-foundations/05-hands-on-gcloud-cli.md"}],"title":"Part 1 - GCP Foundations"},{"children":[{"file":"part2-running-experiments-on-gcp-vms/index.md"},{"file":"part2-running-experiments-on-gcp-vms/01-hands-on-running-ai-experiments-on-gcp-vms.md"}],"title":"Part 2 - Running Experiments On GCP VMs"},{"children":[{"file":"part3-vertex-ai/index.md"},{"file":"part3-vertex-ai/01-hands-on-vertex-ai-custom-training-job.md"},{"file":"part3-vertex-ai/02-hands-on-vertex-ai-notebook-colab-enterprise.md"}],"title":"Part 3 - Vertex AI"},{"children":[{"file":"part4-cost-management/index.md"},{"file":"part4-cost-management/01-hands-on-cost-management.md"}],"title":"Part 4 - Cost Management"},{"children":[{"file":"appendix/01-gpu-maths.md"},{"file":"append"}],"title":"Appendix"}],"exports":[],"bibliography":[],"index":"index","pages":[{"level":1,"title":"Part 1 - GCP Foundations"},{"slug":"part1-gcp-foundations.index","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.understanding-gcp","title":"GCP Core Concepts","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.gcp-console","title":"The GCP Console","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/gcp-console-overview-3bc9f60c20034de4d1d7608c1c20958b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.the-gcloud-cli","title":"The Google Cloud CLI (gcloud)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.hands-on-gcp-console","title":"Hands-On 01: Create a VM via GCP Console","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/hands-on1-gcp-consol-455c36015a61307f47b7e22a1c0c327a.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.hands-on-gcloud-cli","title":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/hands-on2-gcloud-vm--5dab86ddf7d1bb7cb3b274cc2bd0b0cc.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Part 2 - Running Experiments On GCP VMs"},{"slug":"part2-running-experiments-on-gcp-vms.index","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part2-running-experiments-on-gcp-vms.hands-on-running-ai-experiments-on-gcp-vms","title":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/create_new_project_w-03c4f91a18e3e9153e1f0e80158d0ad8.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Part 3 - Vertex AI"},{"slug":"part3-vertex-ai.index","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part3-vertex-ai.hands-on-vertex-ai-custom-training-job","title":"Hands-On 01: Vertex AI Custom Training Jobs","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/gcloud-submit-vertex-bc8a636bcd50705f5dfebf9ad2877d2a.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part3-vertex-ai.hands-on-vertex-ai-notebook-colab-enterprise","title":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/gcp-console-colab-en-667cd5e857b7833a4103273ae3f7f888.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Part 4 - Cost Management"},{"slug":"part4-cost-management.index","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part4-cost-management.hands-on-cost-management","title":"Hands-On: Setting Up Billing Alerts and Budgets","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/gcp-console-create-b-738ab6aaceb92003b75efc827d6ff81d.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Appendix"},{"slug":"appendix.gpu-maths","title":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"CONTENT_CDN_PORT":"3100","MODE":"static","BASE_URL":"/gcp-tutorial-aims"},"routes/$":{"config":{"version":3,"myst":"1.8.1","title":"AIMS GCP ML Tutorial","options":{"favicon":"/gcp-tutorial-aims/build/favicon-95849f84c6f92f7b96b6606b5f8b6f36.ico","logo_text":"AIMS GCP ML Tutorial","folders":true},"nav":[],"actions":[],"projects":[{"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true}},"title":"AIMS GCP Tutorial","description":"A no-fluff guide to using GCP for ML experiments and research","authors":[{"id":"AIMS TtT Team","name":"AIMS TtT Team"}],"github":"https://github.com/rexsimiloluwa/aims-ai-gcp-tutorial","keywords":["GCP","AI","GPU","Machine Learning","Research","Experiments","MLOps"],"id":"7eddb7b1-9986-44d1-8417-fe2b2bb4b498","toc":[{"file":"index.md"},{"children":[{"file":"part1-gcp-foundations/index.md"},{"file":"part1-gcp-foundations/01-understanding-gcp.md"},{"file":"part1-gcp-foundations/02-gcp-console.md"},{"file":"part1-gcp-foundations/03-the-gcloud-cli.md"},{"file":"part1-gcp-foundations/04-hands-on-gcp-console.md"},{"file":"part1-gcp-foundations/05-hands-on-gcloud-cli.md"}],"title":"Part 1 - GCP Foundations"},{"children":[{"file":"part2-running-experiments-on-gcp-vms/index.md"},{"file":"part2-running-experiments-on-gcp-vms/01-hands-on-running-ai-experiments-on-gcp-vms.md"}],"title":"Part 2 - Running Experiments On GCP VMs"},{"children":[{"file":"part3-vertex-ai/index.md"},{"file":"part3-vertex-ai/01-hands-on-vertex-ai-custom-training-job.md"},{"file":"part3-vertex-ai/02-hands-on-vertex-ai-notebook-colab-enterprise.md"}],"title":"Part 3 - Vertex AI"},{"children":[{"file":"part4-cost-management/index.md"},{"file":"part4-cost-management/01-hands-on-cost-management.md"}],"title":"Part 4 - Cost Management"},{"children":[{"file":"appendix/01-gpu-maths.md"},{"file":"append"}],"title":"Appendix"}],"exports":[],"bibliography":[],"index":"index","pages":[{"level":1,"title":"Part 1 - GCP Foundations"},{"slug":"part1-gcp-foundations.index","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.understanding-gcp","title":"GCP Core Concepts","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.gcp-console","title":"The GCP Console","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/gcp-console-overview-3bc9f60c20034de4d1d7608c1c20958b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.the-gcloud-cli","title":"The Google Cloud CLI (gcloud)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.hands-on-gcp-console","title":"Hands-On 01: Create a VM via GCP Console","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/hands-on1-gcp-consol-455c36015a61307f47b7e22a1c0c327a.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.hands-on-gcloud-cli","title":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/hands-on2-gcloud-vm--5dab86ddf7d1bb7cb3b274cc2bd0b0cc.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Part 2 - Running Experiments On GCP VMs"},{"slug":"part2-running-experiments-on-gcp-vms.index","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part2-running-experiments-on-gcp-vms.hands-on-running-ai-experiments-on-gcp-vms","title":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/create_new_project_w-03c4f91a18e3e9153e1f0e80158d0ad8.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Part 3 - Vertex AI"},{"slug":"part3-vertex-ai.index","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part3-vertex-ai.hands-on-vertex-ai-custom-training-job","title":"Hands-On 01: Vertex AI Custom Training Jobs","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/gcloud-submit-vertex-bc8a636bcd50705f5dfebf9ad2877d2a.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part3-vertex-ai.hands-on-vertex-ai-notebook-colab-enterprise","title":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/gcp-console-colab-en-667cd5e857b7833a4103273ae3f7f888.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Part 4 - Cost Management"},{"slug":"part4-cost-management.index","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part4-cost-management.hands-on-cost-management","title":"Hands-On: Setting Up Billing Alerts and Budgets","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/gcp-console-create-b-738ab6aaceb92003b75efc827d6ff81d.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Appendix"},{"slug":"appendix.gpu-maths","title":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"page":{"version":3,"kind":"Article","sha256":"2e22525fd9dd0b789dfdd454d53a07970a7b896b3cdb392b79b37ad5e733f11d","slug":"appendix.gpu-maths","location":"/appendix/01-gpu-maths.md","dependencies":[],"frontmatter":{"title":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","content_includes_title":false,"authors":[{"id":"AIMS TtT Team","name":"AIMS TtT Team"}],"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true}},"github":"https://github.com/rexsimiloluwa/aims-ai-gcp-tutorial","keywords":["GCP","AI","GPU","Machine Learning","Research","Experiments","MLOps"],"numbering":{"title":{"offset":1}},"source_url":"https://github.com/rexsimiloluwa/aims-ai-gcp-tutorial/blob/main/appendix/01-gpu-maths.md","edit_url":"https://github.com/rexsimiloluwa/aims-ai-gcp-tutorial/edit/main/appendix/01-gpu-maths.md","exports":[{"format":"md","filename":"01-gpu-maths.md","url":"/gcp-tutorial-aims/build/01-gpu-maths-6b8938fe000d8549556db1e6e7389fdd.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Before provisioning a GPU VM or submitting a Vertex AI training job, it is worth spending a few minutes estimating whether your chosen GPU can actually fit your experiment. Running out of GPU memory mid-training is one of the most common and frustrating errors in ML, and it is entirely avoidable with some simple upfront calculation.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"av1RTW6Kn9"}],"key":"GkSafw1YWb"},{"type":"heading","depth":2,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"What Determines GPU Memory Usage?","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"S7WSjShrBP"}],"identifier":"what-determines-gpu-memory-usage","label":"What Determines GPU Memory Usage?","html_id":"what-determines-gpu-memory-usage","implicit":true,"key":"dJxhBSidgW"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The GPU memory required for a training run is determined by four main factors:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"kX76xDhU5G"}],"key":"KciSkocTC0"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Model parameters","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"IuheHh5plz"}],"key":"X6Ax4AZwLi"},{"type":"text","value":" - the weights of the model itself. The memory cost depends on how many parameters the model has and the precision they are stored in","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"cItxDRUkc4"}],"key":"PuUlkTOoEL"}],"key":"lr56CLC5Ma"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Gradients","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"Ft3gbs8dvL"}],"key":"MIj384LmZI"},{"type":"text","value":" - a copy of the model parameters computed during the backward pass. In full fine-tuning these match the size of the model weights","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"t7dJLwD93M"}],"key":"JV0njXjyzG"}],"key":"dvh2lnwDEt"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Optimizer states","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"N8kHyRrrIy"}],"key":"DfEliZEH5I"},{"type":"text","value":" - the Adam optimizer stores two additional copies of the parameters (momentum and variance), roughly doubling the gradient memory cost","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"ZpKrvaLFUu"}],"key":"nrRLSoffRz"}],"key":"c71IfTHmsk"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Activations","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"o5HZiMuoeI"}],"key":"rkA6Vc0SGH"},{"type":"text","value":" - intermediate values computed during the forward pass. These scale with batch size and sequence length","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"e447bdVuQ5"}],"key":"qHgXSkamlF"}],"key":"rjpn86Fq46"}],"key":"LfoVwwoLbf"},{"type":"heading","depth":2,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Worked Example: Fine-tuning Gemma 3 1B Model with LoRA","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"kXLblnP4eF"}],"identifier":"worked-example-fine-tuning-gemma-3-1b-model-with-lora","label":"Worked Example: Fine-tuning Gemma 3 1B Model with LoRA","html_id":"worked-example-fine-tuning-gemma-3-1b-model-with-lora","implicit":true,"key":"hqlMbaN5RI"},{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Let us walk through how to estimate the GPU memory required for fine-tuning Gemma 3 1B with LoRA in ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"P5dTG8i3YQ"},{"type":"inlineCode","value":"bfloat16","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"ffjVLfuXHw"},{"type":"text","value":", which is exactly the experiment we used in this tutorial.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"AURlt7WMHi"}],"key":"S8VKJMT8jM"},{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"strong","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Step 1: Base model weights","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"cH9WQ4asmi"}],"key":"ctBQuvO1fw"}],"key":"VJxNnYwAC3"},{"type":"paragraph","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"Gemma 3 1B has 1 billion parameters. We load it in ","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"w2oTHaaLWk"},{"type":"inlineCode","value":"bfloat16","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"UrJkTyB5ZO"},{"type":"text","value":", where each parameter takes 2 bytes (16 bits = 2 bytes):","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"AsGnJrCVXs"}],"key":"yalQOArFR2"},{"type":"code","lang":"","value":"1,000,000,000 parameters x 2 bytes = 2,000,000,000 bytes = ~2GB","position":{"start":{"line":21,"column":1},"end":{"line":23,"column":1}},"key":"oGDtiGzqFR"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"strong","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Step 2: LoRA adapter weights","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"bRjartBGau"}],"key":"WztYudzlqG"}],"key":"wlYQikI5di"},{"type":"paragraph","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"With LoRA rank 8 targeting ","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"TxPcFmpRmt"},{"type":"inlineCode","value":"q_proj","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"MhDNSXv64w"},{"type":"text","value":" and ","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"IPNsKU8oqU"},{"type":"inlineCode","value":"v_proj","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"TNQC7zMfrw"},{"type":"text","value":", we are training roughly 0.1% of the model’s parameters. This adds a negligible amount of memory:","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"GWZ6pFbkcH"}],"key":"FPxPI4vhUo"},{"type":"code","lang":"","value":"0.1% x 2GB = ~0.002GB","position":{"start":{"line":29,"column":1},"end":{"line":31,"column":1}},"key":"bBbNZRWKnG"},{"type":"paragraph","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"strong","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"Step 3: Gradients and optimizer states","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"RPgY0uukfO"}],"key":"aCd15BgzXF"}],"key":"IVjo65yOwB"},{"type":"paragraph","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"Since LoRA freezes the base model, gradients and optimizer states only apply to the small set of trainable LoRA parameters, not the full model. This adds roughly:","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"KzGzmWykNm"}],"key":"eHgEwc9jZE"},{"type":"code","lang":"","value":"~0.1GB","position":{"start":{"line":36,"column":1},"end":{"line":38,"column":1}},"key":"ZMopAR8A9Q"},{"type":"paragraph","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"strong","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"text","value":"Step 4: Activations","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"oyh5QRbFwM"}],"key":"K0LSv0bCMB"}],"key":"thmS9yN9qk"},{"type":"paragraph","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"text","value":"Activations scale with batch size and sequence length. With batch size 8 and max length 256:","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"F2Qd8vw0Tr"}],"key":"Zi1kywpKQ2"},{"type":"code","lang":"","value":"~0.3GB","position":{"start":{"line":43,"column":1},"end":{"line":45,"column":1}},"key":"zui2lFTJkZ"},{"type":"paragraph","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"strong","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"Total estimate:","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"pSb317nJ6o"}],"key":"aqAfSYQXKi"}],"key":"rsHGXtHVYb"},{"type":"code","lang":"","value":"2GB (base model) + 0.002GB (LoRA) + 0.1GB (gradients) + 0.3GB (activations) = ~2.5GB","position":{"start":{"line":48,"column":1},"end":{"line":50,"column":1}},"key":"nEhtWphscw"},{"type":"paragraph","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"children":[{"type":"text","value":"This is why a 24GB NVIDIA L4 GPU is more than sufficient for this experiment. We are only using about 10% of its available VRAM. You could even run it on a smaller 16GB T4 with room to spare.","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"key":"EYaaanUPIw"}],"key":"ma6JCQe8RM"},{"type":"blockquote","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"children":[{"type":"strong","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"children":[{"type":"text","value":"Note:","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"key":"mWnOb5ECIP"}],"key":"Tpw36rm6uc"},{"type":"text","value":" These are rough estimates. Actual memory usage can vary depending on the framework version, attention implementation, and other factors. It makes sense to run a quick test with a small number of samples before committing to a full training run.","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"key":"YEJgOx7UV4"}],"key":"C0SZEjjBoQ"}],"key":"Kp1WNilRmg"},{"type":"heading","depth":2,"position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"children":[{"type":"text","value":"What If You Were to Run This Without LoRA?","position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"key":"XUFDsgrfbn"}],"identifier":"what-if-you-were-to-run-this-without-lora","label":"What If You Were to Run This Without LoRA?","html_id":"what-if-you-were-to-run-this-without-lora","implicit":true,"key":"fwESWFJ66X"},{"type":"paragraph","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"children":[{"type":"text","value":"For comparison, here is what full fine-tuning of Gemma 3 1B in float32 would cost:","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"key":"sBOtj6IRHe"}],"key":"pxigxoASEY"},{"type":"code","lang":"","value":"1B parameters x 4 bytes (float32) = 4GB (model weights)\n+ 4GB (gradients)\n+ 8GB (Adam optimizer states)\n+ ~1GB (activations)\n= ~17GB total","position":{"start":{"line":59,"column":1},"end":{"line":65,"column":1}},"key":"Z1mOxPuEbt"},{"type":"paragraph","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"children":[{"type":"text","value":"This would not fit on a 16GB T4 and would be tight on a 24GB L4. This is exactly why LoRA is so valuable. It reduces the memory requirement from ~17GB to ~2.5GB for the same base model.","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"key":"yQNq8kOO43"}],"key":"iGAVSeQiN3"},{"type":"heading","depth":2,"position":{"start":{"line":69,"column":1},"end":{"line":69,"column":1}},"children":[{"type":"text","value":"Estimating GPU Hours","position":{"start":{"line":69,"column":1},"end":{"line":69,"column":1}},"key":"s5yxr7pZWY"}],"identifier":"estimating-gpu-hours","label":"Estimating GPU Hours","html_id":"estimating-gpu-hours","implicit":true,"key":"Eu3WkBvTDy"},{"type":"paragraph","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"children":[{"type":"text","value":"Once you know your experiment fits in memory, the next question is how long it will take. A simple estimate:","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"key":"LHddquCgOp"}],"key":"boYcKIIDCZ"},{"type":"code","lang":"","value":"GPU hours = (num_samples x num_epochs) / (steps_per_second x batch_size x 3600)","position":{"start":{"line":72,"column":1},"end":{"line":74,"column":1}},"key":"xhwEhRRZ68"},{"type":"paragraph","position":{"start":{"line":76,"column":1},"end":{"line":76,"column":1}},"children":[{"type":"text","value":"For our experiment on the L4, we use ","position":{"start":{"line":76,"column":1},"end":{"line":76,"column":1}},"key":"E2TZ5OUYV9"},{"type":"strong","position":{"start":{"line":76,"column":1},"end":{"line":76,"column":1}},"children":[{"type":"text","value":"3 steps/second","position":{"start":{"line":76,"column":1},"end":{"line":76,"column":1}},"key":"lRQxNM18RX"}],"key":"a1Qy8sIPdK"},{"type":"text","value":" as our estimate. This comes directly from the training logs we observed during the hands-on session, where the L4 consistently processed between 2.7 and 3.0 steps per second for this specific workload (Gemma 3 1B, LoRA rank 8, bfloat16, batch size 8, sequence length 256). We round up to 3 for a conservative estimate:","position":{"start":{"line":76,"column":1},"end":{"line":76,"column":1}},"key":"pmeerPTrt6"}],"key":"jVm4vqzfzr"},{"type":"code","lang":"","value":"(10,000 samples x 2 epochs) / (3 steps/second x 8 batch size x 3600 seconds)\n= 20,000 / 86,400\n= ~0.23 hours (~14 minutes)","position":{"start":{"line":77,"column":1},"end":{"line":81,"column":1}},"key":"AF1bOdQ39I"},{"type":"paragraph","position":{"start":{"line":83,"column":1},"end":{"line":83,"column":1}},"children":[{"type":"text","value":"This matches what we observed in practice, where the full training run completed in roughly 15 minutes.","position":{"start":{"line":83,"column":1},"end":{"line":83,"column":1}},"key":"Ud8YGFWhQO"}],"key":"AIPY0yo6Hw"},{"type":"paragraph","position":{"start":{"line":85,"column":1},"end":{"line":85,"column":1}},"children":[{"type":"text","value":"A few things that affect training speed:","position":{"start":{"line":85,"column":1},"end":{"line":85,"column":1}},"key":"JeLRiTc74m"}],"key":"gagMpdwtx6"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":87,"column":1},"end":{"line":91,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":87,"column":1},"end":{"line":87,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":87,"column":1},"end":{"line":87,"column":1}},"children":[{"type":"text","value":"Batch size","position":{"start":{"line":87,"column":1},"end":{"line":87,"column":1}},"key":"EQyvTAiLUT"}],"key":"Yk9jaZL24X"},{"type":"text","value":" - larger batches process more samples per step, reducing total steps and training time. However, the tradeoff is that larger batch sizes require more GPU VRAM to store activations, so there is a practical upper limit based on how much memory your GPU has available","position":{"start":{"line":87,"column":1},"end":{"line":87,"column":1}},"key":"Rsnf3TdWcU"}],"key":"RuEqGnHqb3"}],"key":"BmchAxG0fr"},{"type":"listItem","spread":true,"position":{"start":{"line":88,"column":1},"end":{"line":88,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":88,"column":1},"end":{"line":88,"column":1}},"children":[{"type":"text","value":"Sequence length","position":{"start":{"line":88,"column":1},"end":{"line":88,"column":1}},"key":"kl3y71NZwg"}],"key":"T4hCBSgSUl"},{"type":"text","value":" - longer sequences increase the cost of each step due to the attention mechanism scaling quadratically with sequence length","position":{"start":{"line":88,"column":1},"end":{"line":88,"column":1}},"key":"VMmyXPfMlI"}],"key":"UANcIYvYtC"}],"key":"F41gFWPskz"},{"type":"listItem","spread":true,"position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"children":[{"type":"text","value":"Precision","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"key":"UDFHK8fhwD"}],"key":"dB1LtD8F0v"},{"type":"text","value":" - bfloat16 is roughly 2x faster than float32 on modern GPUs","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"key":"XYrEIa1HvK"}],"key":"bLc5ghQ0OH"}],"key":"ZaMmHcFlQK"},{"type":"listItem","spread":true,"position":{"start":{"line":90,"column":1},"end":{"line":91,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":90,"column":1},"end":{"line":90,"column":1}},"children":[{"type":"text","value":"GPU generation","position":{"start":{"line":90,"column":1},"end":{"line":90,"column":1}},"key":"h5FA2aGT9q"}],"key":"Rz86MEhFXH"},{"type":"text","value":" - an A100 is roughly 3 to 4x faster per step than an L4 for the same workload","position":{"start":{"line":90,"column":1},"end":{"line":90,"column":1}},"key":"M2dDodFOjd"}],"key":"anAsusVtU6"}],"key":"jftRqpqlmR"}],"key":"xHdcYNRmW8"},{"type":"paragraph","position":{"start":{"line":92,"column":1},"end":{"line":92,"column":1}},"children":[{"type":"text","value":"Use the ","position":{"start":{"line":92,"column":1},"end":{"line":92,"column":1}},"key":"tCMeocZbWQ"},{"type":"link","url":"https://cloud.google.com/products/calculator","position":{"start":{"line":92,"column":1},"end":{"line":92,"column":1}},"children":[{"type":"text","value":"GCP Pricing Calculator","position":{"start":{"line":92,"column":1},"end":{"line":92,"column":1}},"key":"oqdhbUXNPP"}],"urlSource":"https://cloud.google.com/products/calculator","key":"XNfuKxp3BJ"},{"type":"text","value":" with your estimated GPU hours to get a cost estimate before starting a run.","position":{"start":{"line":92,"column":1},"end":{"line":92,"column":1}},"key":"cBsfo9j7mN"}],"key":"UHFywAoaHv"},{"type":"heading","depth":2,"position":{"start":{"line":94,"column":1},"end":{"line":94,"column":1}},"children":[{"type":"text","value":"Common GCP GPUs and When to Use Them","position":{"start":{"line":94,"column":1},"end":{"line":94,"column":1}},"key":"F466FTNlWA"}],"identifier":"common-gcp-gpus-and-when-to-use-them","label":"Common GCP GPUs and When to Use Them","html_id":"common-gcp-gpus-and-when-to-use-them","implicit":true,"key":"IudXbD0LBa"},{"type":"table","position":{"start":{"line":96,"column":1},"end":{"line":102,"column":1}},"children":[{"type":"tableRow","position":{"start":{"line":96,"column":1},"end":{"line":96,"column":1}},"children":[{"type":"tableCell","header":true,"position":{"start":{"line":96,"column":1},"end":{"line":96,"column":1}},"children":[{"type":"text","value":"GPU","position":{"start":{"line":96,"column":1},"end":{"line":96,"column":1}},"key":"KyHxorrdA9"}],"key":"lmFsCXl23c"},{"type":"tableCell","header":true,"position":{"start":{"line":96,"column":1},"end":{"line":96,"column":1}},"children":[{"type":"text","value":"VRAM","position":{"start":{"line":96,"column":1},"end":{"line":96,"column":1}},"key":"hP5JaSIr4O"}],"key":"NOemjPLGt7"},{"type":"tableCell","header":true,"position":{"start":{"line":96,"column":1},"end":{"line":96,"column":1}},"children":[{"type":"text","value":"Best For","position":{"start":{"line":96,"column":1},"end":{"line":96,"column":1}},"key":"MmKfp7In8H"}],"key":"HEdilnvEyl"}],"key":"E0nRHYs6cJ"},{"type":"tableRow","position":{"start":{"line":98,"column":1},"end":{"line":98,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":98,"column":1},"end":{"line":98,"column":1}},"children":[{"type":"text","value":"NVIDIA T4","position":{"start":{"line":98,"column":1},"end":{"line":98,"column":1}},"key":"Qvi1CBNpNi"}],"key":"ppNkT8Kqe6"},{"type":"tableCell","position":{"start":{"line":98,"column":1},"end":{"line":98,"column":1}},"children":[{"type":"text","value":"16GB","position":{"start":{"line":98,"column":1},"end":{"line":98,"column":1}},"key":"MQcZMWDHO1"}],"key":"TDq6oYP2nH"},{"type":"tableCell","position":{"start":{"line":98,"column":1},"end":{"line":98,"column":1}},"children":[{"type":"text","value":"Small models up to 3B, inference, cost-sensitive runs","position":{"start":{"line":98,"column":1},"end":{"line":98,"column":1}},"key":"OoadwmcGEQ"}],"key":"eoaDRclcxc"}],"key":"HKLw5nAT6S"},{"type":"tableRow","position":{"start":{"line":99,"column":1},"end":{"line":99,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":99,"column":1},"end":{"line":99,"column":1}},"children":[{"type":"text","value":"NVIDIA L4","position":{"start":{"line":99,"column":1},"end":{"line":99,"column":1}},"key":"b8WjZXLDG4"}],"key":"Ihj9QRTH2r"},{"type":"tableCell","position":{"start":{"line":99,"column":1},"end":{"line":99,"column":1}},"children":[{"type":"text","value":"24GB","position":{"start":{"line":99,"column":1},"end":{"line":99,"column":1}},"key":"aJxPnW4VBr"}],"key":"bKnFiymRch"},{"type":"tableCell","position":{"start":{"line":99,"column":1},"end":{"line":99,"column":1}},"children":[{"type":"text","value":"Models up to 7B with LoRA, good price/performance","position":{"start":{"line":99,"column":1},"end":{"line":99,"column":1}},"key":"BAydrAOBnk"}],"key":"MosIs5AcLP"}],"key":"PGxM62nuI2"},{"type":"tableRow","position":{"start":{"line":100,"column":1},"end":{"line":100,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":100,"column":1},"end":{"line":100,"column":1}},"children":[{"type":"text","value":"NVIDIA A100 40GB","position":{"start":{"line":100,"column":1},"end":{"line":100,"column":1}},"key":"inaDUkPOe6"}],"key":"xw1dqIkeJh"},{"type":"tableCell","position":{"start":{"line":100,"column":1},"end":{"line":100,"column":1}},"children":[{"type":"text","value":"40GB","position":{"start":{"line":100,"column":1},"end":{"line":100,"column":1}},"key":"NRSfFxsS5V"}],"key":"xSgt64uKnX"},{"type":"tableCell","position":{"start":{"line":100,"column":1},"end":{"line":100,"column":1}},"children":[{"type":"text","value":"Models up to 13B full fine-tune, large batch training","position":{"start":{"line":100,"column":1},"end":{"line":100,"column":1}},"key":"OX9Lpv8fAH"}],"key":"nznIwRiNXi"}],"key":"Q4ozjVJV5l"},{"type":"tableRow","position":{"start":{"line":101,"column":1},"end":{"line":101,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":101,"column":1},"end":{"line":101,"column":1}},"children":[{"type":"text","value":"NVIDIA A100 80GB","position":{"start":{"line":101,"column":1},"end":{"line":101,"column":1}},"key":"kh0oqjg6Mc"}],"key":"P8aMvHrdtY"},{"type":"tableCell","position":{"start":{"line":101,"column":1},"end":{"line":101,"column":1}},"children":[{"type":"text","value":"80GB","position":{"start":{"line":101,"column":1},"end":{"line":101,"column":1}},"key":"VhJLIHz8Xg"}],"key":"xDcv1Cnb7C"},{"type":"tableCell","position":{"start":{"line":101,"column":1},"end":{"line":101,"column":1}},"children":[{"type":"text","value":"Models up to 70B with LoRA, large context lengths","position":{"start":{"line":101,"column":1},"end":{"line":101,"column":1}},"key":"IQFv1536Uq"}],"key":"R0uroz1MA7"}],"key":"GEd2Ot8I5S"},{"type":"tableRow","position":{"start":{"line":102,"column":1},"end":{"line":102,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":102,"column":1},"end":{"line":102,"column":1}},"children":[{"type":"text","value":"NVIDIA H100","position":{"start":{"line":102,"column":1},"end":{"line":102,"column":1}},"key":"xiI5zATQop"}],"key":"G10wKVfrTv"},{"type":"tableCell","position":{"start":{"line":102,"column":1},"end":{"line":102,"column":1}},"children":[{"type":"text","value":"80GB","position":{"start":{"line":102,"column":1},"end":{"line":102,"column":1}},"key":"t2MEGKNvWU"}],"key":"pRBEFjUUcJ"},{"type":"tableCell","position":{"start":{"line":102,"column":1},"end":{"line":102,"column":1}},"children":[{"type":"text","value":"Largest models, fastest training, highest cost","position":{"start":{"line":102,"column":1},"end":{"line":102,"column":1}},"key":"S43uVgeCve"}],"key":"IYKl7JbrUT"}],"key":"GHsR7xwopF"}],"key":"L92jUJjDBw"},{"type":"heading","depth":2,"position":{"start":{"line":104,"column":1},"end":{"line":104,"column":1}},"children":[{"type":"text","value":"Practical Tips","position":{"start":{"line":104,"column":1},"end":{"line":104,"column":1}},"key":"dHYBSRALr4"}],"identifier":"practical-tips","label":"Practical Tips","html_id":"practical-tips","implicit":true,"key":"VyKVThfRBm"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":106,"column":1},"end":{"line":110,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":106,"column":1},"end":{"line":106,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":106,"column":1},"end":{"line":106,"column":1}},"children":[{"type":"text","value":"Start small and scale up.","position":{"start":{"line":106,"column":1},"end":{"line":106,"column":1}},"key":"B88s1D6pU1"}],"key":"B4MucBElGC"},{"type":"text","value":" Run a quick test with a small number of training examples before committing to a full run. If it fits in memory, scale up.","position":{"start":{"line":106,"column":1},"end":{"line":106,"column":1}},"key":"rCoflGCQbE"}],"key":"OkW5gRsRxU"}],"key":"SjgBzrmzm0"},{"type":"listItem","spread":true,"position":{"start":{"line":107,"column":1},"end":{"line":107,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":107,"column":1},"end":{"line":107,"column":1}},"children":[{"type":"text","value":"Reduce batch size if you hit OOM.","position":{"start":{"line":107,"column":1},"end":{"line":107,"column":1}},"key":"pB846afVpn"}],"key":"EqAxhwTfEI"},{"type":"text","value":" Halving the batch size roughly halves activation memory. Use gradient accumulation to compensate for effective batch size.","position":{"start":{"line":107,"column":1},"end":{"line":107,"column":1}},"key":"PcHKNty0aE"}],"key":"TQpldBGLDg"}],"key":"tm7pSZxkyV"},{"type":"listItem","spread":true,"position":{"start":{"line":108,"column":1},"end":{"line":108,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":108,"column":1},"end":{"line":108,"column":1}},"children":[{"type":"text","value":"Use bfloat16.","position":{"start":{"line":108,"column":1},"end":{"line":108,"column":1}},"key":"oqSZSitxPj"}],"key":"QDK89ZZasM"},{"type":"text","value":" Switching from float32 to bfloat16 halves model and gradient memory with minimal impact on training quality. This is why we set ","position":{"start":{"line":108,"column":1},"end":{"line":108,"column":1}},"key":"NGMGurPenw"},{"type":"inlineCode","value":"bf16=True","position":{"start":{"line":108,"column":1},"end":{"line":108,"column":1}},"key":"wvE62ODE1g"},{"type":"text","value":" in the training arguments.","position":{"start":{"line":108,"column":1},"end":{"line":108,"column":1}},"key":"lDud3wo2VQ"}],"key":"dit2rQt2QO"}],"key":"vshi4TFDIj"},{"type":"listItem","spread":true,"position":{"start":{"line":109,"column":1},"end":{"line":110,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"children":[{"type":"text","value":"Check memory usage during training.","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"MjtS5hSblB"}],"key":"xmFdQ8vo0n"},{"type":"text","value":" Run ","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"xLR07mwiBg"},{"type":"inlineCode","value":"nvidia-smi","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"UlA7bpzwK3"},{"type":"text","value":" in a separate terminal or tmux window during training to see live GPU memory utilization.","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"uuepMHs9vS"}],"key":"MSJw95IZD3"}],"key":"vkw2Os0BHK"}],"key":"Ip9Ah9Dh9K"},{"type":"heading","depth":2,"position":{"start":{"line":111,"column":1},"end":{"line":111,"column":1}},"children":[{"type":"text","value":"References and Further Reading","position":{"start":{"line":111,"column":1},"end":{"line":111,"column":1}},"key":"sjzDEbDj3d"}],"identifier":"references-and-further-reading","label":"References and Further Reading","html_id":"references-and-further-reading","implicit":true,"key":"Lm36ggOMrL"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":113,"column":1},"end":{"line":117,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":113,"column":1},"end":{"line":113,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://www.nvidia.com/en-us/data-center/products/gpus/","position":{"start":{"line":113,"column":1},"end":{"line":113,"column":1}},"children":[{"type":"text","value":"NVIDIA GPU Specifications","position":{"start":{"line":113,"column":1},"end":{"line":113,"column":1}},"key":"GvDxVD51fU"}],"urlSource":"https://www.nvidia.com/en-us/data-center/products/gpus/","key":"uLiKLaZyaY"},{"type":"text","value":" - Full specs for all NVIDIA data center GPUs including VRAM and compute capacity","position":{"start":{"line":113,"column":1},"end":{"line":113,"column":1}},"key":"s8M2UmUtU6"}],"key":"d2RoHG0HfY"}],"key":"w2URZoyAa0"},{"type":"listItem","spread":true,"position":{"start":{"line":114,"column":1},"end":{"line":114,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://cloud.google.com/compute/gpus-pricing","position":{"start":{"line":114,"column":1},"end":{"line":114,"column":1}},"children":[{"type":"text","value":"GCP GPU Pricing","position":{"start":{"line":114,"column":1},"end":{"line":114,"column":1}},"key":"rIRUcraUk4"}],"urlSource":"https://cloud.google.com/compute/gpus-pricing","key":"DJ5ObCC3UU"},{"type":"text","value":" - Full pricing breakdown for GPU VM instances on GCP","position":{"start":{"line":114,"column":1},"end":{"line":114,"column":1}},"key":"xxAyB7WHo1"}],"key":"XRRqjV1XlG"}],"key":"CvSpr2XJHg"},{"type":"listItem","spread":true,"position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://huggingface.co/spaces/hf-accelerate/model-memory-usage","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"children":[{"type":"text","value":"HuggingFace Model Memory Calculator","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"lB5tYB6xdH"}],"urlSource":"https://huggingface.co/spaces/hf-accelerate/model-memory-usage","key":"c9UzZHFvi2"},{"type":"text","value":" - A handy tool for estimating memory requirements for HuggingFace models","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"ROuDJNkwXJ"}],"key":"oxV6iymgIu"}],"key":"MnDbzeMRUC"},{"type":"listItem","spread":true,"position":{"start":{"line":116,"column":1},"end":{"line":116,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://arxiv.org/abs/2106.09685","position":{"start":{"line":116,"column":1},"end":{"line":116,"column":1}},"children":[{"type":"text","value":"LoRA: Low-Rank Adaptation of Large Language Models","position":{"start":{"line":116,"column":1},"end":{"line":116,"column":1}},"key":"LSolme2JFj"}],"urlSource":"https://arxiv.org/abs/2106.09685","key":"or63u30q7j"},{"type":"text","value":" - The original LoRA paper by Hu et al.","position":{"start":{"line":116,"column":1},"end":{"line":116,"column":1}},"key":"fVRAT0A2Pw"}],"key":"EaGmCFjvev"}],"key":"Ipr0AlgUka"},{"type":"listItem","spread":true,"position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://arxiv.org/abs/1710.03740","position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"children":[{"type":"text","value":"Mixed Precision Training","position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"key":"giAzbiaH3i"}],"urlSource":"https://arxiv.org/abs/1710.03740","key":"AjrP2PmTqT"},{"type":"text","value":" - The paper introducing bfloat16 and float16 training","position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"key":"Bvvzd2eZt5"}],"key":"a1Szf8hQPX"}],"key":"jZ89IuWa7Q"}],"key":"RdpyvvS4by"}],"key":"Oy7ngBgGVw"}],"key":"l1ZsNQonhd"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Hands-On: Setting Up Billing Alerts and Budgets","url":"/part4-cost-management/hands-on-cost-management","group":"Part 4 - Cost Management"}}},"domain":"http://localhost:3000"},"project":{"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true}},"title":"AIMS GCP Tutorial","description":"A no-fluff guide to using GCP for ML experiments and research","authors":[{"id":"AIMS TtT Team","name":"AIMS TtT Team"}],"github":"https://github.com/rexsimiloluwa/aims-ai-gcp-tutorial","keywords":["GCP","AI","GPU","Machine Learning","Research","Experiments","MLOps"],"id":"7eddb7b1-9986-44d1-8417-fe2b2bb4b498","toc":[{"file":"index.md"},{"children":[{"file":"part1-gcp-foundations/index.md"},{"file":"part1-gcp-foundations/01-understanding-gcp.md"},{"file":"part1-gcp-foundations/02-gcp-console.md"},{"file":"part1-gcp-foundations/03-the-gcloud-cli.md"},{"file":"part1-gcp-foundations/04-hands-on-gcp-console.md"},{"file":"part1-gcp-foundations/05-hands-on-gcloud-cli.md"}],"title":"Part 1 - GCP Foundations"},{"children":[{"file":"part2-running-experiments-on-gcp-vms/index.md"},{"file":"part2-running-experiments-on-gcp-vms/01-hands-on-running-ai-experiments-on-gcp-vms.md"}],"title":"Part 2 - Running Experiments On GCP VMs"},{"children":[{"file":"part3-vertex-ai/index.md"},{"file":"part3-vertex-ai/01-hands-on-vertex-ai-custom-training-job.md"},{"file":"part3-vertex-ai/02-hands-on-vertex-ai-notebook-colab-enterprise.md"}],"title":"Part 3 - Vertex AI"},{"children":[{"file":"part4-cost-management/index.md"},{"file":"part4-cost-management/01-hands-on-cost-management.md"}],"title":"Part 4 - Cost Management"},{"children":[{"file":"appendix/01-gpu-maths.md"},{"file":"append"}],"title":"Appendix"}],"exports":[],"bibliography":[],"index":"index","pages":[{"level":1,"title":"Part 1 - GCP Foundations"},{"slug":"part1-gcp-foundations.index","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.understanding-gcp","title":"GCP Core Concepts","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.gcp-console","title":"The GCP Console","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/gcp-console-overview-3bc9f60c20034de4d1d7608c1c20958b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.the-gcloud-cli","title":"The Google Cloud CLI (gcloud)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.hands-on-gcp-console","title":"Hands-On 01: Create a VM via GCP Console","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/hands-on1-gcp-consol-455c36015a61307f47b7e22a1c0c327a.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part1-gcp-foundations.hands-on-gcloud-cli","title":"Hands-On 02: Automation with the Google Cloud CLI (gcloud)","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/hands-on2-gcloud-vm--5dab86ddf7d1bb7cb3b274cc2bd0b0cc.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Part 2 - Running Experiments On GCP VMs"},{"slug":"part2-running-experiments-on-gcp-vms.index","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part2-running-experiments-on-gcp-vms.hands-on-running-ai-experiments-on-gcp-vms","title":"Hands-On 01: Fine-tuning a Small Language Model (Gemma 3 1B) on a GCP VM","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/create_new_project_w-03c4f91a18e3e9153e1f0e80158d0ad8.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Part 3 - Vertex AI"},{"slug":"part3-vertex-ai.index","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part3-vertex-ai.hands-on-vertex-ai-custom-training-job","title":"Hands-On 01: Vertex AI Custom Training Jobs","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/gcloud-submit-vertex-bc8a636bcd50705f5dfebf9ad2877d2a.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part3-vertex-ai.hands-on-vertex-ai-notebook-colab-enterprise","title":"Hands-On 02: Vertex AI Notebooks (Colab Enterprise)","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/gcp-console-colab-en-667cd5e857b7833a4103273ae3f7f888.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Part 4 - Cost Management"},{"slug":"part4-cost-management.index","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"part4-cost-management.hands-on-cost-management","title":"Hands-On: Setting Up Billing Alerts and Budgets","description":"","date":"","thumbnail":"/gcp-tutorial-aims/build/gcp-console-create-b-738ab6aaceb92003b75efc827d6ff81d.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Appendix"},{"slug":"appendix.gpu-maths","title":"Appendix A: GPU Maths - Choosing the Right GPU for Your Experiment","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/gcp-tutorial-aims/build/manifest-54BD5942.js";
import * as route0 from "/gcp-tutorial-aims/build/root-PMP5BIHC.js";
import * as route1 from "/gcp-tutorial-aims/build/routes/$-5ZLZ2O3Y.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/gcp-tutorial-aims/build/entry.client-PCJPW7TK.js");</script></body></html>